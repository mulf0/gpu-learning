<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPU Architecture — GPU Learning</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../base.css" />
    <style>
      /* Lesson page - no sidebar */
      body {
        display: block;
      }

      .lesson-nav {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        background: rgba(10, 10, 15, 0.9);
        backdrop-filter: blur(20px);
        border-bottom: 1px solid var(--border-subtle);
        z-index: 100;
        padding: var(--space-md) var(--space-xl);
      }

      .lesson-nav__inner {
        max-width: 800px;
        margin: 0 auto;
        display: flex;
        justify-content: space-between;
        align-items: center;
      }

      .lesson-nav__title {
        font-family: var(--font-mono);
        font-size: 0.85rem;
        color: var(--text-secondary);
        letter-spacing: 0.05em;
      }

      .lesson-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 6rem var(--space-xl) var(--space-3xl);
      }

      .hierarchy {
        display: flex;
        flex-direction: column;
        gap: var(--space-sm);
      }

      .hier-item {
        display: flex;
        align-items: center;
        gap: var(--space-md);
        padding: var(--space-md);
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        cursor: pointer;
        transition: all var(--transition-normal);
      }

      .hier-item:hover,
      .hier-item:focus {
        border-color: var(--accent-blue);
        outline: none;
      }

      .hier-item.active {
        border-color: var(--accent-blue);
        background: var(--bg-tertiary);
      }

      .hier-icon {
        width: 40px;
        height: 40px;
        border-radius: var(--radius-sm);
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 1.2rem;
        flex-shrink: 0;
      }

      .hier-info h5 {
        margin: 0 0 var(--space-xs);
        font-size: 0.95rem;
        font-weight: 500;
      }

      .hier-info p {
        margin: 0;
        font-size: 0.8rem;
        color: var(--text-muted);
      }

      .hier-detail {
        display: none;
        padding: var(--space-md) var(--space-lg);
        margin-left: 56px;
        background: var(--bg-tertiary);
        border-left: 3px solid var(--accent-blue);
        border-radius: 0 var(--radius-md) var(--radius-md) 0;
        font-size: 0.9rem;
        color: var(--text-secondary);
        animation: fadeIn 0.2s ease;
      }

      .hier-detail.visible {
        display: block;
      }

      @keyframes fadeIn {
        from { opacity: 0; transform: translateY(-5px); }
        to { opacity: 1; transform: translateY(0); }
      }

      .spec-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(140px, 1fr));
        gap: var(--space-md);
        margin: var(--space-lg) 0;
      }

      .spec-card {
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        padding: var(--space-md);
        text-align: center;
      }

      .spec-card__value {
        font-family: var(--font-mono);
        font-size: 1.5rem;
        font-weight: 600;
        color: var(--accent-blue);
      }

      .spec-card__label {
        font-size: 0.75rem;
        color: var(--text-muted);
        margin-top: var(--space-xs);
      }

      .thread-viz {
        display: flex;
        flex-wrap: wrap;
        gap: 2px;
        padding: var(--space-md);
        background: var(--bg-secondary);
        border-radius: var(--radius-md);
        margin: var(--space-md) 0;
      }

      .thread-viz__thread {
        width: 12px;
        height: 12px;
        border-radius: 2px;
        background: var(--accent-blue);
        opacity: 0.3;
        transition: all var(--transition-fast);
      }

      .thread-viz__thread.active {
        opacity: 1;
        transform: scale(1.1);
      }

      .thread-viz__thread.diverged {
        background: var(--accent-orange);
      }

      .warp-demo {
        margin: var(--space-lg) 0;
      }

      .warp-demo__controls {
        display: flex;
        gap: var(--space-sm);
        margin-bottom: var(--space-md);
      }

      .occupancy-calc {
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        padding: var(--space-lg);
        margin: var(--space-lg) 0;
      }

      .occupancy-calc__row {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: var(--space-sm) 0;
        border-bottom: 1px solid var(--border-subtle);
      }

      .occupancy-calc__row:last-child {
        border-bottom: none;
        padding-top: var(--space-md);
        font-weight: 600;
      }

      .occupancy-calc__label {
        color: var(--text-secondary);
      }

      .occupancy-calc__value {
        font-family: var(--font-mono);
        color: var(--text-primary);
      }

      .occupancy-bar {
        height: 24px;
        background: var(--bg-tertiary);
        border-radius: var(--radius-sm);
        overflow: hidden;
        margin-top: var(--space-md);
      }

      .occupancy-bar__fill {
        height: 100%;
        background: linear-gradient(90deg, var(--accent-green), var(--accent-blue));
        transition: width var(--transition-normal);
        display: flex;
        align-items: center;
        justify-content: flex-end;
        padding-right: var(--space-sm);
        font-family: var(--font-mono);
        font-size: 0.75rem;
        color: white;
      }
    </style>
  </head>
  <body>
    <!-- Navigation -->
    <nav class="lesson-nav">
      <div class="lesson-nav__inner">
        <a href="../index.html" class="lesson-nav__title">← COURSE INDEX</a>
      </div>
    </nav>

    <div class="lesson-container">
      <!-- Hero -->
      <header class="hero hero--lesson">
        <div class="hero__label">GPU Fundamentals</div>
        <h1 class="hero__title">GPU Architecture</h1>
        <p class="hero__desc">
          Understanding the hardware execution model. From SMs to warps to threads—
          learn how modern NVIDIA GPUs actually execute your code.
        </p>
      </header>

      <!-- Section 1: The Execution Model -->
      <section class="section" id="section-0">
        <div class="section__number">01 — THE BIG PICTURE</div>
        <h2 class="section__title">Throughput Over Latency</h2>

        <p>
          CPUs optimize for <strong>latency</strong>—making single tasks fast.
          GPUs optimize for <strong>throughput</strong>—completing many tasks in parallel,
          even if each individual task takes longer.
        </p>

        <p>
          A CPU might have <a href="https://en.wikipedia.org/wiki/Multi-core_processor" target="_blank" rel="noopener">8-16 cores running at 4+ GHz</a> with massive caches.
          A GPU has <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation" target="_blank" rel="noopener">thousands of smaller cores running at ~2 GHz</a> with limited cache per core.
          The magic is in the parallelism.
        </p>

        <div class="spec-grid">
          <div class="spec-card">
            <div class="spec-card__value">~192</div>
            <div class="spec-card__label"><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">SMs (B200)*</a></div>
          </div>
          <div class="spec-card">
            <div class="spec-card__value">192GB</div>
            <div class="spec-card__label"><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">HBM3e</a></div>
          </div>
          <div class="spec-card">
            <div class="spec-card__value">~8 TB/s</div>
            <div class="spec-card__label"><a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/" target="_blank" rel="noopener">Memory BW*</a></div>
          </div>
          <div class="spec-card">
            <div class="spec-card__value">~2.5 GHz</div>
            <div class="spec-card__label"><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">Boost Clock*</a></div>
          </div>
        </div>
        <p class="text-muted text-small" style="margin-top: var(--space-sm);">
          *B200 specifications from <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">NVIDIA Blackwell Architecture</a>. Verify against official datasheets for production use.
        </p>

        <div class="callout info">
          <div class="callout-title">Key Insight</div>
          <p class="mb-0">
            GPU programming is about keeping thousands of threads busy. When one thread
            waits for memory, others execute. This <strong>latency hiding</strong> is
            fundamental to GPU performance.
          </p>
        </div>

      </section>

      <!-- Section 2: The Hierarchy -->
      <section class="section" id="section-1">
        <div class="section__number">02 — EXECUTION HIERARCHY</div>
        <h2 class="section__title">GPU → SM → Warp → Thread</h2>

        <p>
          NVIDIA GPUs have a strict hierarchy. Understanding each level is essential
          for writing efficient kernels.
        </p>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--blue">Click to explore</div>
          </div>

          <div class="hierarchy" id="hierarchy-viz">
            <div class="hier-item" data-detail="d-gpu" tabindex="0">
              <div class="hier-icon bg-blue">GPU</div>
              <div class="hier-info">
                <h5>GPU Device</h5>
                <p><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">Blackwell B200</a>: up to 192 SMs, 192GB HBM3e, ~8TB/s bandwidth*</p>
              </div>
            </div>
            <div class="hier-detail" id="d-gpu">
              The top-level device. What matters most: <strong>SM count</strong> (parallel execution units)
              and <strong>memory bandwidth</strong> (data throughput). More SMs = more parallelism.
              Higher bandwidth = faster data movement.
            </div>

            <div class="hier-item" data-detail="d-sm" tabindex="0">
              <div class="hier-icon bg-green">SM</div>
              <div class="hier-info">
                <h5>Streaming Multiprocessor</h5>
                <p>The fundamental execution unit</p>
              </div>
            </div>
            <div class="hier-detail" id="d-sm">
              Each SM contains: <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-multithreading" target="_blank" rel="noopener">4 warp schedulers</a>, <a href="https://resources.nvidia.com/en-us-tensor-core" target="_blank" rel="noopener">~256KB registers, 228KB shared memory</a>,
              L1 cache, CUDA cores, and <strong>Tensor Cores</strong>. Multiple thread blocks
              can run on one SM simultaneously (limited by resources).
            </div>

            <div class="hier-item" data-detail="d-warp" tabindex="0">
              <div class="hier-icon bg-purple">Warp</div>
              <div class="hier-info">
                <h5><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" target="_blank" rel="noopener">32 Threads in Lockstep</a></h5>
                <p>The atomic scheduling unit (SIMT)</p>
              </div>
            </div>
            <div class="hier-detail" id="d-warp">
              <strong>This is the most important concept.</strong> All <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" target="_blank" rel="noopener">32 threads in a warp</a>
              execute the same instruction simultaneously. Divergent branches serialize execution.
              Always think in warps, not individual threads.
            </div>

            <div class="hier-item" data-detail="d-wg" tabindex="0">
              <div class="hier-icon bg-orange">WG</div>
              <div class="hier-info">
                <h5><a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html" target="_blank" rel="noopener">Warpgroup (Hopper+)</a></h5>
                <p>4 warps (128 threads) for Tensor Core operations</p>
              </div>
            </div>
            <div class="hier-detail" id="d-wg">
              Modern Tensor Core operations work at <a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html" target="_blank" rel="noopener">warpgroup granularity</a>. This enables
              larger tiles (64x64+), asynchronous execution, and better hardware utilization.
              <a href="https://github.com/NVIDIA/cutlass/blob/main/media/docs/cute/00_quickstart.md" target="_blank" rel="noopener">CuTe DSL</a> uses warpgroups for SM90+ architectures.
            </div>

            <div class="hier-item" data-detail="d-thread" tabindex="0">
              <div class="hier-icon bg-cyan">T</div>
              <div class="hier-info">
                <h5>Thread</h5>
                <p>Individual execution context with private registers</p>
              </div>
            </div>
            <div class="hier-detail" id="d-thread">
              Each thread has its own registers and local memory. Threads within a warp
              can communicate via shuffle instructions. Threads within a block can
              communicate via shared memory and synchronize with __syncthreads().
            </div>
          </div>
        </div>

      </section>

      <!-- Section 3: Warps Deep Dive -->
      <section class="section" id="section-2">
        <div class="section__number">03 — WARPS</div>
        <h2 class="section__title">The Warp: GPU's Atomic Unit</h2>

        <p>
          A warp is <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" target="_blank" rel="noopener">32 threads that execute in <strong>SIMT</strong></a> (Single Instruction,
          Multiple Threads) fashion. Every thread in a warp executes the same instruction,
          but on different data.
        </p>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--purple">Interactive: Warp Execution</div>
          </div>

          <p class="text-secondary text-small">
            Click buttons to see how warps execute. Green = active, Orange = diverged/waiting.
          </p>

          <div class="warp-demo">
            <div class="warp-demo__controls">
              <button class="btn btn--primary" id="warp-unified">Unified Execution</button>
              <button class="btn" id="warp-diverge">Branch Divergence</button>
              <button class="btn" id="warp-reset">Reset</button>
            </div>

            <div class="thread-viz" id="warp-viz">
              <!-- 32 threads -->
            </div>

            <div class="result mt-md">
              <div class="result__label">Status</div>
              <p class="text-secondary text-small" id="warp-status" style="margin: 0">
                Click a button to simulate warp execution.
              </p>
            </div>
          </div>
        </div>

        <div class="callout warn">
          <div class="callout-title"><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#simt-architecture" target="_blank" rel="noopener">Warp Divergence</a></div>
          <p class="mb-0">
            When threads in a warp take different branches (if/else), execution <strong>serializes</strong>.
            Both paths run, with inactive threads masked. 16 threads per path = 50% throughput.
            Minimize divergence within warps for maximum performance.
          </p>
        </div>

        <div class="quiz" id="warp-quiz">
          <div class="quiz-q">A warp contains how many threads?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>16 threads</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>32 threads</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>64 threads</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>128 threads (that's a warpgroup)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>

      </section>

      <!-- Section 4: Blocks and Grids -->
      <section class="section" id="section-3">
        <div class="section__number">04 — BLOCKS & GRIDS</div>
        <h2 class="section__title">Organizing Your Parallelism</h2>

        <p>
          <strong>Block</strong> (also called CTA - Cooperative Thread Array): A group of warps
          that share resources and can synchronize. Blocks run on a single SM.
        </p>

        <p>
          <strong>Grid</strong>: Your problem decomposition into blocks. The grid is how you
          map your problem to the GPU's parallel execution model.
        </p>

        <div class="diagram">
          <div class="diagram-title">Block Composition</div>
          <pre style="margin: 0; padding: var(--space-md); background: var(--bg-tertiary); border-radius: var(--radius-sm);">
Grid (your problem)
 └── Block 0 (256 threads = 8 warps)
 │    ├── Warp 0: threads 0-31
 │    ├── Warp 1: threads 32-63
 │    ├── ...
 │    └── Warp 7: threads 224-255
  │    └── [<a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-7-x" target="_blank" rel="noopener">Shared Memory: 48KB</a>]
 │    └── [Can __syncthreads()]
 │
 └── Block 1 (256 threads = 8 warps)
 └── Block 2 ...
 └── Block N-1</pre>
        </div>

        <div class="card">
          <h4>Block Size Selection</h4>
          <p>Common block sizes and their trade-offs:</p>
          <ul>
            <li><strong>128 threads (4 warps)</strong>: 1 warpgroup, good for Tensor Core ops</li>
            <li><strong>256 threads (8 warps)</strong>: Balanced, most common choice</li>
            <li><strong>512 threads (16 warps)</strong>: More parallelism, higher register pressure</li>
          </ul>
          <p class="mb-0 text-muted">
            Block size affects: shared memory per thread, register availability, and occupancy.
          </p>
        </div>

        <div class="quiz" id="block-quiz">
          <div class="quiz-q">What can threads within the same block do that threads in different blocks cannot?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Access global memory</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Share data via shared memory and synchronize</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Run on the same SM</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Use Tensor Cores</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>

      </section>

      <!-- Section 5: Occupancy -->
      <section class="section" id="section-4">
        <div class="section__number">05 — OCCUPANCY</div>
        <h2 class="section__title">Keeping the GPU Busy</h2>

        <p>
          <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy" target="_blank" rel="noopener"><strong>Occupancy</strong></a> is the ratio of active warps to the maximum warps an SM
          can support. Higher occupancy generally means better latency hiding, but it's not
          the only factor in performance.
        </p>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--green">Interactive: Occupancy Calculator</div>
          </div>

          <div class="occupancy-calc">
            <div class="occupancy-calc__row">
              <span class="occupancy-calc__label">Block Size</span>
              <span class="occupancy-calc__value">
                <select id="occ-block-size" class="input" style="width: auto;">
                  <option value="128">128 threads</option>
                  <option value="256" selected>256 threads</option>
                  <option value="512">512 threads</option>
                </select>
              </span>
            </div>
            <div class="occupancy-calc__row">
              <span class="occupancy-calc__label">Registers per Thread</span>
              <span class="occupancy-calc__value">
                <select id="occ-regs" class="input" style="width: auto;">
                  <option value="32">32 regs</option>
                  <option value="64" selected>64 regs</option>
                  <option value="128">128 regs</option>
                  <option value="256">256 regs</option>
                </select>
              </span>
            </div>
            <div class="occupancy-calc__row">
              <span class="occupancy-calc__label">Shared Memory per Block</span>
              <span class="occupancy-calc__value">
                <select id="occ-smem" class="input" style="width: auto;">
                  <option value="0">0 KB</option>
                  <option value="16" selected>16 KB</option>
                  <option value="48">48 KB</option>
                  <option value="96">96 KB</option>
                </select>
              </span>
            </div>
            <div class="occupancy-calc__row">
              <span class="occupancy-calc__label">Max Warps per SM</span>
              <span class="occupancy-calc__value" id="occ-max-warps">64</span>
            </div>
            <div class="occupancy-calc__row">
              <span class="occupancy-calc__label">Active Warps</span>
              <span class="occupancy-calc__value" id="occ-active-warps">32</span>
            </div>
            <div class="occupancy-calc__row">
              <span class="occupancy-calc__label">Occupancy</span>
              <span class="occupancy-calc__value text-green" id="occ-result">50%</span>
            </div>
          </div>

          <div class="occupancy-bar">
            <div class="occupancy-bar__fill" id="occ-bar" style="width: 50%">50%</div>
          </div>

          <p class="text-muted text-small mt-md mb-0">
            Simplified model based on <a href="https://resources.nvidia.com/en-us-tensor-core" target="_blank" rel="noopener">Hopper SM</a> (64 max warps, 256KB registers, 228KB shared memory).
          </p>
        </div>

        <div class="callout info">
          <div class="callout-title">Occupancy vs Performance</div>
          <p class="mb-0">
            100% occupancy doesn't guarantee best performance. Sometimes using more registers
            (lower occupancy) enables better instruction-level parallelism. Profile your actual
            kernel to find the sweet spot.
          </p>
        </div>

      </section>

      <!-- Completion -->
      <section class="section" style="text-align: center; padding: var(--space-2xl) 0">
        <div class="section__number">NEXT STEPS</div>
        <h2 class="section__title">Architecture Understood</h2>
        <p style="max-width: 500px; margin: 0 auto var(--space-xl)">
          You now understand how GPUs execute code at the hardware level. Next up:
          the memory hierarchy—the key to actual performance optimization.
        </p>
        <a href="../index.html" class="btn btn--primary">Back to Course Index</a>
        <a href="memory-hierarchy.html" class="btn" style="margin-left: var(--space-sm)">
          Next: Memory Hierarchy
        </a>
      </section>

      <!-- References -->
      <section class="section" id="references">
        <div class="section__number">REFERENCES</div>
        <h2 class="section__title">Citations & Further Reading</h2>

        <!-- Video Resources -->
        <div class="card">
          <h4>Video Resources</h4>
          <p class="text-secondary text-small" style="margin-bottom: var(--space-md);">
            High-quality explanations of GPU architecture concepts.
          </p>
          
          <div style="margin-bottom: var(--space-lg);">
            <strong>How do Graphics Cards Work? (Branch Education)</strong>
            <p class="text-muted text-small" style="margin: var(--space-xs) 0;">
              Excellent visual explanation of GPU architecture fundamentals, parallelism, and memory hierarchy.
            </p>
            <a href="https://www.youtube.com/watch?v=h9Z4oGN89MU" target="_blank" rel="noopener" style="color: var(--accent-blue);">
              Watch on YouTube
            </a>
          </div>
          
          <div style="margin-bottom: var(--space-md);">
            <strong>CUDA Programming Model (NVIDIA Developer)</strong>
            <p class="text-muted text-small" style="margin: var(--space-xs) 0;">
              Official NVIDIA explanation of threads, blocks, grids, and the CUDA execution model.
            </p>
            <a href="https://developer.nvidia.com/blog/cuda-refresher-cuda-programming-model/" target="_blank" rel="noopener" style="color: var(--accent-blue);">
              Read: CUDA Refresher Series - developer.nvidia.com
            </a>
          </div>
        </div>

        <div class="card" style="margin-top: var(--space-lg);">
          <h4>Primary Documentation</h4>
          <ol style="margin: 0; padding-left: var(--space-lg); color: var(--text-secondary);">
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA CUDA C++ Programming Guide</strong><br>
              Chapters 4-5: Thread Hierarchy, SIMT Architecture, Memory Hierarchy<br>
              <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy" target="_blank" rel="noopener" style="color: var(--accent-blue);">docs.nvidia.com/cuda/cuda-c-programming-guide</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA CUDA C++ Best Practices Guide</strong><br>
              Performance optimization, occupancy, memory access patterns<br>
              <a href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/" target="_blank" rel="noopener" style="color: var(--accent-blue);">docs.nvidia.com/cuda/cuda-c-best-practices-guide</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA Hopper Architecture Whitepaper</strong><br>
              SM specifications, warpgroup operations, Tensor Core details<br>
              <a href="https://resources.nvidia.com/en-us-hopper-architecture" target="_blank" rel="noopener" style="color: var(--accent-blue);">resources.nvidia.com/en-us-hopper-architecture</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA Blackwell Architecture</strong><br>
              Fifth-gen Tensor Cores, NVLink 5, 208B transistors<br>
              <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener" style="color: var(--accent-blue);">nvidia.com/blackwell-architecture</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA GB200 NVL72 Specifications</strong><br>
              Official Blackwell product specifications and performance data<br>
              <a href="https://www.nvidia.com/en-us/data-center/gb200-nvl72/" target="_blank" rel="noopener" style="color: var(--accent-blue);">nvidia.com/data-center/gb200-nvl72</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA H100 Tensor Core GPU Datasheet</strong><br>
              Hardware specifications: 256KB registers/SM, 228KB SMEM, 64 max warps<br>
              <a href="https://resources.nvidia.com/en-us-hopper-architecture/nvidia-tensor-core-gpu-datasheet" target="_blank" rel="noopener" style="color: var(--accent-blue);">H100 Datasheet (PDF)</a>
            </li>
          </ol>
        </div>

        <div class="card" style="margin-top: var(--space-lg);">
          <h4>Key Specifications with Sources</h4>
          <table style="width: 100%; font-size: 0.85rem; border-collapse: collapse;">
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <th style="text-align: left; padding: var(--space-sm);">Specification</th>
              <th style="text-align: left; padding: var(--space-sm);">Value</th>
              <th style="text-align: left; padding: var(--space-sm);">Source</th>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">Warp size</td>
              <td style="padding: var(--space-sm);">32 threads</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">CUDA Programming Guide, Ch. 4</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">Warpgroup (Hopper+)</td>
              <td style="padding: var(--space-sm);">128 threads (4 warps)</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">Hopper Architecture Whitepaper</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">H100 registers/SM</td>
              <td style="padding: var(--space-sm);">256KB (65,536 x 32-bit)</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">H100 Datasheet</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">H100 shared memory/SM</td>
              <td style="padding: var(--space-sm);">Up to 228KB configurable</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">H100 Datasheet</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">Max warps per SM</td>
              <td style="padding: var(--space-sm);">64 (Hopper)</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">Hopper Architecture Whitepaper</td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">Blackwell transistors</td>
              <td style="padding: var(--space-sm);">208 billion</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">NVIDIA Blackwell Architecture</td>
            </tr>
            <tr>
              <td style="padding: var(--space-sm);">B200 HBM bandwidth</td>
              <td style="padding: var(--space-sm);">~8 TB/s</td>
              <td style="padding: var(--space-sm); color: var(--text-muted);">GB200 NVL72 Specifications</td>
            </tr>
          </table>
        </div>

        <p class="text-muted text-small" style="margin-top: var(--space-lg);">
          Note: GPU specifications vary by SKU and configuration. Always verify against 
          official NVIDIA datasheets for production use. Memory latencies are approximate 
          and vary by access pattern and workload.
        </p>
      </section>
    </div>

    <script src="../scripts/components.js"></script>
    <script>
      // Hierarchy interaction
      document.querySelectorAll('.hier-item').forEach(item => {
        item.addEventListener('click', () => {
          const detailId = item.getAttribute('data-detail');
          const detail = document.getElementById(detailId);

          // Toggle active state
          const wasActive = item.classList.contains('active');
          document.querySelectorAll('.hier-item').forEach(i => i.classList.remove('active'));
          document.querySelectorAll('.hier-detail').forEach(d => d.classList.remove('visible'));

          if (!wasActive) {
            item.classList.add('active');
            detail.classList.add('visible');
          }
        });
      });

      // Warp visualization
      const warpViz = document.getElementById('warp-viz');
      const warpStatus = document.getElementById('warp-status');

      // Create 32 thread elements
      for (let i = 0; i < 32; i++) {
        const thread = document.createElement('div');
        thread.className = 'thread-viz__thread';
        thread.title = `Thread ${i}`;
        warpViz.appendChild(thread);
      }

      const threads = warpViz.querySelectorAll('.thread-viz__thread');

      document.getElementById('warp-unified').addEventListener('click', () => {
        threads.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            t.classList.remove('diverged');
          }, i * 20);
        });
        warpStatus.textContent = 'All 32 threads executing the same instruction in parallel. Maximum throughput.';
      });

      document.getElementById('warp-diverge').addEventListener('click', () => {
        threads.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            if (i % 2 === 0) {
              t.classList.add('diverged');
            } else {
              t.classList.remove('diverged');
            }
          }, i * 20);
        });
        warpStatus.textContent = 'Branch divergence! Even threads take path A (orange), odd threads take path B (blue). Execution serializes—both paths run sequentially. 50% throughput.';
      });

      document.getElementById('warp-reset').addEventListener('click', () => {
        threads.forEach(t => {
          t.classList.remove('active', 'diverged');
        });
        warpStatus.textContent = 'Click a button to simulate warp execution.';
      });

      // Occupancy calculator
      function calculateOccupancy() {
        const blockSize = parseInt(document.getElementById('occ-block-size').value);
        const regsPerThread = parseInt(document.getElementById('occ-regs').value);
        const smemPerBlock = parseInt(document.getElementById('occ-smem').value) * 1024;

        // Hopper SM limits (simplified)
        const maxWarpsPerSM = 64;
        const maxRegsPerSM = 256 * 1024; // 256KB = 65536 32-bit regs
        const maxSmemPerSM = 228 * 1024;
        const maxBlocksPerSM = 32;

        const warpsPerBlock = blockSize / 32;
        const regsPerBlock = blockSize * regsPerThread;

        // Calculate limits
        const blocksByWarps = Math.floor(maxWarpsPerSM / warpsPerBlock);
        const blocksByRegs = Math.floor(maxRegsPerSM / regsPerBlock);
        const blocksBySmem = smemPerBlock > 0 ? Math.floor(maxSmemPerSM / smemPerBlock) : maxBlocksPerSM;
        const blocksByMax = maxBlocksPerSM;

        const activeBlocks = Math.min(blocksByWarps, blocksByRegs, blocksBySmem, blocksByMax);
        const activeWarps = activeBlocks * warpsPerBlock;
        const occupancy = Math.round((activeWarps / maxWarpsPerSM) * 100);

        document.getElementById('occ-active-warps').textContent = activeWarps;
        document.getElementById('occ-result').textContent = occupancy + '%';
        document.getElementById('occ-bar').style.width = occupancy + '%';
        document.getElementById('occ-bar').textContent = occupancy + '%';
      }

      document.getElementById('occ-block-size').addEventListener('change', calculateOccupancy);
      document.getElementById('occ-regs').addEventListener('change', calculateOccupancy);
      document.getElementById('occ-smem').addEventListener('change', calculateOccupancy);

      calculateOccupancy();
    </script>
  </body>
</html>
