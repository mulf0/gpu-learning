<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPU â†’ NVFP4 Kernels | Full Course</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="base.css" />
  </head>
  <body class="has-sidebar">
    <a href="#main" class="skip-link">Skip to content</a>

    <button class="mobile-btn" aria-label="Menu">
      <svg
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="2"
      >
        <path d="M3 12h18M3 6h18M3 18h18" />
      </svg>
    </button>

    <nav class="sidebar">
      <div class="sidebar__header">
        <div class="sidebar__logo">GPU â†’ NVFP4</div>
        <div class="sidebar__sub">4-Week Intensive</div>
      </div>

      <!-- Week 1: "Why GPUs?" -->
      <div class="nav-section open">
        <button class="nav-section__btn">
          <span class="nav-icon bg-green">1</span>
          Week 1: Why GPUs?
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#week1" class="nav-item">Overview</a>
          <a href="notebooks/week1/00_ready_check.ipynb" class="nav-item">Day 0: Ready Check</a>
          <a href="notebooks/week1/01_numpy_baseline.ipynb" class="nav-item">Day 1: NumPy Baseline</a>
          <a href="notebooks/week1/02_cupy_intro.ipynb" class="nav-item">Day 2: CuPy Intro</a>
          <a href="notebooks/week1/03_gpu_architecture.ipynb" class="nav-item">Day 3: GPU Architecture</a>
          <a href="lessons/gpu-architecture.html" class="nav-item nav-item--sub">Interactive: Warps & Occupancy</a>
          <a href="notebooks/week1/04_first_triton_kernel.ipynb" class="nav-item">Day 4: First Triton Kernel</a>
          <a href="notebooks/week1/05_memory_hierarchy.ipynb" class="nav-item">Day 5: Memory Hierarchy</a>
          <a href="lessons/memory-hierarchy.html" class="nav-item nav-item--sub">Interactive: Coalescing & Banks</a>
          <a href="notebooks/week1/06_tiling_basics.ipynb" class="nav-item">Day 6: Tiling Basics</a>
          <a href="notebooks/week1/07_fast_matmul.ipynb" class="nav-item">Day 7: Fast Matmul</a>
        </div>
      </div>

      <!-- Week 2: "The Memory Game" -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-purple">2</span>
          Week 2: Memory Game
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#week2" class="nav-item">Overview</a>
          <a href="#w2-profiling" class="nav-item">Day 1: Profile Like a Pro</a>
          <a href="#w2-coalescing" class="nav-item">Day 2: Coalescing Experiments</a>
          <a href="#w2-banks" class="nav-item">Day 3: Bank Conflict Lab</a>
          <a href="#w2-pipelining" class="nav-item">Day 4: Pipelining</a>
          <a href="#w2-tma" class="nav-item">Day 5: TMA (Hopper+)</a>
          <a href="#w2-tensorcores" class="nav-item">Day 6: Tensor Cores</a>
          <a href="#w2-gemm" class="nav-item">Day 7: 80% Peak GEMM</a>
        </div>
      </div>

      <!-- Week 3: "Numbers & Attention" -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-orange">3</span>
          Week 3: Attention
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#week3" class="nav-item">Overview</a>
          <a href="#w3-dotproduct" class="nav-item">Day 1: Dot Product</a>
          <a href="#w3-softmax" class="nav-item">Day 2: Softmax Problem</a>
          <a href="attention-math.html" class="nav-item nav-item--sub">Interactive: Softmax Viz</a>
          <a href="#w3-stable" class="nav-item">Day 3: Stable Softmax</a>
          <a href="#w3-attention" class="nav-item">Day 4: Full Attention</a>
          <a href="#w3-online" class="nav-item">Day 5: Online Softmax</a>
          <a href="#w3-tiled" class="nav-item">Day 6: Tiled Attention</a>
          <a href="#w3-flash" class="nav-item">Day 7: FlashAttention</a>
        </div>
      </div>

      <!-- Week 4: "Compression & Production" -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-cyan">4</span>
          Week 4: Production
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#week4" class="nav-item">Overview</a>
          <a href="#w4-fp8" class="nav-item">Day 1: FP16 â†’ FP8</a>
          <a href="#w4-quantize" class="nav-item">Day 2: Quantization</a>
          <a href="#w4-int4" class="nav-item">Day 3: INT8/INT4</a>
          <a href="#w4-nvfp4" class="nav-item">Day 4: NVFP4</a>
          <a href="#w4-kvcache" class="nav-item">Day 5: KV Cache</a>
          <a href="#w4-fused" class="nav-item">Day 6: Fused Attention</a>
          <a href="#w4-integration" class="nav-item">Day 7: Production</a>
        </div>
      </div>

      <!-- Reference Materials -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-yellow">R</span>
          Reference
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="lessons/gpu-architecture.html" class="nav-item">GPU Architecture</a>
          <a href="lessons/memory-hierarchy.html" class="nav-item">Memory Hierarchy</a>
          <a href="attention-math.html" class="nav-item">Attention Math</a>
          <a href="math-prerequisites.html" class="nav-item">Math Self-Check</a>
        </div>
      </div>

      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-pink">R</span>
          Resources
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#reading" class="nav-item">Reading List</a>
          <a href="#checklist" class="nav-item">Knowledge Check</a>
        </div>
      </div>
    </nav>

    <main id="main" class="main-content">
      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     WEEK 1: "WHY GPUs?" â€” FROM SLOW TO FAST
     Concepts INTERLEAVED: GPU architecture (Day 3), Memory hierarchy (Day 5)
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

      <section class="section" id="week1">
        <div class="section__head">
          <div class="section__num bg-green">W1</div>
          <div>
            <h2>Week 1: "Why GPUs?" â€” From Slow to Fast</h2>
            <p class="mb-0 text-muted">Matrix multiplication journey: 1 GFLOPS â†’ 1000+ GFLOPS</p>
          </div>
        </div>

        <p>
          <strong>No separate prerequisites.</strong> Concepts introduced exactly when you need them
          to solve the next problem. Start with familiar Python, end with fast GPU kernels.
        </p>

        <div class="callout info">
          <div class="callout-title">Daily Structure (~1 hour each)</div>
          <p class="mb-0">
            <strong>Challenge</strong> (5 min) â†’ <strong>Explore</strong> (15 min) â†’
            <strong>Concept</strong> (10 min) â†’ <strong>Code It</strong> (30 min) â†’
            <strong>Verify</strong> (10 min)
          </p>
        </div>

        <!-- Days 0-2: Python baseline -->
        <h3 class="mt-xl">Days 0-2: Establish Baselines</h3>
        <div class="grid grid-3">
          <a href="notebooks/week1/00_ready_check.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 0: Ready Check</h4>
            <p class="mb-0 text-muted">Environment + math diagnostic</p>
          </a>
          <a href="notebooks/week1/01_numpy_baseline.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 1: NumPy</h4>
            <p class="mb-0 text-muted">CPU baseline, GFLOPS</p>
          </a>
          <a href="notebooks/week1/02_cupy_intro.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 2: CuPy</h4>
            <p class="mb-0 text-muted">50-100x GPU speedup</p>
          </a>
        </div>

        <!-- Day 3: GPU Architecture EMBEDDED -->
        <h3 class="mt-xl">Day 3: How GPUs Actually Work</h3>
        <p>
          CuPy is fast, but <em>why</em>? To write our own fast kernels, we need to understand
          how GPUs execute code. Today we learn: SMs, warps, and the SIMT model.
        </p>

        <div class="grid grid-2">
          <a href="notebooks/week1/03_gpu_architecture.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 3 Notebook</h4>
            <p>Hands-on exploration of GPU hierarchy with Python simulations.</p>
            <p class="mb-0 text-blue"><strong>Open Notebook â†’</strong></p>
          </a>
          <a href="lessons/gpu-architecture.html" class="card" style="display: block; text-decoration: none; margin: 0; border-left: 4px solid var(--accent-purple);">
            <h4>Interactive: Warp Execution</h4>
            <p>Visualize warp divergence, use the occupancy calculator.</p>
            <p class="mb-0 text-purple"><strong>Try Interactive â†’</strong></p>
          </a>
        </div>

        <div class="callout info mt-lg">
          <div class="callout-title">Key Concepts (Day 3)</div>
          <p class="mb-0">
            <strong>Warp:</strong> 32 threads in lockstep (SIMT) â€¢
            <strong>Block:</strong> Warps sharing SMEM + sync â€¢
            <strong>Grid:</strong> Problem decomposition â€¢
            <strong>Divergence:</strong> Kills performance
          </p>
        </div>

        <!-- Day 4: First Triton Kernel -->
        <h3 class="mt-xl">Day 4: Your First GPU Kernel</h3>
        <a href="notebooks/week1/04_first_triton_kernel.ipynb" class="card" style="display: block; text-decoration: none;">
          <h4>Day 4: First Triton Kernel</h4>
          <p>Write vector addition in Triton. Learn index arithmetic, program IDs, and blocks.</p>
          <p class="mb-0 text-blue"><strong>Open Notebook â†’</strong></p>
        </a>

        <!-- Day 5: Memory Hierarchy EMBEDDED -->
        <h3 class="mt-xl">Day 5: Why Naive Kernels Are Slow</h3>
        <p>
          Your kernel works but it's 10-50x slower than CuPy. Why? The answer is
          <strong>memory bandwidth</strong>. Today we learn the memory hierarchy and why
          access patterns determine performance.
        </p>

        <div class="grid grid-2">
          <a href="notebooks/week1/05_memory_hierarchy.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 5 Notebook</h4>
            <p>Measure coalescing impact, understand arithmetic intensity.</p>
            <p class="mb-0 text-blue"><strong>Open Notebook â†’</strong></p>
          </a>
          <a href="lessons/memory-hierarchy.html" class="card" style="display: block; text-decoration: none; margin: 0; border-left: 4px solid var(--accent-purple);">
            <h4>Interactive: Coalescing & Banks</h4>
            <p>Visualize memory access patterns, bank conflicts, bandwidth.</p>
            <p class="mb-0 text-purple"><strong>Try Interactive â†’</strong></p>
          </a>
        </div>

        <div class="diagram mt-lg">
          <div class="diagram-title">Memory Hierarchy Latency</div>
          <div class="mem-bars">
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">Registers</span>
                <span class="mem-bar-stats">~1 cycle</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-green" style="--w: 100%">Fastest</div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">Shared Memory</span>
                <span class="mem-bar-stats">~20 cycles</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-blue" style="--w: 85%">Fast</div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">HBM (Global)</span>
                <span class="mem-bar-stats">~400 cycles</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-orange" style="--w: 25%">Slow</div>
              </div>
            </div>
          </div>
        </div>

        <!-- Days 6-7: Tiling and Fast Matmul -->
        <h3 class="mt-xl">Days 6-7: The Solution â€” Tiling</h3>
        <p>
          <strong>Data reuse</strong> is the key. Load data once from slow HBM, reuse many times
          from fast shared memory. By Day 7, achieve 500+ GFLOPS.
        </p>

        <div class="grid grid-2">
          <a href="notebooks/week1/06_tiling_basics.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 6: Tiling Basics</h4>
            <p class="mb-0 text-muted">Tile coordinates, shared memory, 2D indexing</p>
          </a>
          <a href="notebooks/week1/07_fast_matmul.ipynb" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Day 7: Fast Matmul</h4>
            <p class="mb-0 text-muted">Tiled matmul kernel achieving 500+ GFLOPS</p>
          </a>
        </div>

        <div class="insight mt-lg">
          <div class="insight-icon">ðŸŽ¯</div>
          <div>
            <h4>Week 1 Milestone</h4>
            <p>
              By end of Day 7: You've gone from naive Python (0.001 GFLOPS) to a tiled GPU
              kernel (500+ GFLOPS) â€” <strong>500,000x improvement</strong>. You understand
              WHY each optimization matters because you built it yourself.
            </p>
          </div>
        </div>
      </section>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     WEEK 2: "THE MEMORY GAME" â€” OPTIMIZATION DEEP DIVE
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

      <section class="section" id="week2">
        <div class="section__head">
          <div class="section__num bg-purple">W2</div>
          <div>
            <h2>Week 2: "The Memory Game" â€” Optimization Deep Dive</h2>
            <p class="mb-0 text-muted">Push matmul to 80%+ of theoretical peak</p>
          </div>
        </div>

        <p>
          Week 1 got us to 500 GFLOPS. Week 2 pushes to 80%+ of peak. We'll profile,
          experiment, and master: coalescing, bank conflicts, pipelining, TMA, and Tensor Cores.
        </p>

        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4>Days 1-3: Profiling & Experiments</h4>
            <ul class="mb-0">
              <li>Day 1: Nsight Compute profiling</li>
              <li>Day 2: Coalescing experiments</li>
              <li>Day 3: Bank conflict laboratory</li>
            </ul>
          </div>
          <div class="card" style="margin: 0">
            <h4>Days 4-7: Advanced Techniques</h4>
            <ul class="mb-0">
              <li>Day 4: Software pipelining</li>
              <li>Day 5: TMA (Hopper+)</li>
              <li>Day 6: Tensor Cores</li>
              <li>Day 7: Production GEMM</li>
            </ul>
          </div>
        </div>

        <a href="lessons/memory-hierarchy.html" class="card mt-lg" style="display: block; text-decoration: none;">
          <h4>Interactive Reference: Memory Hierarchy Deep Dive</h4>
          <p>Coalescing visualization, bank conflict simulator, TMA diagrams, bandwidth calculator.</p>
          <p class="mb-0 text-purple"><strong>Open Interactive â†’</strong></p>
        </a>
      </section>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     WEEK 3: "NUMBERS & ATTENTION" â€” PRECISION MEETS PERFORMANCE
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

      <section class="section" id="week3">
        <div class="section__head">
          <div class="section__num bg-orange">W3</div>
          <div>
            <h2>Week 3: "Numbers & Attention" â€” Build FlashAttention</h2>
            <p class="mb-0 text-muted">Floating point precision, softmax stability, online algorithms</p>
          </div>
        </div>

        <p>
          Attention is THE bottleneck in LLMs. This week: understand the math deeply,
          then build FlashAttention from scratch. Math concepts introduced when needed.
        </p>

        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4>Days 1-3: The Math</h4>
            <ul class="mb-0">
              <li>Day 1: Dot product as similarity</li>
              <li>Day 2: Softmax overflow problem</li>
              <li>Day 3: Stable softmax (max-subtraction)</li>
            </ul>
          </div>
          <div class="card" style="margin: 0">
            <h4>Days 4-7: FlashAttention</h4>
            <ul class="mb-0">
              <li>Day 4: Full attention formula</li>
              <li>Day 5: Online softmax algorithm</li>
              <li>Day 6: Tiled attention</li>
              <li>Day 7: Complete FlashAttention</li>
            </ul>
          </div>
        </div>

        <a href="attention-math.html" class="card mt-lg" style="display: block; text-decoration: none;">
          <h4>Interactive Reference: Attention Math</h4>
          <p>Dot product calculator, softmax visualization, online softmax simulation, FP format explorer.</p>
          <p class="mb-0 text-purple"><strong>Open Interactive â†’</strong></p>
        </a>
      </section>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     WEEK 4: "COMPRESSION & PRODUCTION" â€” REAL-WORLD DEPLOYMENT
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

      <section class="section" id="week4">
        <div class="section__head">
          <div class="section__num bg-cyan">W4</div>
          <div>
            <h2>Week 4: "Compression & Production" â€” Mixed-Precision KV Cache</h2>
            <p class="mb-0 text-muted">FP8, NVFP4, quantization, production integration</p>
          </div>
        </div>

        <p>
          Final project: Mixed-precision KV cache kernel (K=FP8, V=NVFP4). Achieve ~2.6x
          memory reduction with quality preserved. Integrate with real inference frameworks.
        </p>

        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4>Days 1-4: Quantization</h4>
            <ul class="mb-0">
              <li>Day 1: FP16 â†’ FP8</li>
              <li>Day 2: Quantization fundamentals</li>
              <li>Day 3: INT8/INT4</li>
              <li>Day 4: NVFP4 two-level scaling</li>
            </ul>
          </div>
          <div class="card" style="margin: 0">
            <h4>Days 5-7: Integration</h4>
            <ul class="mb-0">
              <li>Day 5: KV cache strategy</li>
              <li>Day 6: Fused quantized attention</li>
              <li>Day 7: Production integration</li>
            </ul>
          </div>
        </div>

        <div class="insight mt-lg">
          <div class="insight-icon">ðŸŽ¯</div>
          <div>
            <h4>Final Project Target</h4>
            <p>
              FP8 K + NVFP4 V: <strong>2.6Ã— memory reduction</strong> with quality preserved.
              Working integration with TensorRT-LLM or vLLM.
            </p>
          </div>
        </div>
      </section>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     REFERENCE: DETAILED FOUNDATIONS (kept for deep dives)
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

      <section class="section" id="reference">
        <div class="section__head">
          <div class="section__num bg-yellow">REF</div>
          <div>
            <h2>Reference Materials</h2>
            <p class="mb-0 text-muted">Deep dives for when you need more detail</p>
          </div>
        </div>

        <p>
          These materials provide deeper coverage than the daily notebooks. Use them when
          you want to go beyond the essentials or need a refresher on specific topics.
        </p>

        <div class="grid grid-2">
          <a href="lessons/gpu-architecture.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>GPU Architecture</h4>
            <p class="mb-0 text-muted">SMs, warps, warpgroups, occupancy calculator</p>
          </a>
          <a href="lessons/memory-hierarchy.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Memory Hierarchy</h4>
            <p class="mb-0 text-muted">Coalescing, bank conflicts, TMA, bandwidth</p>
          </a>
          <a href="attention-math.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Attention Math</h4>
            <p class="mb-0 text-muted">Dot products, softmax, online algorithms, FP formats</p>
          </a>
          <a href="math-prerequisites.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Math Self-Assessment</h4>
            <p class="mb-0 text-muted">Diagnostic quiz, learning resources</p>
          </a>
        </div>
      </section>

      <!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
     RESOURCES
     â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->

      <section class="section" id="reading">
        <div class="section__head">
          <div class="section__num bg-pink">R</div>
          <div>
            <h2>Reading List</h2>
            <p class="mb-0 text-muted">Ordered by week</p>
          </div>
        </div>

        <h3>Week 1: GPU Fundamentals</h3>
        <div class="resources">
          <a
            href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CUDA Programming Guide â†’</h4>
            <p>Chapters 1-5</p></a
          >
          <a
            href="https://docs.nvidia.com/cutlass/latest/media/docs/pythonDSL/overview.html"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CuTe DSL Docs â†’</h4>
            <p>Official documentation</p></a
          >
          <a
            href="https://github.com/NVIDIA/cutlass"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CUTLASS GitHub â†’</h4>
            <p>Examples and source</p></a
          >
        </div>

        <h3>Week 2: Tensor Cores & CuTe</h3>
        <div class="resources">
          <a
            href="https://docs.nvidia.com/cutlass/latest/media/docs/cpp/cute/index.html"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CuTe C++ Docs â†’</h4>
            <p>Concepts apply to DSL</p></a
          >
          <a
            href="https://arxiv.org/abs/1804.06826"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>Volta Microbenchmarking â†’</h4>
            <p>Jia et al. paper</p></a
          >
        </div>

        <h3>Week 3: Quantization</h3>
        <div class="resources">
          <a
            href="https://arxiv.org/abs/2208.07339"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>LLM.int8() â†’</h4>
            <p>Dettmers et al.</p></a
          >
          <a
            href="https://arxiv.org/abs/2211.10438"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>SmoothQuant â†’</h4>
            <p>Xiao et al.</p></a
          >
          <a
            href="https://arxiv.org/abs/2306.00978"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>AWQ â†’</h4>
            <p>Lin et al.</p></a
          >
          <a
            href="https://arxiv.org/abs/2205.14135"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>FlashAttention â†’</h4>
            <p>Dao et al.</p></a
          >
        </div>

        <h3>Week 4: Systems</h3>
        <div class="resources">
          <a
            href="https://github.com/NVIDIA/TensorRT-LLM"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>TensorRT-LLM â†’</h4>
            <p>Source code</p></a
          >
          <a
            href="https://github.com/vllm-project/vllm"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>vLLM â†’</h4>
            <p>Architecture docs</p></a
          >
          <a
            href="https://arxiv.org/abs/2211.05102"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>Scaling Inference â†’</h4>
            <p>Pope et al.</p></a
          >
        </div>
      </section>

      <section class="section" id="checklist">
        <div class="section__head">
          <div class="section__num bg-red">?</div>
          <div>
            <h2>Knowledge Check</h2>
            <p class="mb-0 text-muted">Can you answer these confidently?</p>
          </div>
        </div>

        <ul>
          <li>Why does coalesced memory access matter?</li>
          <li>What determines Tensor Core throughput vs memory throughput?</li>
          <li>Why does block scaling reduce quantization error?</li>
          <li>Why is decode memory-bound while prefill is compute-bound?</li>
          <li>What does TMA buy you over manual loads?</li>
          <li>Why can V tolerate more quantization error than K?</li>
          <li>How does software pipelining hide memory latency?</li>
          <li>What's the NVFP4 two-level scaling mechanism?</li>
        </ul>
      </section>

      <!-- Technical References -->
      <section class="section" id="technical-references">
        <div class="section__head">
          <div class="section__num bg-blue">R</div>
          <div>
            <h2>Technical References</h2>
            <p class="mb-0 text-muted">Sources for specifications and claims</p>
          </div>
        </div>

        <h3>Video Resources</h3>
        <p>These videos provide excellent visual explanations of the concepts covered in this course:</p>

        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4>CUDA Programming Course</h4>
            <p>Comprehensive CUDA programming course covering GPU architecture and kernel development.</p>
            <a href="https://www.youtube.com/watch?v=86FAWCzIe_4" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube (freeCodeCamp)</strong></a>
          </div>
          <div class="card" style="margin: 0">
            <h4>How to Profile CUDA Kernels in PyTorch</h4>
            <p>Practical guide to profiling and optimizing CUDA kernels with Nsight tools.</p>
            <a href="https://www.youtube.com/watch?v=LuhJEEJQgUM" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube</strong></a>
          </div>
        </div>

        <div class="grid grid-2 mt-lg">
          <div class="card" style="margin: 0">
            <h4>How GPU Computing Works</h4>
            <p>Excellent visual explanation of GPU architecture fundamentals and parallelism.</p>
            <a href="https://www.youtube.com/watch?v=h9Z4oGN89MU" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube (Branch Education)</strong></a>
          </div>
          <div class="card" style="margin: 0">
            <h4>FlashAttention Explained</h4>
            <p>Tri Dao explains FlashAttention's IO-aware algorithm and tiling strategy.</p>
            <a href="https://www.youtube.com/watch?v=gMOAud7hZg4" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube (Stanford MLSys)</strong></a>
          </div>
        </div>

        <h3 class="mt-lg">NVIDIA Architecture Documentation</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Topic</th>
              <th>Source</th>
              <th>Link</th>
            </tr>
            <tr>
              <td>CUDA execution model, warps</td>
              <td>CUDA C++ Programming Guide</td>
              <td><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank" rel="noopener">docs.nvidia.com</a></td>
            </tr>
            <tr>
              <td>Hopper SM specs, warpgroups</td>
              <td>Hopper Architecture Whitepaper</td>
              <td><a href="https://resources.nvidia.com/en-us-hopper-architecture" target="_blank" rel="noopener">resources.nvidia.com</a></td>
            </tr>
            <tr>
              <td>H100 specifications</td>
              <td>H100 Tensor Core GPU Datasheet</td>
              <td><a href="https://resources.nvidia.com/en-us-hopper-architecture/nvidia-tensor-core-gpu-datasheet" target="_blank" rel="noopener">H100 Datasheet</a></td>
            </tr>
            <tr>
              <td>Blackwell, NVFP4, FP4/FP6</td>
              <td>Blackwell Architecture Overview</td>
              <td><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">nvidia.com</a></td>
            </tr>
            <tr>
              <td>CuTe DSL, CUTLASS</td>
              <td>CUTLASS Documentation</td>
              <td><a href="https://github.com/NVIDIA/cutlass" target="_blank" rel="noopener">github.com/NVIDIA/cutlass</a></td>
            </tr>
          </table>
        </div>

        <h3>Floating Point & Quantization Standards</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Format</th>
              <th>Specification</th>
              <th>Link</th>
            </tr>
            <tr>
              <td>FP32, FP16</td>
              <td>IEEE 754-2019</td>
              <td><a href="https://ieeexplore.ieee.org/document/8766229" target="_blank" rel="noopener">IEEE Xplore</a></td>
            </tr>
            <tr>
              <td>FP8 (E4M3, E5M2)</td>
              <td>FP8 Formats for Deep Learning</td>
              <td><a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener">arXiv:2209.05433</a></td>
            </tr>
            <tr>
              <td>MXFP4, MXFP6, MXFP8</td>
              <td>OCP Microscaling (MX) Spec v1.0</td>
              <td><a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf" target="_blank" rel="noopener">opencompute.org</a></td>
            </tr>
          </table>
        </div>

        <h3>Key Research Papers</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Paper</th>
              <th>Authors</th>
              <th>Link</th>
            </tr>
            <tr>
              <td>Attention Is All You Need</td>
              <td>Vaswani et al., 2017</td>
              <td><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">arXiv:1706.03762</a></td>
            </tr>
            <tr>
              <td>FlashAttention</td>
              <td>Dao et al., 2022</td>
              <td><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">arXiv:2205.14135</a></td>
            </tr>
            <tr>
              <td>FlashAttention-2</td>
              <td>Dao, 2023</td>
              <td><a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener">arXiv:2307.08691</a></td>
            </tr>
            <tr>
              <td>LLM.int8()</td>
              <td>Dettmers et al., 2022</td>
              <td><a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="noopener">arXiv:2208.07339</a></td>
            </tr>
            <tr>
              <td>SmoothQuant</td>
              <td>Xiao et al., 2022</td>
              <td><a href="https://arxiv.org/abs/2211.10438" target="_blank" rel="noopener">arXiv:2211.10438</a></td>
            </tr>
            <tr>
              <td>AWQ</td>
              <td>Lin et al., 2023</td>
              <td><a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener">arXiv:2306.00978</a></td>
            </tr>
            <tr>
              <td>Volta Microbenchmarking</td>
              <td>Jia et al., 2018</td>
              <td><a href="https://arxiv.org/abs/1804.06826" target="_blank" rel="noopener">arXiv:1804.06826</a></td>
            </tr>
          </table>
        </div>

        <h3>Specifications Cited in This Course</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Claim</th>
              <th>Value</th>
              <th>Source</th>
            </tr>
            <tr>
              <td>Warp size</td>
              <td>32 threads</td>
              <td>CUDA Programming Guide, Ch. 4</td>
            </tr>
            <tr>
              <td>Warpgroup size (Hopper+)</td>
              <td>128 threads (4 warps)</td>
              <td>Hopper Architecture Whitepaper</td>
            </tr>
            <tr>
              <td>Blackwell B200 SMEM</td>
              <td>228 KB/SM</td>
              <td>Blackwell Architecture Overview</td>
            </tr>
            <tr>
              <td>Blackwell B200 HBM</td>
              <td>192 GB</td>
              <td>GB200 NVL72 Specifications</td>
            </tr>
            <tr>
              <td>Blackwell B200 HBM bandwidth</td>
              <td>~8 TB/s</td>
              <td>GB200 NVL72 Specifications</td>
            </tr>
            <tr>
              <td>SMEM bank count</td>
              <td>32 banks</td>
              <td>CUDA Programming Guide, Ch. 5.3.2</td>
            </tr>
            <tr>
              <td>Volta WMMA shape</td>
              <td>16x16x16 (FP16)</td>
              <td>CUDA WMMA API Documentation</td>
            </tr>
            <tr>
              <td>FP8 E4M3 range</td>
              <td>Â±448</td>
              <td>arXiv:2209.05433, Table 1</td>
            </tr>
            <tr>
              <td>FP4 E2M1 range</td>
              <td>Â±6</td>
              <td>OCP MX Spec v1.0, Section 5</td>
            </tr>
            <tr>
              <td>NVFP4 block size</td>
              <td>16 elements</td>
              <td>NVIDIA Blackwell Documentation</td>
            </tr>
            <tr>
              <td>MXFP4 block size</td>
              <td>32 elements</td>
              <td>OCP MX Spec v1.0, Section 4</td>
            </tr>
            <tr>
              <td>NVFP4 scale format</td>
              <td>E4M3 (fractional)</td>
              <td>NVIDIA Blackwell Documentation</td>
            </tr>
            <tr>
              <td>MXFP4 scale format</td>
              <td>E8M0 (power-of-2)</td>
              <td>OCP MX Spec v1.0, Section 4</td>
            </tr>
            <tr>
              <td>128K attention matrix size</td>
              <td>~16 billion elements/head</td>
              <td>128K Ã— 128K = 16.4B (calculation)</td>
            </tr>
            <tr>
              <td>FP8 KV cache compression</td>
              <td>2.0x vs FP16</td>
              <td>16-bit â†’ 8-bit (calculation)</td>
            </tr>
            <tr>
              <td>Mixed K(FP8)/V(NVFP4) compression</td>
              <td>~2.6x vs FP16</td>
              <td>Average of 2x (K) and 4x (V), weighted</td>
            </tr>
          </table>
        </div>

        <div class="callout warn mt-lg">
          <div class="callout-title">Note on Latency Values</div>
          <p class="mb-0">
            Memory latency figures (cycles) are approximate and vary significantly based on access pattern,
            cache hit rate, memory contention, and specific workload characteristics. The values shown
            represent typical measured ranges from microbenchmarks, not guaranteed specifications.
            Bandwidth numbers represent theoretical peak values. Always profile your specific kernel
            to understand actual performance characteristics.
          </p>
        </div>

        <div class="callout info mt-lg">
          <div class="callout-title">Additional Resources</div>
          <p class="mb-0">
            For the latest specifications, consult the official
            <a href="https://docs.nvidia.com/cuda/" target="_blank" rel="noopener">NVIDIA CUDA Documentation</a> and
            <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">GPU Compute Capability tables</a>.
            Architecture-specific details are available in the respective whitepapers linked above.
          </p>
        </div>
      </section>
    </main>

    <script src="scripts/components.js"></script>
  </body>
</html>
