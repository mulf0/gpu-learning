<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPU Kernel Mastery | From Python to Production</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="base.css" />
    <style>
      /* Code blocks */
      .code-block {
        background: var(--bg-tertiary);
        border-radius: var(--radius-md);
        padding: var(--space-md);
        overflow-x: auto;
        font-family: 'IBM Plex Mono', monospace;
        font-size: 0.85rem;
        line-height: 1.5;
        margin: var(--space-md) 0;
      }
      .code-block code {
        color: var(--text-primary);
      }
      .code-block .comment { color: var(--text-muted); }
      .code-block .keyword { color: var(--accent-purple); }
      .code-block .string { color: var(--accent-green); }
      .code-block .number { color: var(--accent-orange); }
      .code-block .function { color: var(--accent-blue); }

      /* Interactive containers */
      .interactive {
        background: var(--bg-card);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-lg);
        padding: var(--space-lg);
        margin: var(--space-lg) 0;
      }
      .interactive__title {
        display: flex;
        align-items: center;
        gap: var(--space-sm);
        font-weight: 600;
        margin-bottom: var(--space-md);
        color: var(--accent-purple);
      }
      .interactive__title::before {
        content: '';
        width: 8px;
        height: 8px;
        background: var(--accent-purple);
        border-radius: 50%;
      }

      /* Thread visualization */
      .thread-viz {
        display: grid;
        grid-template-columns: repeat(16, 1fr);
        gap: 3px;
        margin: var(--space-md) 0;
      }
      .thread-viz__thread {
        aspect-ratio: 1;
        background: var(--bg-tertiary);
        border-radius: var(--radius-sm);
        transition: all 0.2s;
      }
      .thread-viz__thread.active {
        background: var(--accent-blue);
      }
      .thread-viz__thread.diverged {
        background: var(--accent-orange);
      }

      /* Coalescing visualization */
      .coal-viz {
        display: flex;
        flex-direction: column;
        gap: var(--space-md);
      }
      .coal-viz__row {
        display: flex;
        align-items: center;
        gap: var(--space-sm);
      }
      .coal-viz__label {
        width: 80px;
        font-size: 0.75rem;
        color: var(--text-muted);
      }
      .coal-viz__cells {
        display: flex;
        gap: 2px;
        flex: 1;
      }
      .coal-viz__cell {
        width: 28px;
        height: 28px;
        display: flex;
        align-items: center;
        justify-content: center;
        background: var(--bg-tertiary);
        border-radius: var(--radius-sm);
        font-size: 0.7rem;
        transition: all 0.2s;
      }
      .coal-viz__cell.active { background: var(--accent-blue); color: white; }
      .coal-viz__cell.accessed { background: var(--accent-green); color: white; }
      .coal-viz__cell.conflict { background: var(--accent-orange); color: white; }

      /* Bank visualization */
      .bank-viz {
        display: flex;
        gap: var(--space-sm);
        margin: var(--space-md) 0;
        overflow-x: auto;
        padding-bottom: var(--space-sm);
      }
      .bank-viz__bank {
        display: flex;
        flex-direction: column;
        gap: 2px;
        min-width: 40px;
      }
      .bank-viz__label {
        font-size: 0.65rem;
        text-align: center;
        color: var(--text-muted);
        padding: var(--space-xs);
      }
      .bank-viz__slot {
        height: 28px;
        display: flex;
        align-items: center;
        justify-content: center;
        background: var(--bg-tertiary);
        border-radius: var(--radius-sm);
        font-size: 0.7rem;
        transition: all 0.2s;
      }
      .bank-viz__slot.accessed { background: var(--accent-green); color: white; }
      .bank-viz__slot.conflict { background: var(--accent-orange); color: white; }

      /* Result display */
      .result-box {
        background: var(--bg-tertiary);
        border-radius: var(--radius-sm);
        padding: var(--space-md);
        margin-top: var(--space-md);
        font-size: 0.9rem;
      }

      /* Lab callout */
      .lab-callout {
        background: linear-gradient(135deg, var(--bg-card) 0%, rgba(99, 102, 241, 0.1) 100%);
        border: 2px solid var(--accent-purple);
        border-radius: var(--radius-lg);
        padding: var(--space-xl);
        margin: var(--space-xl) 0;
        text-align: center;
      }
      .lab-callout h3 {
        color: var(--accent-purple);
        margin-bottom: var(--space-sm);
      }
      .lab-callout__links {
        display: flex;
        flex-wrap: wrap;
        gap: var(--space-sm);
        justify-content: center;
        margin-top: var(--space-md);
      }
      .lab-callout__links a {
        background: var(--bg-tertiary);
        padding: var(--space-sm) var(--space-md);
        border-radius: var(--radius-md);
        text-decoration: none;
        color: var(--text-primary);
        font-size: 0.85rem;
        transition: all 0.2s;
      }
      .lab-callout__links a:hover {
        background: var(--accent-purple);
        color: white;
      }

      /* Occupancy calculator */
      .occ-calc {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: var(--space-md);
      }
      .occ-calc__row {
        display: flex;
        justify-content: space-between;
        align-items: center;
        padding: var(--space-sm) 0;
        border-bottom: 1px solid var(--border-subtle);
      }
      .occ-calc__row:last-child { border-bottom: none; }
      .occ-bar {
        height: 24px;
        background: var(--bg-tertiary);
        border-radius: var(--radius-sm);
        overflow: hidden;
        margin-top: var(--space-md);
      }
      .occ-bar__fill {
        height: 100%;
        background: var(--accent-green);
        display: flex;
        align-items: center;
        justify-content: center;
        color: white;
        font-size: 0.75rem;
        font-weight: 600;
        transition: width 0.3s;
      }
    </style>
  </head>
  <body class="has-sidebar">
    <a href="#main" class="skip-link">Skip to content</a>

    <button class="mobile-btn" aria-label="Menu">
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
        <path d="M3 12h18M3 6h18M3 18h18" />
      </svg>
    </button>

    <nav class="sidebar">
      <div class="sidebar__header">
        <div class="sidebar__logo">GPU Kernel Mastery</div>
        <div class="sidebar__sub">From Python to Production</div>
      </div>

      <!-- Part 1 -->
      <div class="nav-section open">
        <button class="nav-section__btn">
          <span class="nav-icon bg-green">1</span>
          From Python to GPU
          <svg class="nav-chevron" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#part1" class="nav-item">Overview</a>
          <a href="#p1-challenge" class="nav-item">The Challenge</a>
          <a href="#p1-cupy" class="nav-item">CuPy Magic</a>
          <a href="#p1-architecture" class="nav-item">GPU Architecture</a>
          <a href="#p1-kernel" class="nav-item">First Kernel</a>
          <a href="#p1-memory" class="nav-item">Memory Hierarchy</a>
          <a href="#p1-tiling" class="nav-item">Tiling</a>
          <a href="#p1-lab" class="nav-item nav-item--sub">Practice Labs</a>
        </div>
      </div>

      <!-- Part 2 -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-purple">2</span>
          Optimization Mastery
          <svg class="nav-chevron" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#part2" class="nav-item">Overview</a>
          <a href="#p2-profiling" class="nav-item">Profiling</a>
          <a href="#p2-coalescing" class="nav-item">Coalescing</a>
          <a href="#p2-banks" class="nav-item">Bank Conflicts</a>
          <a href="#p2-tensorcores" class="nav-item">Tensor Cores</a>
          <a href="#p2-lab" class="nav-item nav-item--sub">Practice Labs</a>
        </div>
      </div>

      <!-- Part 3 -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-orange">3</span>
          FlashAttention
          <svg class="nav-chevron" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#part3" class="nav-item">Overview</a>
          <a href="#p3-dotproduct" class="nav-item">Dot Products</a>
          <a href="#p3-softmax" class="nav-item">Softmax</a>
          <a href="#p3-online" class="nav-item">Online Softmax</a>
          <a href="#p3-flash" class="nav-item">FlashAttention</a>
          <a href="#p3-lab" class="nav-item nav-item--sub">Practice Labs</a>
        </div>
      </div>

      <!-- Part 4 -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-cyan">4</span>
          Production
          <svg class="nav-chevron" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#part4" class="nav-item">Overview</a>
          <a href="#p4-fp8" class="nav-item">FP8 Formats</a>
          <a href="#p4-quantization" class="nav-item">Quantization</a>
          <a href="#p4-nvfp4" class="nav-item">NVFP4</a>
          <a href="#p4-kvcache" class="nav-item">KV Cache</a>
          <a href="#p4-lab" class="nav-item nav-item--sub">Practice Labs</a>
        </div>
      </div>

      <!-- Reference -->
      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-yellow">R</span>
          Reference
          <svg class="nav-chevron" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="lessons/gpu-architecture.html" class="nav-item">GPU Architecture Deep Dive</a>
          <a href="lessons/memory-hierarchy.html" class="nav-item">Memory Hierarchy Deep Dive</a>
          <a href="attention-math.html" class="nav-item">Attention Math Deep Dive</a>
          <a href="lessons/math-foundations.html" class="nav-item">Math Foundations</a>
        </div>
      </div>
    </nav>

    <main id="main" class="main-content">

      <!-- ═══════════════════════════════════════════════════════════════════════
           PART 1: FROM PYTHON TO GPU MASTERY
           ═══════════════════════════════════════════════════════════════════════ -->

      <section class="section" id="part1">
        <div class="section__head">
          <div class="section__num bg-green">1</div>
          <div>
            <h2>From Python to GPU Mastery</h2>
            <p class="mb-0 text-muted">Your journey: 0.001 GFLOPS to 500+ GFLOPS</p>
          </div>
        </div>

        <p>
          You start with slow Python. By the end of this part, you'll have achieved a
          <strong>500,000x speedup</strong> and understand exactly why each optimization matters.
          No separate prerequisites—concepts are introduced exactly when you need them.
        </p>
      </section>

      <!-- The Challenge -->
      <section class="section" id="p1-challenge">
        <h3>The Challenge: Your Code is Slow</h3>

        <p>
          Let's establish a baseline. Here's matrix multiplication in pure Python with NumPy:
        </p>

        <div class="code-block">
<code><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> time

<span class="comment"># Two 1024x1024 matrices</span>
A = np.<span class="function">random.randn</span>(<span class="number">1024</span>, <span class="number">1024</span>).astype(np.float32)
B = np.<span class="function">random.randn</span>(<span class="number">1024</span>, <span class="number">1024</span>).astype(np.float32)

<span class="comment"># Time it</span>
start = time.<span class="function">perf_counter</span>()
C = A @ B
elapsed = time.<span class="function">perf_counter</span>() - start

<span class="comment"># Calculate GFLOPS (2 * N^3 operations for matmul)</span>
flops = <span class="number">2</span> * <span class="number">1024</span>**<span class="number">3</span>
gflops = flops / elapsed / <span class="number">1e9</span>
<span class="function">print</span>(<span class="string">f"NumPy: {gflops:.1f} GFLOPS"</span>)  <span class="comment"># ~50-100 GFLOPS on modern CPU</span></code>
        </div>

        <p>
          NumPy uses optimized BLAS libraries, so it's not terrible—maybe 50-100 GFLOPS on a good CPU.
          But your GPU can do <strong>100+ TFLOPS</strong>. That's 1000x more potential, sitting unused.
        </p>

        <div class="callout info">
          <div class="callout-title">What's a GFLOP?</div>
          <p class="mb-0">
            <strong>GFLOPS</strong> = billion floating-point operations per second. Matrix multiplication
            of two N×N matrices requires 2N³ operations (multiply + add for each element).
            For 1024×1024: 2 × 1024³ ≈ 2.1 billion operations.
          </p>
        </div>
      </section>

      <!-- CuPy Magic -->
      <section class="section" id="p1-cupy">
        <h3>The Instant Fix: CuPy</h3>

        <p>
          One import changes everything:
        </p>

        <div class="code-block">
<code><span class="keyword">import</span> cupy <span class="keyword">as</span> cp  <span class="comment"># Drop-in NumPy replacement for GPU</span>

<span class="comment"># Same code, different import</span>
A_gpu = cp.<span class="function">asarray</span>(A)  <span class="comment"># Copy to GPU</span>
B_gpu = cp.<span class="function">asarray</span>(B)

start = time.<span class="function">perf_counter</span>()
C_gpu = A_gpu @ B_gpu
cp.cuda.<span class="function">Stream.null.synchronize</span>()  <span class="comment"># Wait for GPU</span>
elapsed = time.<span class="function">perf_counter</span>() - start

gflops = <span class="number">2</span> * <span class="number">1024</span>**<span class="number">3</span> / elapsed / <span class="number">1e9</span>
<span class="function">print</span>(<span class="string">f"CuPy: {gflops:.1f} GFLOPS"</span>)  <span class="comment"># ~5000-10000 GFLOPS!</span></code>
        </div>

        <p>
          <strong>50-100x speedup with zero algorithm changes.</strong> CuPy calls cuBLAS under the hood,
          which is NVIDIA's heavily optimized matrix library.
        </p>

        <p>
          But <em>why</em> is it so fast? To write our own fast kernels, we need to understand
          how GPUs execute code.
        </p>
      </section>

      <!-- GPU Architecture -->
      <section class="section" id="p1-architecture">
        <h3>How GPUs Actually Work</h3>

        <p>
          A CPU has a few powerful cores optimized for complex tasks. A GPU has <strong>thousands
          of simple cores</strong> optimized for parallel work.
        </p>

        <div class="diagram">
          <div class="diagram-title">GPU Hierarchy</div>
          <pre style="margin: 0; padding: var(--space-md); background: var(--bg-tertiary); border-radius: var(--radius-sm);">
GPU (e.g., H100)
 └── 132 SMs (Streaming Multiprocessors)
      └── Each SM has:
           ├── 64 max concurrent warps
           ├── 4 warp schedulers
           ├── 256KB registers
           └── 228KB shared memory

Warp = 32 threads executing in lockstep (SIMT)
Block = Group of warps sharing SMEM + sync
Grid = Your problem decomposed into blocks</pre>
        </div>

        <p>
          The key insight: <strong>32 threads in a warp execute the same instruction simultaneously</strong>
          on different data. This is SIMT (Single Instruction, Multiple Threads).
        </p>

        <!-- Warp Visualization -->
        <div class="interactive">
          <div class="interactive__title">Warp Execution Visualizer</div>
          <p class="text-muted text-small">
            See how 32 threads execute together. Green = active, Orange = diverged (waiting).
          </p>

          <div style="display: flex; gap: var(--space-sm); margin-bottom: var(--space-md);">
            <button class="btn btn--primary" id="warp-unified">Unified Execution</button>
            <button class="btn" id="warp-diverge">Branch Divergence</button>
            <button class="btn" id="warp-reset">Reset</button>
          </div>

          <div class="thread-viz" id="warp-viz"></div>

          <div class="result-box" id="warp-status">
            Click a button to simulate warp execution.
          </div>
        </div>

        <div class="callout warn">
          <div class="callout-title">Branch Divergence Kills Performance</div>
          <p class="mb-0">
            When threads in a warp take different branches (<code>if/else</code>), execution
            <strong>serializes</strong>. Both paths run sequentially with inactive threads masked.
            If half take each path = 50% throughput. Minimize divergence within warps.
          </p>
        </div>

        <div class="quiz" id="warp-quiz">
          <div class="quiz-q">A warp contains how many threads?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>16 threads</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>32 threads</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>64 threads</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>128 threads (that's a warpgroup)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <!-- First Kernel -->
      <section class="section" id="p1-kernel">
        <h3>Your First GPU Kernel</h3>

        <p>
          Let's write a simple kernel in <a href="https://triton-lang.org/" target="_blank" rel="noopener">Triton</a>,
          a Python-like language for GPU programming:
        </p>

        <div class="code-block">
<code><span class="keyword">import</span> triton
<span class="keyword">import</span> triton.language <span class="keyword">as</span> tl

<span class="keyword">@triton.jit</span>
<span class="keyword">def</span> <span class="function">add_kernel</span>(x_ptr, y_ptr, out_ptr, n, BLOCK: tl.constexpr):
    <span class="comment"># Which block am I?</span>
    pid = tl.<span class="function">program_id</span>(<span class="number">0</span>)
    
    <span class="comment"># Calculate which elements this block handles</span>
    offsets = pid * BLOCK + tl.<span class="function">arange</span>(<span class="number">0</span>, BLOCK)
    
    <span class="comment"># Mask for bounds checking</span>
    mask = offsets < n
    
    <span class="comment"># Load, compute, store</span>
    x = tl.<span class="function">load</span>(x_ptr + offsets, mask=mask)
    y = tl.<span class="function">load</span>(y_ptr + offsets, mask=mask)
    tl.<span class="function">store</span>(out_ptr + offsets, x + y, mask=mask)

<span class="comment"># Launch kernel</span>
n = <span class="number">1024</span>
grid = (n // <span class="number">256</span>,)  <span class="comment"># Number of blocks</span>
<span class="function">add_kernel</span>[grid](x, y, out, n, BLOCK=<span class="number">256</span>)</code>
        </div>

        <p>
          Key concepts:
        </p>
        <ul>
          <li><strong>program_id</strong>: Which block am I? (like threadIdx in CUDA)</li>
          <li><strong>BLOCK</strong>: How many elements each block processes</li>
          <li><strong>offsets</strong>: Linear indices into the array</li>
          <li><strong>mask</strong>: Bounds checking (don't access beyond array end)</li>
        </ul>

        <div class="callout info">
          <div class="callout-title">Index Arithmetic</div>
          <p class="mb-0">
            For 2D arrays stored in row-major order: <code>linear_idx = row * num_cols + col</code>
            <br>This formula is the foundation of all GPU memory access patterns.
          </p>
        </div>
      </section>

      <!-- Memory Hierarchy -->
      <section class="section" id="p1-memory">
        <h3>Why Naive Kernels Are Slow</h3>

        <p>
          Your kernel works, but it's 10-50x slower than cuBLAS. Why? <strong>Memory bandwidth
          is the bottleneck</strong>, not compute.
        </p>

        <div class="diagram">
          <div class="diagram-title">Memory Hierarchy Latency</div>
          <div class="mem-bars">
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">Registers</span>
                <span class="mem-bar-stats">~1 cycle, ~20 TB/s</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-green" style="--w: 100%">Fastest</div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">Shared Memory</span>
                <span class="mem-bar-stats">~20 cycles, ~12 TB/s</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-blue" style="--w: 85%">Fast</div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">L2 Cache</span>
                <span class="mem-bar-stats">~200 cycles, ~5 TB/s</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-purple" style="--w: 50%">Medium</div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">HBM (Global)</span>
                <span class="mem-bar-stats">~400 cycles, ~3 TB/s</span>
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-orange" style="--w: 25%">Slow</div>
              </div>
            </div>
          </div>
        </div>

        <p>
          <strong>400 cycles</strong> to read from global memory! If your kernel reads from HBM
          every operation, most time is spent waiting, not computing.
        </p>

        <!-- Coalescing Visualization -->
        <div class="interactive">
          <div class="interactive__title">Memory Coalescing Visualizer</div>
          <p class="text-muted text-small">
            Adjacent threads should access adjacent memory addresses for maximum bandwidth.
          </p>

          <div style="display: flex; gap: var(--space-sm); margin-bottom: var(--space-md);">
            <button class="btn btn--primary" id="coal-good">Coalesced</button>
            <button class="btn" id="coal-strided">Strided</button>
            <button class="btn" id="coal-random">Random</button>
          </div>

          <div class="coal-viz">
            <div class="coal-viz__row">
              <span class="coal-viz__label">Threads:</span>
              <div class="coal-viz__cells" id="coal-threads"></div>
            </div>
            <div class="coal-viz__row">
              <span class="coal-viz__label">Memory:</span>
              <div class="coal-viz__cells" id="coal-memory"></div>
            </div>
          </div>

          <div class="result-box" id="coal-result">
            Click a pattern to see memory access behavior.
          </div>
        </div>

        <div class="quiz" id="coal-quiz">
          <div class="quiz-q">Which access pattern achieves maximum bandwidth?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Thread i accesses address i (coalesced)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Thread i accesses address i*32 (strided)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Thread i accesses a random address</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <!-- Tiling -->
      <section class="section" id="p1-tiling">
        <h3>The Solution: Tiling</h3>

        <p>
          The key insight: <strong>load data once from slow HBM, reuse many times from fast
          shared memory</strong>.
        </p>

        <div class="code-block">
<code><span class="comment"># Tiled matrix multiplication concept:</span>
<span class="comment"># Instead of: for each output element, read entire row/column from HBM</span>
<span class="comment"># Do this: load tiles into shared memory, compute partial results</span>

<span class="keyword">for</span> tile <span class="keyword">in</span> <span class="function">range</span>(num_tiles):
    <span class="comment"># 1. Load tile of A and tile of B into shared memory</span>
    A_tile = tl.<span class="function">load</span>(A_ptr + tile_offsets)  <span class="comment"># One HBM read</span>
    B_tile = tl.<span class="function">load</span>(B_ptr + tile_offsets)  <span class="comment"># One HBM read</span>
    
    <span class="comment"># 2. Compute partial results using shared memory (FAST!)</span>
    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="function">range</span>(TILE_SIZE):
        C_acc += A_tile[:, k] * B_tile[k, :]  <span class="comment"># Many reuses!</span>
    
    <span class="comment"># 3. Synchronize before loading next tile</span>
    tl.<span class="function">debug_barrier</span>()</code>
        </div>

        <p>
          <strong>Data reuse</strong> is everything. A TILE_SIZE×TILE_SIZE tile is loaded once
          but used TILE_SIZE times for computation. This transforms your kernel from
          memory-bound to compute-bound.
        </p>

        <div class="callout info">
          <div class="callout-title">Arithmetic Intensity</div>
          <p class="mb-0">
            <strong>Arithmetic Intensity = FLOPS / Bytes</strong><br>
            Naive matmul: ~1 FLOP/byte (memory-bound)<br>
            Tiled matmul: ~100 FLOPS/byte (compute-bound)<br>
            Higher intensity = less time waiting for memory.
          </p>
        </div>

        <div class="insight mt-lg">
          <div class="insight-icon">+</div>
          <div>
            <h4>Part 1 Milestone</h4>
            <p class="mb-0">
              By implementing tiled matmul, you can achieve <strong>500+ GFLOPS</strong>—a
              500,000x improvement over naive Python. You understand WHY: coalesced access,
              data reuse in shared memory, and hiding memory latency.
            </p>
          </div>
        </div>
      </section>

      <!-- Lab Section -->
      <section class="section" id="p1-lab">
        <div class="lab-callout">
          <h3>Practice Labs: Build It Yourself</h3>
          <p>
            Apply what you've learned with hands-on Jupyter notebooks.
            Each lab takes 30-60 minutes.
          </p>
          <div class="lab-callout__links">
            <a href="notebooks/part1/00_ready_check.ipynb">Environment Check</a>
            <a href="notebooks/part1/01_numpy_baseline.ipynb">NumPy Baseline</a>
            <a href="notebooks/part1/02_cupy_intro.ipynb">CuPy Introduction</a>
            <a href="notebooks/part1/03_gpu_architecture.ipynb">GPU Architecture</a>
            <a href="notebooks/part1/04_first_triton_kernel.ipynb">First Triton Kernel</a>
            <a href="notebooks/part1/05_memory_hierarchy.ipynb">Memory Hierarchy</a>
            <a href="notebooks/part1/06_tiling_basics.ipynb">Tiling Basics</a>
            <a href="notebooks/part1/07_fast_matmul.ipynb">Fast Matrix Multiplication</a>
          </div>
        </div>
      </section>

      <!-- ═══════════════════════════════════════════════════════════════════════
           PART 2: OPTIMIZATION MASTERY (Placeholder - to be expanded)
           ═══════════════════════════════════════════════════════════════════════ -->

      <section class="section" id="part2">
        <div class="section__head">
          <div class="section__num bg-purple">2</div>
          <div>
            <h2>Optimization Mastery</h2>
            <p class="mb-0 text-muted">Push to 80% of theoretical peak</p>
          </div>
        </div>

        <p>
          Part 1 got us to 500 GFLOPS. Part 2 pushes to 80%+ of peak through
          profiling, coalescing experiments, bank conflict elimination, and Tensor Cores.
        </p>

        <p class="text-muted">
          <em>Full content coming soon. For now, use the practice labs below.</em>
        </p>

        <div class="lab-callout" id="p2-lab">
          <h3>Practice Labs</h3>
          <div class="lab-callout__links">
            <a href="notebooks/part2/01_profiling.ipynb">Profiling</a>
            <a href="notebooks/part2/02_coalescing.ipynb">Coalescing</a>
            <a href="notebooks/part2/03_bank_conflicts.ipynb">Bank Conflicts</a>
            <a href="notebooks/part2/04_pipelining.ipynb">Pipelining</a>
            <a href="notebooks/part2/05_tma.ipynb">TMA (Hopper+)</a>
            <a href="notebooks/part2/06_tensor_cores.ipynb">Tensor Cores</a>
            <a href="notebooks/part2/07_optimized_gemm.ipynb">Optimized GEMM</a>
          </div>
        </div>
      </section>

      <!-- ═══════════════════════════════════════════════════════════════════════
           PART 3: FLASHATTENTION (Placeholder - to be expanded)
           ═══════════════════════════════════════════════════════════════════════ -->

      <section class="section" id="part3">
        <div class="section__head">
          <div class="section__num bg-orange">3</div>
          <div>
            <h2>Building FlashAttention</h2>
            <p class="mb-0 text-muted">The algorithm that changed LLM inference</p>
          </div>
        </div>

        <p>
          Attention is THE bottleneck in LLMs. This part: understand the math deeply,
          handle numerical stability, then build FlashAttention from scratch.
        </p>

        <p class="text-muted">
          <em>Full content coming soon. For now, use the practice labs and
          <a href="attention-math.html">Attention Math interactive</a>.</em>
        </p>

        <div class="lab-callout" id="p3-lab">
          <h3>Practice Labs</h3>
          <div class="lab-callout__links">
            <a href="notebooks/part3/01_dot_product.ipynb">Dot Products</a>
            <a href="notebooks/part3/02_softmax_problem.ipynb">Softmax Problem</a>
            <a href="notebooks/part3/03_stable_softmax.ipynb">Stable Softmax</a>
            <a href="notebooks/part3/04_full_attention.ipynb">Full Attention</a>
            <a href="notebooks/part3/05_online_softmax.ipynb">Online Softmax</a>
            <a href="notebooks/part3/06_tiled_attention.ipynb">Tiled Attention</a>
            <a href="notebooks/part3/07_flash_attention.ipynb">FlashAttention</a>
          </div>
        </div>
      </section>

      <!-- ═══════════════════════════════════════════════════════════════════════
           PART 4: PRODUCTION (Placeholder - to be expanded)
           ═══════════════════════════════════════════════════════════════════════ -->

      <section class="section" id="part4">
        <div class="section__head">
          <div class="section__num bg-cyan">4</div>
          <div>
            <h2>Production Quantization</h2>
            <p class="mb-0 text-muted">From 16 bits to 4, quality preserved</p>
          </div>
        </div>

        <p>
          Final project: Mixed-precision KV cache kernel. FP8 for keys, NVFP4 for values.
          ~2.6x memory reduction with quality preserved.
        </p>

        <p class="text-muted">
          <em>Full content and notebooks coming soon.</em>
        </p>

        <div class="lab-callout" id="p4-lab">
          <h3>Practice Labs</h3>
          <p class="text-muted">Notebooks for Part 4 are being created.</p>
        </div>
      </section>

      <!-- ═══════════════════════════════════════════════════════════════════════
           REFERENCE SECTION
           ═══════════════════════════════════════════════════════════════════════ -->

      <section class="section" id="reference">
        <div class="section__head">
          <div class="section__num bg-yellow">REF</div>
          <div>
            <h2>Reference Materials</h2>
            <p class="mb-0 text-muted">Deep dives for when you need more detail</p>
          </div>
        </div>

        <div class="grid grid-2">
          <a href="lessons/gpu-architecture.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>GPU Architecture</h4>
            <p class="mb-0 text-muted">SMs, warps, warpgroups, occupancy calculator</p>
          </a>
          <a href="lessons/memory-hierarchy.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Memory Hierarchy</h4>
            <p class="mb-0 text-muted">Coalescing, bank conflicts, TMA, bandwidth</p>
          </a>
          <a href="attention-math.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Attention Math</h4>
            <p class="mb-0 text-muted">Dot products, softmax, online algorithms, FP formats</p>
          </a>
          <a href="lessons/math-foundations.html" class="card" style="display: block; text-decoration: none; margin: 0;">
            <h4>Math Foundations</h4>
            <p class="mb-0 text-muted">Index arithmetic, tiling, floating point, quantization</p>
          </a>
        </div>
      </section>

    </main>

    <script src="scripts/components.js"></script>
    <script>
      // ============================================
      // Warp Visualization
      // ============================================
      const warpViz = document.getElementById('warp-viz');
      const warpStatus = document.getElementById('warp-status');

      // Create 32 thread elements (2 rows of 16)
      for (let i = 0; i < 32; i++) {
        const thread = document.createElement('div');
        thread.className = 'thread-viz__thread';
        thread.title = `Thread ${i}`;
        warpViz.appendChild(thread);
      }

      const threads = warpViz.querySelectorAll('.thread-viz__thread');

      document.getElementById('warp-unified').addEventListener('click', () => {
        threads.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            t.classList.remove('diverged');
          }, i * 15);
        });
        warpStatus.innerHTML = '<strong style="color: var(--accent-green)">All 32 threads</strong> executing the same instruction in parallel. Maximum throughput.';
      });

      document.getElementById('warp-diverge').addEventListener('click', () => {
        threads.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            if (i % 2 === 0) {
              t.classList.add('diverged');
            } else {
              t.classList.remove('diverged');
            }
          }, i * 15);
        });
        warpStatus.innerHTML = '<strong style="color: var(--accent-orange)">Branch divergence!</strong> Even threads (orange) take path A, odd threads (blue) take path B. Execution serializes—50% throughput.';
      });

      document.getElementById('warp-reset').addEventListener('click', () => {
        threads.forEach(t => t.classList.remove('active', 'diverged'));
        warpStatus.textContent = 'Click a button to simulate warp execution.';
      });

      // ============================================
      // Coalescing Visualization
      // ============================================
      const coalThreads = document.getElementById('coal-threads');
      const coalMemory = document.getElementById('coal-memory');
      const coalResult = document.getElementById('coal-result');

      // Create 16 thread and memory cells
      for (let i = 0; i < 16; i++) {
        const thread = document.createElement('div');
        thread.className = 'coal-viz__cell';
        thread.textContent = i;
        coalThreads.appendChild(thread);

        const mem = document.createElement('div');
        mem.className = 'coal-viz__cell';
        mem.textContent = i;
        coalMemory.appendChild(mem);
      }

      const threadCells = coalThreads.querySelectorAll('.coal-viz__cell');
      const memCells = coalMemory.querySelectorAll('.coal-viz__cell');

      function resetCoalViz() {
        threadCells.forEach(t => t.classList.remove('active'));
        memCells.forEach(m => m.classList.remove('accessed', 'conflict'));
      }

      document.getElementById('coal-good').addEventListener('click', () => {
        resetCoalViz();
        threadCells.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            memCells[i].classList.add('accessed');
          }, i * 30);
        });
        coalResult.innerHTML = '<strong style="color: var(--accent-green)">1 memory transaction</strong> — All 16 threads access consecutive addresses. Hardware coalesces into a single 128-byte transaction. Maximum bandwidth.';
      });

      document.getElementById('coal-strided').addEventListener('click', () => {
        resetCoalViz();
        threadCells.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            const targetIdx = (i * 2) % 16;
            memCells[targetIdx].classList.add('conflict');
          }, i * 30);
        });
        coalResult.innerHTML = '<strong style="color: var(--accent-orange)">Multiple transactions</strong> — Strided access (every other element). Half the bytes in each cache line are wasted. ~50% bandwidth utilization.';
      });

      document.getElementById('coal-random').addEventListener('click', () => {
        resetCoalViz();
        const randomOrder = [7, 2, 14, 5, 11, 0, 9, 4, 13, 1, 8, 15, 3, 10, 6, 12];
        threadCells.forEach((t, i) => {
          setTimeout(() => {
            t.classList.add('active');
            memCells[randomOrder[i]].classList.add('conflict');
          }, i * 30);
        });
        coalResult.innerHTML = '<strong style="color: var(--accent-orange)">Up to 16 transactions</strong> — Random access pattern. Each thread may hit a different cache line. Worst case for bandwidth.';
      });
    </script>
  </body>
</html>
