<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>GPU ‚Üí NVFP4 Kernels | Full Course</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="base.css" />
  </head>
  <body class="has-sidebar">
    <a href="#main" class="skip-link">Skip to content</a>

    <button class="mobile-btn" aria-label="Menu">
      <svg
        width="24"
        height="24"
        viewBox="0 0 24 24"
        fill="none"
        stroke="currentColor"
        stroke-width="2"
      >
        <path d="M3 12h18M3 6h18M3 18h18" />
      </svg>
    </button>

    <nav class="sidebar">
      <div class="sidebar__header">
        <div class="sidebar__logo">GPU ‚Üí NVFP4</div>
        <div class="sidebar__sub">4-Week Intensive</div>
      </div>

      <div class="nav-section open">
        <button class="nav-section__btn">
          <span class="nav-icon bg-blue">F</span>
          Foundations
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="lessons/gpu-architecture.html" class="nav-item">GPU Architecture</a>
          <a href="#memory" class="nav-item">Memory Hierarchy</a>
          <a href="#tensor-cores" class="nav-item">Tensor Cores</a>
          <a href="#pipelining" class="nav-item">Software Pipelining</a>
          <a href="#quantization" class="nav-item">Quantization</a>
          <a href="#attention" class="nav-item">Attention & KV Cache</a>
        </div>
      </div>

      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-green">1</span>
          Week 1: CuTe DSL
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#w1-overview" class="nav-item">Overview</a>
          <a href="#day1" class="nav-item">Day 1: Layouts</a>
          <a href="#day2" class="nav-item">Day 2: First Kernels</a>
          <a href="#day3" class="nav-item">Day 3: Tensors & TMA</a>
          <a href="#day4" class="nav-item">Day 4: MMA Ops</a>
          <a href="#day5" class="nav-item">Day 5: Pipelined GEMM</a>
          <a href="#day6" class="nav-item">Day 6: Quantization</a>
          <a href="#day7" class="nav-item">Day 7: Integration</a>
        </div>
      </div>

      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-purple">2</span>
          Week 2: Deep Dive
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#w2-cutlass" class="nav-item">CUTLASS Architecture</a>
          <a href="#w2-cute" class="nav-item">CuTe Tutorials</a>
          <a href="#w2-bench" class="nav-item">Microbenchmarking</a>
        </div>
      </div>

      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-orange">3</span>
          Week 3: Quantization
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#w3-papers" class="nav-item">Key Papers</a>
        </div>
      </div>

      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-cyan">4</span>
          Week 4: Systems
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#w4-trtllm" class="nav-item">TensorRT-LLM</a>
          <a href="#w4-project" class="nav-item">Final Project</a>
        </div>
      </div>

      <div class="nav-section">
        <button class="nav-section__btn">
          <span class="nav-icon bg-pink">R</span>
          Resources
          <svg
            class="nav-chevron"
            width="16"
            height="16"
            viewBox="0 0 24 24"
            fill="none"
            stroke="currentColor"
            stroke-width="2"
          >
            <path d="M9 18l6-6-6-6" />
          </svg>
        </button>
        <div class="nav-items">
          <a href="#reading" class="nav-item">Reading List</a>
          <a href="#checklist" class="nav-item">Knowledge Check</a>
        </div>
      </div>
    </nav>

    <main id="main" class="main-content">
      <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     FOUNDATIONS
     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->

      <section class="section" id="gpu-arch">
        <div class="section__head">
          <div class="section__num bg-blue">01</div>
          <div>
            <h2>GPU Architecture</h2>
            <p class="mb-0 text-muted">Understanding the hardware</p>
          </div>
        </div>

        <p>
          Modern NVIDIA GPUs are hierarchical parallel processors designed for
          <strong>throughput over latency</strong>. Understanding the execution
          model is essential for writing efficient kernels.
        </p>

        <a href="lessons/gpu-architecture.html" class="card" style="display: block; text-decoration: none;">
          <h4>GPU Architecture Interactive Lesson</h4>
          <p>
            Complete coverage of GPU execution hierarchy: SMs, warps, warpgroups, blocks, and grids.
            Includes interactive visualizations for warp execution and an occupancy calculator.
          </p>
          <p class="mb-0 text-blue"><strong>Start Lesson ‚Üí</strong></p>
        </a>

        <div class="callout info mt-lg">
          <div class="callout-title">Key Concepts</div>
          <p class="mb-0">
            <strong>Warp:</strong> 32 threads in lockstep (SIMT) ‚Ä¢
            <strong>Warpgroup:</strong> 4 warps for Tensor Core ops ‚Ä¢
            <strong>Block:</strong> Warps sharing SMEM + sync ‚Ä¢
            <strong>Grid:</strong> Problem decomposition
          </p>
        </div>
      </section>

      <section class="section" id="memory">
        <div class="section__head">
          <div class="section__num bg-green">02</div>
          <div>
            <h2>Memory Hierarchy</h2>
            <p class="mb-0 text-muted">The key to performance</p>
          </div>
        </div>

        <p>
          GPU performance is often <strong>memory-bound</strong>. Understanding
          the hierarchy is essential.
        </p>

        <div class="diagram">
          <div class="diagram-title">Latency & Bandwidth (<a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">Blackwell B200</a>)*</div>
          <div class="mem-bars">
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">Registers</span
                ><span class="mem-bar-stats"
                  >256KB/SM ‚Ä¢ 0 cycles ‚Ä¢ ~20 TB/s</span
                >
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-green" style="--w: 100%">
                  Fastest
                </div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">Shared Memory</span
                ><span class="mem-bar-stats"
                  >228KB/SM ‚Ä¢ approx. 20 cycles ‚Ä¢ approx. 20 TB/s</span
                >
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-blue" style="--w: 85%">
                  Programmer-managed
                </div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">L2 Cache</span
                ><span class="mem-bar-stats"
                  >approx. 60MB ‚Ä¢ approx. 200 cycles ‚Ä¢ approx. 10 TB/s</span
                >
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-purple" style="--w: 50%">
                  Hardware-managed
                </div>
              </div>
            </div>
            <div class="mem-bar">
              <div class="mem-bar-head">
                <span class="mem-bar-label">HBM (Global)</span
                ><span class="mem-bar-stats"
                  >192GB ‚Ä¢ approx. 400 cycles ‚Ä¢ approx. 8 TB/s</span
                >
              </div>
              <div class="mem-bar-track">
                <div class="mem-bar-fill bg-orange" style="--w: 25%">
                  High latency
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="insight">
          <div class="insight-icon">üí°</div>
          <div>
            <h4>The Fundamental Game</h4>
            <p>
              Every HBM trip = hundreds of cycles.
              <strong>Maximize data reuse</strong> in registers/SMEM. <a href="https://github.com/NVIDIA/cutlass" target="_blank" rel="noopener">Optimized
              GEMM achieves >80% peak FLOPS</a>.
            </p>
          </div>
        </div>

        <h3>Access Patterns</h3>
        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4>üîÑ Coalescing</h4>
            <p>
              Adjacent threads ‚Üí adjacent addresses = single transaction.
              Maximum bandwidth.
            </p>
            <pre><code>// Good: data[threadIdx.x]
// Bad:  data[threadIdx.x * 32]</code></pre>
          </div>
          <div class="card" style="margin: 0">
            <h4>üè¶ Bank Conflicts</h4>
            <p>
              SMEM has <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#shared-memory-5-x" target="_blank" rel="noopener">32 banks</a>. Multiple threads hitting same bank (different
              addresses) serialize.
            </p>
            <pre><code>// Conflict: smem[tid * 4]
// Fixed:    smem[tid * 4 + tid/8]</code></pre>
          </div>
        </div>

        <h3>TMA: Tensor Memory Accelerator</h3>
        <p>
          <a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html" target="_blank" rel="noopener">Hopper+ dedicated hardware</a> for bulk data movement. Offloads addressing
          from SMs.
        </p>

        <div class="table-wrap">
          <table>
            <tr>
              <th>Aspect</th>
              <th>Manual Loads</th>
              <th>TMA</th>
            </tr>
            <tr>
              <td>Address computation</td>
              <td>SM cycles</td>
              <td>TMA unit (free)</td>
            </tr>
            <tr>
              <td>Tile dimensions</td>
              <td>Manual loop</td>
              <td>Descriptor-based</td>
            </tr>
            <tr>
              <td>Multicast</td>
              <td>Not available</td>
              <td>Built-in</td>
            </tr>
          </table>
        </div>
      </section>

      <section class="section" id="tensor-cores">
        <div class="section__head">
          <div class="section__num bg-purple">03</div>
          <div>
            <h2>Tensor Cores</h2>
            <p class="mb-0 text-muted">Specialized MMA units</p>
          </div>
        </div>

        <p>
          Fixed-function hardware for <strong>D = A√óB + C</strong> with
          constrained shapes and types.
        </p>

        <div class="callout info">
          <div class="callout-title">üìê Not Magic</div>
          <p class="mb-0">
            <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma" target="_blank" rel="noopener">Tensor Cores</a> perform small matrix ops in one cycle. You must match
            supported dimensions and types.
          </p>
        </div>

        <h3>Evolution</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Gen</th>
              <th>Arch</th>
              <th>WMMA Shape (m√ón√ók)</th>
              <th>Types Added</th>
            </tr>
            <tr>
              <td>1st</td>
              <td><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma" target="_blank" rel="noopener">Volta</a></td>
              <td>16√ó16√ó16</td>
              <td>FP16</td>
            </tr>
            <tr>
              <td>2nd</td>
              <td><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#wmma" target="_blank" rel="noopener">Turing</a></td>
              <td>16√ó16√ó16, 8√ó8√ó16</td>
              <td>INT8</td>
            </tr>
            <tr>
              <td>3rd</td>
              <td><a href="https://docs.nvidia.com/cuda/ampere-tuning-guide/index.html" target="_blank" rel="noopener">Ampere</a></td>
              <td>16√ó8√ó16</td>
              <td>BF16, TF32</td>
            </tr>
            <tr>
              <td>4th</td>
              <td><a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html" target="_blank" rel="noopener">Hopper</a></td>
              <td>Warpgroup MMA (64√ó)</td>
              <td class="text-green">FP8</td>
            </tr>
            <tr>
              <td>5th</td>
              <td><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">Blackwell</a></td>
              <td>‚Äî</td>
              <td class="text-blue">FP4, FP6</td>
            </tr>
          </table>
        </div>
        <p class="text-muted text-small">
          Shapes shown are programmer-facing WMMA/MMA API dimensions. Hardware may use smaller internal tiles.
        </p>

        <h3><a href="https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html" target="_blank" rel="noopener">Warpgroup MMA</a> (Hopper/Blackwell)</h3>
        <p>128 threads cooperate on larger tiles (64√ó64+). Benefits:</p>
        <div class="grid grid-3">
          <div class="card" style="margin: 0">
            <h4 class="text-green">Async Execution</h4>
            <p class="mb-0">MMA while preparing next data</p>
          </div>
          <div class="card" style="margin: 0">
            <h4 class="text-blue">Better Utilization</h4>
            <p class="mb-0">Larger tiles amortize overhead</p>
          </div>
          <div class="card" style="margin: 0">
            <h4 class="text-purple">Pipeline Overlap</h4>
            <p class="mb-0">Natural TMA integration</p>
          </div>
        </div>
      </section>

      <section class="section" id="pipelining">
        <div class="section__head">
          <div class="section__num bg-orange">04</div>
          <div>
            <h2>Software Pipelining</h2>
            <p class="mb-0 text-muted">Hiding memory latency</p>
          </div>
        </div>

        <p>Without pipelining, Tensor Cores sit idle waiting for data.</p>

        <div class="diagram">
          <div class="diagram-title">Sequential vs Pipelined</div>
          <p>
            <strong>Sequential:</strong> LOAD ‚Üí MMA ‚Üí LOAD ‚Üí MMA (~50%
            utilization)
          </p>
          <p class="mb-0">
            <strong>Pipelined:</strong> Loads overlap with compute. Near 100% in
            steady state.
          </p>
        </div>

        <div class="insight">
          <div class="insight-icon">üîÑ</div>
          <div>
            <h4>Requirements</h4>
            <p>
              Multiple SMEM buffers (1 per stage), async copy (TMA/cp.async),
              sync barriers between producer (load) and consumer (compute).
            </p>
          </div>
        </div>

        <h3>Depth Tradeoffs</h3>
        <div class="slider-wrap">
          <div class="slider-head">
            <span>Pipeline Stages</span
            ><span class="slider-val" id="pipe-val">2</span>
          </div>
          <input
            type="range"
            min="1"
            max="5"
            value="2"
            data-target="pipe-val"
            data-callback="updatePipeline"
          />
        </div>
        <div id="pipe-info" class="card">
          <h4>2 Stages</h4>
          <p>
            <strong class="text-green">Pros:</strong> Lower SMEM, simpler sync
          </p>
          <p class="mb-0">
            <strong class="text-red">Cons:</strong> May not fully hide latency
          </p>
        </div>
      </section>

      <section class="section" id="quantization">
        <div class="section__head">
          <div class="section__num bg-yellow">05</div>
          <div>
            <h2>Quantization</h2>
            <p class="mb-0 text-muted">
              Compressing without losing intelligence
            </p>
          </div>
        </div>

        <h3>Floating Point Anatomy</h3>
        <div class="diagram">
          <div class="diagram-title">Format Comparison</div>
          <div class="fp-formats">
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format" target="_blank" rel="noopener" class="fp-name">FP32 (32 bits)</a
                ><span class="fp-range">¬±3.4√ó10¬≥‚Å∏</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 8">Exp(8)</div>
                <div class="fp-bit fp-m" style="flex: 23">Mantissa(23)</div>
              </div>
            </div>
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format" target="_blank" rel="noopener" class="fp-name">FP16 (16 bits)</a
                ><span class="fp-range">¬±65,504</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 5">E(5)</div>
                <div class="fp-bit fp-m" style="flex: 10">M(10)</div>
              </div>
            </div>
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener" class="fp-name">FP8 E4M3 (8 bits)</a
                ><span class="fp-range">¬±448</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 4">E(4)</div>
                <div class="fp-bit fp-m" style="flex: 3">M(3)</div>
              </div>
            </div>
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf" target="_blank" rel="noopener" class="fp-name">FP4 E2M1 (4 bits)</a
                ><span class="fp-range">¬±6</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 2">E</div>
                <div class="fp-bit fp-m" style="flex: 1">M</div>
              </div>
            </div>
          </div>
        </div>

        <p>
          <strong>Exponent</strong> = range. <strong>Mantissa</strong> =
          precision. Quantization finds minimum bits without quality loss.
        </p>

        <h3>NVFP4 Deep Dive</h3>
        <p>
          <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">NVIDIA's Blackwell-native 4-bit format</a> with
          <strong>two-level scaling</strong>:
        </p>

        <div class="card">
          <h4>üî¢ Two-Level Scaling</h4>
          <div class="equation" style="margin: var(--space-md) 0">
            value = <span class="text-blue">fp4</span> √ó
            <span class="text-green">block_scale<sub>E4M3</sub></span> √ó
            <span class="text-purple">tensor_scale<sub>FP32</sub></span>
          </div>
          <p>
            <strong>Level 1:</strong> E4M3 scale per 16 elements (fractional,
            not power-of-2)
          </p>
          <p class="mb-0">
            <strong>Level 2:</strong> FP32 tensor scale normalizes into E4M3
            range
          </p>
        </div>

        <div class="table-wrap">
          <table>
            <tr>
              <th>Aspect</th>
              <th>MXFP4</th>
              <th>NVFP4</th>
            </tr>
            <tr>
              <td>Block size</td>
              <td>32</td>
              <td class="text-green">16</td>
            </tr>
            <tr>
              <td>Scale format</td>
              <td>E8M0 (power-of-2)</td>
              <td class="text-green">E4M3 (fractional)</td>
            </tr>
            <tr>
              <td>Second-level</td>
              <td>None</td>
              <td class="text-green">FP32 per-tensor</td>
            </tr>
          </table>
        </div>

        <div class="quiz">
          <div class="quiz-q">
            Why does NVFP4 use E4M3 scales instead of E8M0?
          </div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>E4M3 has more bits</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span
                >E4M3 enables fractional scaling (not just powers of 2)</span
              >
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>E4M3 is faster</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <section class="section" id="attention">
        <div class="section__head">
          <div class="section__num bg-cyan">06</div>
          <div>
            <h2>Attention & KV Cache</h2>
            <p class="mb-0 text-muted">The memory bottleneck</p>
          </div>
        </div>

        <h3>Attention Computation</h3>
        <pre><code>scores = Q @ K.T / sqrt(d)  # [seq, seq] ‚Üê THE PROBLEM
weights = softmax(scores)
output = weights @ V</code></pre>

        <div class="callout warn">
          <div class="callout-title">‚ö†Ô∏è Quadratic Problem</div>
          <p class="mb-0">
            Attention matrix is [seq √ó seq]. At 128K context:
            <strong>16 billion elements per head</strong>. Can't materialize ‚Üí
            <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention</a>.
          </p>
        </div>

        <h3>KV Cache</h3>
        <p>
          Store K, V for previous tokens. New token: compute only its K, V. O(N)
          vs O(N¬≤).
        </p>

        <div class="insight">
          <div class="insight-icon">üíæ</div>
          <div>
            <h4>The Bottleneck</h4>
            <p>
              <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">KV cache grows linearly</a>. 70B @ 128K = <strong>tens of GB</strong>.
              Decode becomes memory-bandwidth bound.
            </p>
          </div>
        </div>

        <h3>Mixed Precision KV Cache</h3>
        <p>Not all components equally sensitive:</p>

        <div class="grid grid-2">
          <div
            class="card"
            style="margin: 0; border-top: 4px solid var(--accent-red)"
          >
            <h4>K: High Sensitivity</h4>
            <p>
              Directly affects Q¬∑K<sup>T</sup> scores. Softmax amplifies errors.
            </p>
            <p class="text-blue mb-0"><strong>‚Üí Keep at FP8</strong></p>
          </div>
          <div
            class="card"
            style="margin: 0; border-top: 4px solid var(--accent-green)"
          >
            <h4>V: Lower Sensitivity</h4>
            <p>
              Gets averaged: output = Œ£(a<sub>i</sub>¬∑v<sub>i</sub>). Errors
              partially cancel.
            </p>
            <p class="text-green mb-0"><strong>‚Üí Can use NVFP4</strong></p>
          </div>
        </div>

        <div class="table-wrap mt-lg">
          <table>
            <tr>
              <th>Scheme</th>
              <th>K</th>
              <th>V</th>
              <th>vs FP16</th>
            </tr>
            <tr>
              <td>Baseline</td>
              <td>FP16</td>
              <td>FP16</td>
              <td>1.0√ó</td>
            </tr>
            <tr>
              <td>FP8 KV</td>
              <td>FP8</td>
              <td>FP8</td>
              <td>2.0√ó</td>
            </tr>
            <tr style="background: var(--glow-green)">
              <td><strong>Mixed (target)</strong></td>
              <td>FP8</td>
              <td>NVFP4</td>
              <td class="text-green"><strong>~2.6√ó</strong></td>
            </tr>
          </table>
        </div>

        <div class="insight mt-lg">
          <div class="insight-icon">üéØ</div>
          <div>
            <h4>Your Kernel Target</h4>
            <p>
              FP8 K + NVFP4 V: <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener"><strong>2.6√ó memory reduction</strong></a> with
              quality preserved. This is what you'll build.
            </p>
          </div>
        </div>
      </section>

      <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     WEEK 1
     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->

      <section class="section" id="w1-overview">
        <div class="section__head">
          <div class="section__num bg-green">W1</div>
          <div>
            <h2>Week 1: CuTe DSL Crash Course</h2>
            <p class="mb-0 text-muted">From zero to NVFP4 kernel</p>
          </div>
        </div>

        <p>
          Seven days from basic CuTe concepts to a working NVFP4 quantization
          kernel.
        </p>

        <div class="callout info">
          <div class="callout-title">‚è∞ Daily Rhythm</div>
          <p class="mb-0">
            9-11: Reading ‚Ä¢ 11-12: Design ‚Ä¢ 13-17: Implementation ‚Ä¢ 17-18:
            Profile ‚Ä¢ 18-18:30: Notes
          </p>
        </div>

        <div id="day1" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">1</div>
            <div>
              <div class="sched-title">Layouts: The Core Abstraction</div>
              <div class="sched-sub">Shape, stride, index math</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">Read CuTe Layout Docs</div>
                <p>
                  01_layout.html, 02_layout_algebra.html ‚Äî Shape and Stride as
                  first-class objects
                </p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">Hands-on Manipulation</div>
                <p>
                  Create row/col major layouts, tile 1024√ó1024 matrix, verify
                  index math
                </p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                Script creating 5 layouts, printing shapes/strides, verifying
                index math for test coordinates.
              </p>
            </div>
          </div>
        </div>

        <div id="day2" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">2</div>
            <div>
              <div class="sched-title">First Kernels</div>
              <div class="sched-sub">@jit, @kernel decorators</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">DSL Introduction</div>
                <p>
                  dsl_introduction.html, dsl_code_generation.html,
                  dsl_control_flow.html
                </p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">Implement Kernels</div>
                <p>
                  1) Vector add, 2) Naive transpose, 3) SMEM transpose with bank
                  conflict handling
                </p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                All three kernels working, verified vs NumPy. Profile with
                Nsight Compute.
              </p>
            </div>
          </div>
        </div>

        <div id="day3" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">3</div>
            <div>
              <div class="sched-title">Tensors & TMA</div>
              <div class="sched-sub">make_tensor, partitioning, async</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">Tensor Abstraction</div>
                <p>
                  03_tensor.html, 0z_tma_tensors.html ‚Äî gmem ‚Üí smem ‚Üí rmem flow
                </p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">TMA Copy Kernel</div>
                <p>2D tile copy using TMA. Compare bandwidth vs naive.</p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                TMA copy kernel showing significant bandwidth improvement.
              </p>
            </div>
          </div>
        </div>

        <div id="day4" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">4</div>
            <div>
              <div class="sched-title">MMA Operations</div>
              <div class="sched-sub">Tensor Core fundamentals, GEMM</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">MMA Atom Docs</div>
                <p>
                  0t_mma_atom.html, 0x_gemm_tutorial.html,
                  blackwell_functionality.html
                </p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">Simple GEMM</div>
                <p>
                  C = A @ B using warpgroup MMA. Single tile, no pipelining.
                </p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                Working GEMM for 256√ó256√ó256. Verify vs torch.matmul.
              </p>
            </div>
          </div>
        </div>

        <div id="day5" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">5</div>
            <div>
              <div class="sched-title">Pipelined GEMM</div>
              <div class="sched-sub">Producer-consumer, async overlap</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">Pipeline Docs</div>
                <p>pipeline.html, CUTLASS examples ‚Äî stages and barriers</p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">Add Pipelining</div>
                <p>Multi-stage: TMA loads while MMA computes.</p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                Pipelined GEMM with measurable speedup. Profile to verify
                overlap.
              </p>
            </div>
          </div>
        </div>

        <div id="day6" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">6</div>
            <div>
              <div class="sched-title">Quantization Primitives</div>
              <div class="sched-sub">FP8, FP4, NVFP4</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">Numerics Deep Dive</div>
                <p>
                  Study E4M3, E2M1. Write Python reference for NVFP4
                  quantize/dequantize.
                </p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">Quantization Kernels</div>
                <p>
                  quantize_to_nvfp4 and dequantize_from_nvfp4 with block
                  scaling.
                </p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                Round-trip test (FP16‚ÜíNVFP4‚ÜíFP16). Plot error histogram.
              </p>
            </div>
          </div>
        </div>

        <div id="day7" class="sched-day">
          <div class="sched-head">
            <div class="sched-num bg-green">7</div>
            <div>
              <div class="sched-title">Integration: V Cache Kernel</div>
              <div class="sched-sub">Putting it together</div>
            </div>
            <svg
              class="sched-chev"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
            >
              <path d="M6 9l6 6 6-6" />
            </svg>
          </div>
          <div class="sched-body">
            <div class="sched-task">
              <div class="sched-time">Morning</div>
              <div>
                <div class="sched-task-title">Architecture Design</div>
                <p>
                  Memory layout for K (FP8), V (NVFP4). Fusion opportunities.
                  Block/grid dims.
                </p>
              </div>
            </div>
            <div class="sched-task">
              <div class="sched-time">Afternoon</div>
              <div>
                <div class="sched-task-title">V Projection + NVFP4</div>
                <p>Fused: hidden @ W_v ‚Üí quantize ‚Üí store to V_cache.</p>
              </div>
            </div>
            <div class="card mt-lg">
              <h4>üìã Deliverable</h4>
              <p class="mb-0">
                Working V projection + NVFP4 storage. Verify: V_reconstructed ‚âà
                V_ref (atol=0.1).
              </p>
            </div>
          </div>
        </div>
      </section>

      <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     WEEK 2
     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->

      <section class="section" id="w2-cutlass">
        <div class="section__head">
          <div class="section__num bg-purple">W2</div>
          <div>
            <h2>Week 2: Tensor Cores & CuTe Deep Dive</h2>
            <p class="mb-0 text-muted">Mastering the abstractions</p>
          </div>
        </div>

        <div class="card">
          <h4>üìö Reading Focus</h4>
          <ul>
            <li>
              <strong>CUTLASS:</strong> "Efficient GEMM in CUDA" ‚Äî the canonical
              reference
            </li>
            <li>
              <strong>CuTe tutorials:</strong> 01_layout through
              0x_gemm_tutorial (all)
            </li>
            <li>
              <strong>Paper:</strong> <a href="https://arxiv.org/abs/1804.06826" target="_blank" rel="noopener">"Dissecting the NVIDIA Volta GPU
              Architecture via Microbenchmarking"</a> (Jia et al.)
            </li>
          </ul>
        </div>

        <h3 id="w2-cute">Key CuTe Tutorials</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Tutorial</th>
              <th>Concepts</th>
              <th>Why</th>
            </tr>
            <tr>
              <td>01_layout</td>
              <td>Shape, Stride, coalesce</td>
              <td>Foundation for everything</td>
            </tr>
            <tr>
              <td>02_layout_algebra</td>
              <td>Composition, complement</td>
              <td>Advanced tiling</td>
            </tr>
            <tr>
              <td>03_tensor</td>
              <td>make_tensor, partitioning</td>
              <td>Data distribution</td>
            </tr>
            <tr>
              <td>0t_mma_atom</td>
              <td>Hardware MMA shapes</td>
              <td>TC constraints</td>
            </tr>
            <tr>
              <td>0x_gemm_tutorial</td>
              <td>Full GEMM</td>
              <td>Everything together</td>
            </tr>
          </table>
        </div>

        <h3 id="w2-bench">Microbenchmarking</h3>
        <div class="card">
          <h4>üî¨ Experiments</h4>
          <ol>
            <li>Measure actual SMEM bandwidth vs theoretical</li>
            <li>Quantify bank conflict penalty (1-way vs 32-way)</li>
            <li>TMA latency vs manual loads for various tile sizes</li>
            <li>Warpgroup MMA throughput at different occupancies</li>
          </ol>
        </div>
      </section>

      <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     WEEK 3
     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->

      <section class="section" id="w3-papers">
        <div class="section__head">
          <div class="section__num bg-orange">W3</div>
          <div>
            <h2>Week 3: Quantization & LLM Inference</h2>
            <p class="mb-0 text-muted">State-of-the-art techniques</p>
          </div>
        </div>

        <div class="card">
          <h4>üìÑ Papers to Read</h4>
          <ul>
            <li>
              <a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="noopener"><strong>LLM.int8():</strong></a> 8-bit Matrix Multiplication ‚Äî outlier
              handling
            </li>
            <li>
              <a href="https://arxiv.org/abs/2211.10438" target="_blank" rel="noopener"><strong>SmoothQuant:</strong></a> Migrate quantization difficulty from
              activations to weights
            </li>
            <li>
              <a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener"><strong>AWQ:</strong></a> Activation-aware Weight Quantization ‚Äî
              sensitivity analysis
            </li>
            <li>
              <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener"><strong>FlashAttention 1</strong></a> & <a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener"><strong>2:</strong></a> IO-aware attention
              algorithms
            </li>
          </ul>
        </div>

        <h3>Key Insights</h3>

        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4 class="text-blue"><a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="noopener">LLM.int8()</a></h4>
            <p class="mb-0">
              Emergent outliers in activations break naive INT8. Solution:
              mixed-precision decomposition, handle outliers in FP16.
            </p>
          </div>
          <div class="card" style="margin: 0">
            <h4 class="text-green"><a href="https://arxiv.org/abs/2211.10438" target="_blank" rel="noopener">SmoothQuant</a></h4>
            <p class="mb-0">
              Activations hard to quantize (outliers), weights easy.
              Mathematically migrate difficulty: Y = (X/s) ¬∑ (sW).
            </p>
          </div>
          <div class="card" style="margin: 0">
            <h4 class="text-purple"><a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener">AWQ</a></h4>
            <p class="mb-0">
              Not all weights equal. Protect salient weights (those that
              multiply large activations) with per-channel scaling.
            </p>
          </div>
          <div class="card" style="margin: 0">
            <h4 class="text-cyan"><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention</a></h4>
            <p class="mb-0">
              Never materialize [seq√óseq]. Tile computation, online softmax,
              recompute on backward. IO-aware.
            </p>
          </div>
        </div>
      </section>

      <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     WEEK 4
     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->

      <section class="section" id="w4-trtllm">
        <div class="section__head">
          <div class="section__num bg-cyan">W4</div>
          <div>
            <h2>Week 4: Systems Integration</h2>
            <p class="mb-0 text-muted">Production deployment</p>
          </div>
        </div>

        <h3>TensorRT-LLM</h3>
        <p>NVIDIA's optimized inference engine. Study:</p>
        <ul>
          <li>Plugin architecture for custom kernels</li>
          <li>Existing FP8 KV cache implementation</li>
          <li>How quantization configs flow through</li>
        </ul>

        <h3>vLLM Architecture</h3>
        <p>PagedAttention pioneer. Study:</p>
        <ul>
          <li>Block-based KV cache management</li>
          <li>Continuous batching scheduler</li>
          <li>Early NVFP4 support integration points</li>
        </ul>

        <h3 id="w4-project">Final Project: Mixed Precision KV Cache</h3>

        <div class="card">
          <h4>üéØ Deliverable</h4>
          <p>
            Full attention kernel with mixed K(FP8)/V(NVFP4), benchmarked
            against TensorRT-LLM FP8 baseline.
          </p>
          <ol>
            <li>
              <strong>Correctness:</strong> Output matches FP16 reference
              (reasonable tolerance)
            </li>
            <li>
              <strong>Performance:</strong> Memory reduction measured,
              throughput compared
            </li>
            <li>
              <strong>Quality:</strong> Perplexity/accuracy on standard
              benchmarks
            </li>
          </ol>
        </div>
      </section>

      <!-- ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
     RESOURCES
     ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê -->

      <section class="section" id="reading">
        <div class="section__head">
          <div class="section__num bg-pink">R</div>
          <div>
            <h2>Reading List</h2>
            <p class="mb-0 text-muted">Ordered by week</p>
          </div>
        </div>

        <h3>Week 1: GPU Fundamentals</h3>
        <div class="resources">
          <a
            href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CUDA Programming Guide ‚Üí</h4>
            <p>Chapters 1-5</p></a
          >
          <a
            href="https://docs.nvidia.com/cutlass/latest/media/docs/pythonDSL/overview.html"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CuTe DSL Docs ‚Üí</h4>
            <p>Official documentation</p></a
          >
          <a
            href="https://github.com/NVIDIA/cutlass"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CUTLASS GitHub ‚Üí</h4>
            <p>Examples and source</p></a
          >
        </div>

        <h3>Week 2: Tensor Cores & CuTe</h3>
        <div class="resources">
          <a
            href="https://docs.nvidia.com/cutlass/latest/media/docs/cpp/cute/index.html"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>CuTe C++ Docs ‚Üí</h4>
            <p>Concepts apply to DSL</p></a
          >
          <a
            href="https://arxiv.org/abs/1804.06826"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>Volta Microbenchmarking ‚Üí</h4>
            <p>Jia et al. paper</p></a
          >
        </div>

        <h3>Week 3: Quantization</h3>
        <div class="resources">
          <a
            href="https://arxiv.org/abs/2208.07339"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>LLM.int8() ‚Üí</h4>
            <p>Dettmers et al.</p></a
          >
          <a
            href="https://arxiv.org/abs/2211.10438"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>SmoothQuant ‚Üí</h4>
            <p>Xiao et al.</p></a
          >
          <a
            href="https://arxiv.org/abs/2306.00978"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>AWQ ‚Üí</h4>
            <p>Lin et al.</p></a
          >
          <a
            href="https://arxiv.org/abs/2205.14135"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>FlashAttention ‚Üí</h4>
            <p>Dao et al.</p></a
          >
        </div>

        <h3>Week 4: Systems</h3>
        <div class="resources">
          <a
            href="https://github.com/NVIDIA/TensorRT-LLM"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>TensorRT-LLM ‚Üí</h4>
            <p>Source code</p></a
          >
          <a
            href="https://github.com/vllm-project/vllm"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>vLLM ‚Üí</h4>
            <p>Architecture docs</p></a
          >
          <a
            href="https://arxiv.org/abs/2211.05102"
            class="res-link"
            target="_blank"
            rel="noopener"
            ><h4>Scaling Inference ‚Üí</h4>
            <p>Pope et al.</p></a
          >
        </div>
      </section>

      <section class="section" id="checklist">
        <div class="section__head">
          <div class="section__num bg-red">?</div>
          <div>
            <h2>Knowledge Check</h2>
            <p class="mb-0 text-muted">Can you answer these confidently?</p>
          </div>
        </div>

        <ul>
          <li>Why does coalesced memory access matter?</li>
          <li>What determines Tensor Core throughput vs memory throughput?</li>
          <li>Why does block scaling reduce quantization error?</li>
          <li>Why is decode memory-bound while prefill is compute-bound?</li>
          <li>What does TMA buy you over manual loads?</li>
          <li>Why can V tolerate more quantization error than K?</li>
          <li>How does software pipelining hide memory latency?</li>
          <li>What's the NVFP4 two-level scaling mechanism?</li>
        </ul>
      </section>

      <!-- Technical References -->
      <section class="section" id="technical-references">
        <div class="section__head">
          <div class="section__num bg-blue">R</div>
          <div>
            <h2>Technical References</h2>
            <p class="mb-0 text-muted">Sources for specifications and claims</p>
          </div>
        </div>

        <h3>Video Resources</h3>
        <p>These videos provide excellent visual explanations of the concepts covered in this course:</p>

        <div class="grid grid-2">
          <div class="card" style="margin: 0">
            <h4>CUDA Programming Course</h4>
            <p>Comprehensive CUDA programming course covering GPU architecture and kernel development.</p>
            <a href="https://www.youtube.com/watch?v=86FAWCzIe_4" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube (freeCodeCamp)</strong></a>
          </div>
          <div class="card" style="margin: 0">
            <h4>How to Profile CUDA Kernels in PyTorch</h4>
            <p>Practical guide to profiling and optimizing CUDA kernels with Nsight tools.</p>
            <a href="https://www.youtube.com/watch?v=LuhJEEJQgUM" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube</strong></a>
          </div>
        </div>

        <div class="grid grid-2 mt-lg">
          <div class="card" style="margin: 0">
            <h4>How GPU Computing Works</h4>
            <p>Excellent visual explanation of GPU architecture fundamentals and parallelism.</p>
            <a href="https://www.youtube.com/watch?v=h9Z4oGN89MU" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube (Branch Education)</strong></a>
          </div>
          <div class="card" style="margin: 0">
            <h4>FlashAttention Explained</h4>
            <p>Tri Dao explains FlashAttention's IO-aware algorithm and tiling strategy.</p>
            <a href="https://www.youtube.com/watch?v=gMOAud7hZg4" target="_blank" rel="noopener" class="text-blue"><strong>Watch on YouTube (Stanford MLSys)</strong></a>
          </div>
        </div>

        <h3 class="mt-lg">NVIDIA Architecture Documentation</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Topic</th>
              <th>Source</th>
              <th>Link</th>
            </tr>
            <tr>
              <td>CUDA execution model, warps</td>
              <td>CUDA C++ Programming Guide</td>
              <td><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank" rel="noopener">docs.nvidia.com</a></td>
            </tr>
            <tr>
              <td>Hopper SM specs, warpgroups</td>
              <td>Hopper Architecture Whitepaper</td>
              <td><a href="https://resources.nvidia.com/en-us-hopper-architecture" target="_blank" rel="noopener">resources.nvidia.com</a></td>
            </tr>
            <tr>
              <td>H100 specifications</td>
              <td>H100 Tensor Core GPU Datasheet</td>
              <td><a href="https://resources.nvidia.com/en-us-hopper-architecture/nvidia-tensor-core-gpu-datasheet" target="_blank" rel="noopener">H100 Datasheet</a></td>
            </tr>
            <tr>
              <td>Blackwell, NVFP4, FP4/FP6</td>
              <td>Blackwell Architecture Overview</td>
              <td><a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener">nvidia.com</a></td>
            </tr>
            <tr>
              <td>CuTe DSL, CUTLASS</td>
              <td>CUTLASS Documentation</td>
              <td><a href="https://github.com/NVIDIA/cutlass" target="_blank" rel="noopener">github.com/NVIDIA/cutlass</a></td>
            </tr>
          </table>
        </div>

        <h3>Floating Point & Quantization Standards</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Format</th>
              <th>Specification</th>
              <th>Link</th>
            </tr>
            <tr>
              <td>FP32, FP16</td>
              <td>IEEE 754-2019</td>
              <td><a href="https://ieeexplore.ieee.org/document/8766229" target="_blank" rel="noopener">IEEE Xplore</a></td>
            </tr>
            <tr>
              <td>FP8 (E4M3, E5M2)</td>
              <td>FP8 Formats for Deep Learning</td>
              <td><a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener">arXiv:2209.05433</a></td>
            </tr>
            <tr>
              <td>MXFP4, MXFP6, MXFP8</td>
              <td>OCP Microscaling (MX) Spec v1.0</td>
              <td><a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf" target="_blank" rel="noopener">opencompute.org</a></td>
            </tr>
          </table>
        </div>

        <h3>Key Research Papers</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Paper</th>
              <th>Authors</th>
              <th>Link</th>
            </tr>
            <tr>
              <td>Attention Is All You Need</td>
              <td>Vaswani et al., 2017</td>
              <td><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">arXiv:1706.03762</a></td>
            </tr>
            <tr>
              <td>FlashAttention</td>
              <td>Dao et al., 2022</td>
              <td><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">arXiv:2205.14135</a></td>
            </tr>
            <tr>
              <td>FlashAttention-2</td>
              <td>Dao, 2023</td>
              <td><a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener">arXiv:2307.08691</a></td>
            </tr>
            <tr>
              <td>LLM.int8()</td>
              <td>Dettmers et al., 2022</td>
              <td><a href="https://arxiv.org/abs/2208.07339" target="_blank" rel="noopener">arXiv:2208.07339</a></td>
            </tr>
            <tr>
              <td>SmoothQuant</td>
              <td>Xiao et al., 2022</td>
              <td><a href="https://arxiv.org/abs/2211.10438" target="_blank" rel="noopener">arXiv:2211.10438</a></td>
            </tr>
            <tr>
              <td>AWQ</td>
              <td>Lin et al., 2023</td>
              <td><a href="https://arxiv.org/abs/2306.00978" target="_blank" rel="noopener">arXiv:2306.00978</a></td>
            </tr>
            <tr>
              <td>Volta Microbenchmarking</td>
              <td>Jia et al., 2018</td>
              <td><a href="https://arxiv.org/abs/1804.06826" target="_blank" rel="noopener">arXiv:1804.06826</a></td>
            </tr>
          </table>
        </div>

        <h3>Specifications Cited in This Course</h3>
        <div class="table-wrap">
          <table>
            <tr>
              <th>Claim</th>
              <th>Value</th>
              <th>Source</th>
            </tr>
            <tr>
              <td>Warp size</td>
              <td>32 threads</td>
              <td>CUDA Programming Guide, Ch. 4</td>
            </tr>
            <tr>
              <td>Warpgroup size (Hopper+)</td>
              <td>128 threads (4 warps)</td>
              <td>Hopper Architecture Whitepaper</td>
            </tr>
            <tr>
              <td>Blackwell B200 SMEM</td>
              <td>228 KB/SM</td>
              <td>Blackwell Architecture Overview</td>
            </tr>
            <tr>
              <td>Blackwell B200 HBM</td>
              <td>192 GB</td>
              <td>GB200 NVL72 Specifications</td>
            </tr>
            <tr>
              <td>Blackwell B200 HBM bandwidth</td>
              <td>~8 TB/s</td>
              <td>GB200 NVL72 Specifications</td>
            </tr>
            <tr>
              <td>SMEM bank count</td>
              <td>32 banks</td>
              <td>CUDA Programming Guide, Ch. 5.3.2</td>
            </tr>
            <tr>
              <td>Volta WMMA shape</td>
              <td>16x16x16 (FP16)</td>
              <td>CUDA WMMA API Documentation</td>
            </tr>
            <tr>
              <td>FP8 E4M3 range</td>
              <td>¬±448</td>
              <td>arXiv:2209.05433, Table 1</td>
            </tr>
            <tr>
              <td>FP4 E2M1 range</td>
              <td>¬±6</td>
              <td>OCP MX Spec v1.0, Section 5</td>
            </tr>
            <tr>
              <td>NVFP4 block size</td>
              <td>16 elements</td>
              <td>NVIDIA Blackwell Documentation</td>
            </tr>
            <tr>
              <td>MXFP4 block size</td>
              <td>32 elements</td>
              <td>OCP MX Spec v1.0, Section 4</td>
            </tr>
            <tr>
              <td>NVFP4 scale format</td>
              <td>E4M3 (fractional)</td>
              <td>NVIDIA Blackwell Documentation</td>
            </tr>
            <tr>
              <td>MXFP4 scale format</td>
              <td>E8M0 (power-of-2)</td>
              <td>OCP MX Spec v1.0, Section 4</td>
            </tr>
            <tr>
              <td>128K attention matrix size</td>
              <td>~16 billion elements/head</td>
              <td>128K √ó 128K = 16.4B (calculation)</td>
            </tr>
            <tr>
              <td>FP8 KV cache compression</td>
              <td>2.0x vs FP16</td>
              <td>16-bit ‚Üí 8-bit (calculation)</td>
            </tr>
            <tr>
              <td>Mixed K(FP8)/V(NVFP4) compression</td>
              <td>~2.6x vs FP16</td>
              <td>Average of 2x (K) and 4x (V), weighted</td>
            </tr>
          </table>
        </div>

        <div class="callout warn mt-lg">
          <div class="callout-title">Note on Latency Values</div>
          <p class="mb-0">
            Memory latency figures (cycles) are approximate and vary significantly based on access pattern,
            cache hit rate, memory contention, and specific workload characteristics. The values shown
            represent typical measured ranges from microbenchmarks, not guaranteed specifications.
            Bandwidth numbers represent theoretical peak values. Always profile your specific kernel
            to understand actual performance characteristics.
          </p>
        </div>

        <div class="callout info mt-lg">
          <div class="callout-title">Additional Resources</div>
          <p class="mb-0">
            For the latest specifications, consult the official
            <a href="https://docs.nvidia.com/cuda/" target="_blank" rel="noopener">NVIDIA CUDA Documentation</a> and
            <a href="https://developer.nvidia.com/cuda-gpus" target="_blank" rel="noopener">GPU Compute Capability tables</a>.
            Architecture-specific details are available in the respective whitepapers linked above.
          </p>
        </div>
      </section>
    </main>

    <script src="scripts/components.js"></script>
  </body>
</html>
