<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Attention Math ‚Äî GPU Learning</title>
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="base.css" />
    <style>
      /* Lesson page - no sidebar */
      body {
        display: block;
      }

      .lesson-nav {
        position: fixed;
        top: 0;
        left: 0;
        right: 0;
        background: rgba(10, 10, 15, 0.9);
        backdrop-filter: blur(20px);
        border-bottom: 1px solid var(--border-subtle);
        z-index: 100;
        padding: var(--space-md) var(--space-xl);
      }

      .lesson-nav__inner {
        max-width: 800px;
        margin: 0 auto;
        display: flex;
        justify-content: space-between;
        align-items: center;
      }

      .lesson-nav__title {
        font-family: var(--font-mono);
        font-size: 0.85rem;
        color: var(--text-secondary);
        letter-spacing: 0.05em;
      }

      .lesson-container {
        max-width: 800px;
        margin: 0 auto;
        padding: 6rem var(--space-xl) var(--space-3xl);
      }

      .vector-row {
        display: flex;
        align-items: center;
        gap: var(--space-md);
        margin-bottom: var(--space-md);
      }

      .vector-row__label {
        font-family: var(--font-mono);
        font-size: 0.85rem;
        color: var(--text-secondary);
        width: 80px;
      }

      .vector-row__inputs {
        display: flex;
        gap: var(--space-sm);
      }
    </style>
  </head>
  <body>
    <!-- Navigation -->
    <nav class="lesson-nav">
      <div class="lesson-nav__inner">
        <a href="index.html" class="lesson-nav__title"
          >‚Üê COURSE INDEX</a
        >
      </div>
    </nav>

    <div class="lesson-container">
      <!-- Hero -->
      <header class="hero hero--lesson">
        <div class="hero__label">Decode Attention Kernel Prereqs</div>
        <h1 class="hero__title">The Math Behind Attention</h1>
        <p class="hero__desc">
          Interactive foundations for writing your own CUDA attention kernel. No
          handwaving‚Äîunderstand what every operation actually does.
        </p>
      </header>

      <!-- Section 1: Dot Product -->
      <section class="section" id="section-0">
        <div class="section__number">01 ‚Äî SIMILARITY</div>
        <h2 class="section__title">The Dot Product: Measuring Relevance</h2>

        <p>
          Attention computes <strong>how relevant</strong> each cached token is
          to your query. The dot product is the measuring stick‚Äîit tells you how
          much two vectors "agree."
        </p>
        <p>
          High dot product = vectors point in similar directions = high
          relevance.<br />
          Zero dot product = vectors are perpendicular = no relationship.
        </p>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--blue">‚ö°</div>
            <span class="card__title">Interactive: Compute Q¬∑K</span>
          </div>

          <div class="vector-row">
            <span class="vector-row__label">Query (Q)</span>
            <div class="vector-row__inputs">
              <input
                type="number"
                class="input"
                id="q0"
                value="1"
                step="0.1"
                style="border-color: var(--accent-blue)"
              />
              <input
                type="number"
                class="input"
                id="q1"
                value="0"
                step="0.1"
                style="border-color: var(--accent-blue)"
              />
              <input
                type="number"
                class="input"
                id="q2"
                value="0.5"
                step="0.1"
                style="border-color: var(--accent-blue)"
              />
              <input
                type="number"
                class="input"
                id="q3"
                value="-0.5"
                step="0.1"
                style="border-color: var(--accent-blue)"
              />
            </div>
          </div>
          <div class="vector-row">
            <span class="vector-row__label">Key (K)</span>
            <div class="vector-row__inputs">
              <input
                type="number"
                class="input"
                id="k0"
                value="0.5"
                step="0.1"
                style="border-color: var(--accent-orange)"
              />
              <input
                type="number"
                class="input"
                id="k1"
                value="0"
                step="0.1"
                style="border-color: var(--accent-orange)"
              />
              <input
                type="number"
                class="input"
                id="k2"
                value="1"
                step="0.1"
                style="border-color: var(--accent-orange)"
              />
              <input
                type="number"
                class="input"
                id="k3"
                value="0"
                step="0.1"
                style="border-color: var(--accent-orange)"
              />
            </div>
          </div>

          <div class="result">
            <div class="result__label">Dot Product (Q ¬∑ K)</div>
            <div class="result__value" id="dot-result">1.00</div>
            <div class="result__formula" id="dot-formula">
              (1√ó0.5) + (0√ó0) + (0.5√ó1) + (-0.5√ó0) = 1.00
            </div>
          </div>
        </div>

        <p>
          In attention, you compute Q¬∑K for <strong>every cached token</strong>.
          With a 4096-token context, that's 4096 dot products just to process
          one query. This is why <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">attention is the bottleneck</a>.
        </p>

      </section>

      <!-- Section 2: Softmax -->
      <section class="section" id="section-1">
        <div class="section__number">02 ‚Äî NORMALIZATION</div>
        <h2 class="section__title">Softmax: From Scores to Probabilities</h2>

        <p>
          Raw dot products can be any value‚Äîpositive, negative, huge, tiny.
          <strong>Softmax</strong> converts them to a probability distribution:
          all positive, sums to 1.
        </p>

        <div class="equation">
          softmax(x<sub>i</sub>) =
          <span class="equation__highlight">exp(x<sub>i</sub>)</span> / Œ£
          exp(x<sub>j</sub>)
        </div>

        <p>
          The exponential amplifies differences. A score of 10 vs 5 becomes
          e<sup>10</sup>/e<sup>5</sup> ‚âà 150√ó more weight, not 2√ó. This makes
          attention "sharp"‚Äîit focuses on the most relevant tokens.
        </p>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--green">üìä</div>
            <span class="card__title">Interactive: Softmax Visualization</span>
          </div>

          <div
            class="flex flex-wrap gap-md"
            style="margin-bottom: var(--space-lg)"
          >
            <div style="text-align: center">
              <span class="text-muted text-small">Score 1</span>
              <input
                type="number"
                class="input"
                id="s0"
                value="2"
                step="0.5"
                style="margin-top: var(--space-sm)"
              />
            </div>
            <div style="text-align: center">
              <span class="text-muted text-small">Score 2</span>
              <input
                type="number"
                class="input"
                id="s1"
                value="1"
                step="0.5"
                style="margin-top: var(--space-sm)"
              />
            </div>
            <div style="text-align: center">
              <span class="text-muted text-small">Score 3</span>
              <input
                type="number"
                class="input"
                id="s2"
                value="0.5"
                step="0.5"
                style="margin-top: var(--space-sm)"
              />
            </div>
            <div style="text-align: center">
              <span class="text-muted text-small">Score 4</span>
              <input
                type="number"
                class="input"
                id="s3"
                value="-1"
                step="0.5"
                style="margin-top: var(--space-sm)"
              />
            </div>
            <div style="text-align: center">
              <span class="text-muted text-small">Score 5</span>
              <input
                type="number"
                class="input"
                id="s4"
                value="0"
                step="0.5"
                style="margin-top: var(--space-sm)"
              />
            </div>
          </div>

          <div class="bar-chart">
            <div class="bar-chart__item">
              <div class="bar-chart__bar" id="bar0"></div>
              <span class="bar-chart__value" id="prob0">0.50</span>
            </div>
            <div class="bar-chart__item">
              <div class="bar-chart__bar" id="bar1"></div>
              <span class="bar-chart__value" id="prob1">0.18</span>
            </div>
            <div class="bar-chart__item">
              <div class="bar-chart__bar" id="bar2"></div>
              <span class="bar-chart__value" id="prob2">0.11</span>
            </div>
            <div class="bar-chart__item">
              <div class="bar-chart__bar" id="bar3"></div>
              <span class="bar-chart__value" id="prob3">0.02</span>
            </div>
            <div class="bar-chart__item">
              <div class="bar-chart__bar" id="bar4"></div>
              <span class="bar-chart__value" id="prob4">0.07</span>
            </div>
          </div>
        </div>

        <div class="quiz" id="softmax-quiz">
          <div class="quiz-q">
            Try setting Score 1 to 100. What happens to the other probabilities?
          </div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>They decrease proportionally</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>They collapse to nearly zero</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>They stay the same</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>

        <p>
          <strong>The problem:</strong> exp(100) = 2.7 √ó 10<sup>43</sup>. That
          overflows <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format" target="_blank" rel="noopener">FP16 (max ‚âà 65504)</a>. Your kernel crashes. This is why we need
          the numerical stability trick.
        </p>

      </section>

      <!-- Section 3: Online Softmax -->
      <section class="section" id="section-2">
        <div class="section__number">03 ‚Äî THE TRICK</div>
        <h2 class="section__title">
          Online Softmax: Streaming Without Overflow
        </h2>

        <p>Two problems with naive softmax:</p>
        <p>
          <strong>1. Overflow:</strong> Large values explode exp().
          <strong>Solution:</strong> Subtract max first.
        </p>
        <p>
          <strong>2. Memory:</strong> You need to see ALL values to compute max.
          But in attention, you're processing in blocks to stay in fast SRAM.
          <strong>Solution:</strong> Online algorithm that updates
          incrementally.
        </p>

        <div class="equation">
          softmax(x<sub>i</sub>) = exp(x<sub>i</sub> -
          <span class="equation__highlight">max(x)</span>) / Œ£ exp(x<sub>j</sub>
          - <span class="equation__highlight">max(x)</span>)
        </div>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--blue">üîÑ</div>
            <span class="card__title"
              >Simulation: Online Softmax Streaming</span
            >
          </div>

          <p
            class="text-secondary text-small"
            style="margin-bottom: var(--space-lg)"
          >
            Watch how the algorithm maintains running statistics as new values
            stream in. This is how <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention</a> processes attention in blocks.
          </p>

          <div class="flex gap-md" style="margin-bottom: var(--space-lg)">
            <button class="btn btn--primary" id="add-block-btn">
              Add Random Block
            </button>
            <button class="btn" id="reset-btn">Reset</button>
          </div>

          <div
            class="stream"
            id="stream-display"
            style="margin-bottom: var(--space-lg)"
          >
            <span class="text-muted text-small"
              >Values will appear here...</span
            >
          </div>

          <div class="state-grid">
            <div class="state-box">
              <div class="state-box__label">Running Max (m)</div>
              <div class="state-box__value" id="state-max">-‚àû</div>
            </div>
            <div class="state-box">
              <div class="state-box__label">Sum of Exp (l)</div>
              <div class="state-box__value" id="state-sum">0.00</div>
            </div>
            <div class="state-box">
              <div class="state-box__label">Values Seen</div>
              <div class="state-box__value" id="state-count">0</div>
            </div>
          </div>

          <div class="result mt-lg">
            <div class="result__label">Key Insight</div>
            <p
              class="text-secondary text-small"
              id="online-insight"
              style="margin: 0"
            >
              Click "Add Random Block" to start the simulation.
            </p>
          </div>
        </div>

        <pre><code><span style="color: var(--text-muted)"># The online softmax update rule</span>
<span style="color: var(--accent-purple)">def</span> <span style="color: var(--accent-blue)">update</span>(m_old, l_old, new_block):
    m_block = new_block.<span style="color: var(--accent-blue)">max</span>()
    m_new = <span style="color: var(--accent-blue)">max</span>(m_old, m_block)
    
    <span style="color: var(--text-muted)"># Rescale old accumulator to new max</span>
    l_new = l_old * <span style="color: var(--accent-blue)">exp</span>(m_old - m_new)
    
    <span style="color: var(--text-muted)"># Add new block contribution</span>
    l_new += <span style="color: var(--accent-blue)">sum</span>(<span style="color: var(--accent-blue)">exp</span>(new_block - m_new))
    
    <span style="color: var(--accent-purple)">return</span> m_new, l_new</code></pre>

      </section>

      <!-- Section 4: Floating Point Formats -->
      <section class="section" id="section-3">
        <div class="section__number">04 ‚Äî PRECISION</div>
        <h2 class="section__title">
          Floating Point: What FP8 and NVFP4 Actually Are
        </h2>

        <p>
          Your KV cache is quantized. Understanding the bit layout tells you
          what precision you're trading for memory bandwidth.
        </p>

        <div class="diagram" id="fp-card">
          <div class="diagram-title">Floating Point Formats</div>
          <div class="fp-formats">
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format" target="_blank" rel="noopener" class="fp-name">FP16 (Half Precision)</a
                ><span class="fp-range">¬±65,504</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 5">Exp(5)</div>
                <div class="fp-bit fp-m" style="flex: 10">Mantissa(10)</div>
              </div>
            </div>
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener" class="fp-name">FP8 E4M3 (KV cache)</a
                ><span class="fp-range">¬±448</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 4">E(4)</div>
                <div class="fp-bit fp-m" style="flex: 3">M(3)</div>
              </div>
            </div>
            <div class="fp-row">
              <div class="fp-head">
                <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener" class="fp-name">NVFP4 E2M1 (Blackwell)</a
                ><span class="fp-range">¬±6</span>
              </div>
              <div class="fp-bits">
                <div class="fp-bit fp-s">S</div>
                <div class="fp-bit fp-e" style="flex: 2">E</div>
                <div class="fp-bit fp-m" style="flex: 1">M</div>
              </div>
            </div>
          </div>
          <p
            class="text-muted text-small"
            style="margin-top: var(--space-md); margin-bottom: 0"
          >
            <span
              style="
                display: inline-block;
                width: 12px;
                height: 12px;
                background: var(--accent-red);
                border-radius: 3px;
                margin-right: 4px;
              "
            ></span>
            Sign
            <span
              style="
                display: inline-block;
                width: 12px;
                height: 12px;
                background: var(--accent-blue);
                border-radius: 3px;
                margin: 0 4px 0 12px;
              "
            ></span>
            Exponent
            <span
              style="
                display: inline-block;
                width: 12px;
                height: 12px;
                background: var(--accent-green);
                border-radius: 3px;
                margin: 0 4px 0 12px;
              "
            ></span>
            Mantissa
          </p>
        </div>

        <p>
          <strong>Why NVFP4 works:</strong> Neural network values cluster
          tightly. A per-block scaling factor (stored in FP8) shifts the
          representable range to where your values actually are. You get <a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener">4√ó memory reduction vs FP16 with ~1% accuracy loss</a>.
        </p>

        <div class="quiz" id="fp-quiz">
          <div class="quiz-q">
            With only 1 mantissa bit (E2M1), how many distinct positive values
            can NVFP4 represent (excluding zero and special values)?
          </div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>4</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>6</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>8</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>16</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>

      </section>

      <!-- Section 5: Full Attention -->
      <section class="section" id="section-4">
        <div class="section__number">05 ‚Äî SYNTHESIS</div>
        <h2 class="section__title">Putting It Together: Full Attention</h2>

        <p>
          Now you have all the pieces. Here's the complete attention equation
          and what each part does:
        </p>

        <div class="equation">
          <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener" style="text-decoration: none; color: inherit;">Attention(Q, K, V) = softmax(<span class="equation__highlight"
            >QK<sup>T</sup></span
          >
          / ‚àöd) ¬∑ V</a>
        </div>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--blue">üéØ</div>
            <span class="card__title"
              >Visualization: Decode Attention Step-by-Step</span
            >
          </div>

          <canvas
            id="attention-canvas"
            class="canvas"
            width="700"
            height="280"
          ></canvas>

          <div class="flex flex-wrap gap-sm mt-lg">
            <button class="btn" id="step-qk">1. Compute QK<sup>T</sup></button>
            <button class="btn" id="step-scale">2. Scale by ‚àöd</button>
            <button class="btn" id="step-softmax">3. Softmax</button>
            <button class="btn" id="step-output">4. Weighted Sum</button>
            <button class="btn" id="step-reset">Reset</button>
          </div>

          <div class="result mt-lg">
            <div class="result__label">Current Step</div>
            <p
              class="text-secondary text-small"
              id="attention-step-desc"
              style="margin: 0"
            >
              Click a step to see what happens at each stage of attention.
            </p>
          </div>
        </div>

        <p><strong>What your kernel must do efficiently:</strong></p>
        <p>
          1. Load Q (single vector for decode)<br />
          2. Stream through KV cache in blocks (fits in SRAM)<br />
          3. Compute dot products, track online softmax statistics<br />
          4. Accumulate weighted V vectors<br />
          5. Output final attention result
        </p>
        <p>
          The bottleneck is <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener"><strong>memory bandwidth</strong></a>‚Äîloading all those
          K and V vectors from HBM. FP8/NVFP4 quantization halves or quarters
          that traffic.
        </p>

      </section>

      <!-- Completion -->
      <section
        class="section"
        style="text-align: center; padding: var(--space-2xl) 0"
      >
        <div class="section__number">NEXT STEPS</div>
        <h2 class="section__title">Ready for Implementation</h2>
        <p style="max-width: 500px; margin: 0 auto var(--space-xl)">
          You now have the mathematical foundation. Next: profile TRT-LLM's
          stock attention, read <a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener">FlashAttention-2 Section 3</a>, and start writing
          your first Triton kernel.
        </p>
        <a href="index.html" class="btn btn--primary">Back to Course Index</a>
        <a
          href="kernel-course.html"
          class="btn"
          style="margin-left: var(--space-sm)"
          >View Full Course</a
        >
      </section>

      <!-- References -->
      <section class="section" id="references">
        <div class="section__number">REFERENCES</div>
        <h2 class="section__title">Citations & Further Reading</h2>

        <!-- Video Resources -->
        <div class="card">
          <h4>Video Resources</h4>
          <p class="text-secondary text-small" style="margin-bottom: var(--space-md);">
            Visual explanations of attention mechanisms and transformers.
          </p>
          
          <div style="margin-bottom: var(--space-lg);">
            <strong>Attention in Transformers (3Blue1Brown)</strong>
            <p class="text-muted text-small" style="margin: var(--space-xs) 0;">
              Outstanding visual explanation of attention, QKV, and the transformer architecture.
            </p>
            <a href="https://www.youtube.com/watch?v=eMlx5fFNoYc" target="_blank" rel="noopener" style="color: var(--accent-blue);">
              Watch on YouTube
            </a>
          </div>

          <div style="margin-bottom: var(--space-lg);">
            <strong>Let's Build GPT: From Scratch (Andrej Karpathy)</strong>
            <p class="text-muted text-small" style="margin: var(--space-xs) 0;">
              Build a transformer from scratch with detailed attention implementation.
            </p>
            <a href="https://www.youtube.com/watch?v=kCc8FmEb1nY" target="_blank" rel="noopener" style="color: var(--accent-blue);">
              Watch on YouTube
            </a>
          </div>

          <div style="margin-bottom: var(--space-md);">
            <strong>FlashAttention Explained (Yannic Kilcher)</strong>
            <p class="text-muted text-small" style="margin: var(--space-xs) 0;">
              Technical paper walkthrough of FlashAttention algorithm and IO-awareness.
            </p>
            <a href="https://www.youtube.com/watch?v=gMOAud7hZg4" target="_blank" rel="noopener" style="color: var(--accent-blue);">
              Watch on YouTube
            </a>
          </div>
        </div>

        <div class="card" style="margin-top: var(--space-lg);">
          <h4>Foundational Papers</h4>
          <ol style="margin: 0; padding-left: var(--space-lg); color: var(--text-secondary);">
            <li style="margin-bottom: var(--space-sm);">
              <strong>Attention Is All You Need</strong> - Vaswani et al., NeurIPS 2017<br>
              The original transformer architecture paper. Defines scaled dot-product attention: Attention(Q,K,V) = softmax(QK^T / sqrt(d)) V<br>
              <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener" style="color: var(--accent-blue);">arXiv:1706.03762</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</strong> - Dao et al., NeurIPS 2022<br>
              Introduces tiled attention with online softmax, reducing memory from O(N^2) to O(N)<br>
              <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener" style="color: var(--accent-blue);">arXiv:2205.14135</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</strong> - Dao, 2023<br>
              Improved parallelization achieving 2x speedup over FlashAttention-1<br>
              <a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener" style="color: var(--accent-blue);">arXiv:2307.08691</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision</strong> - Shah et al., 2024<br>
              Hopper-optimized attention with TMA and FP8 support<br>
              <a href="https://arxiv.org/abs/2407.08608" target="_blank" rel="noopener" style="color: var(--accent-blue);">arXiv:2407.08608</a>
            </li>
          </ol>
        </div>

        <div class="card" style="margin-top: var(--space-lg);">
          <h4>Floating Point Standards & Specifications</h4>
          <ol style="margin: 0; padding-left: var(--space-lg); color: var(--text-secondary);">
            <li style="margin-bottom: var(--space-sm);">
              <strong>IEEE 754-2019</strong> - Standard for Floating-Point Arithmetic<br>
              Defines FP32 (binary32) and FP16 (binary16) formats. FP16 max value: 65504<br>
              <a href="https://ieeexplore.ieee.org/document/8766229" target="_blank" rel="noopener" style="color: var(--accent-blue);">IEEE 754-2019</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>FP8 Formats for Deep Learning</strong> - Micikevicius et al., 2022<br>
              Defines E4M3 (max: 448) and E5M2 (max: 57344) formats. Joint NVIDIA/ARM/Intel specification.<br>
              <a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener" style="color: var(--accent-blue);">arXiv:2209.05433</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>OCP Microscaling (MX) Formats Specification v1.0</strong> - Open Compute Project, 2023<br>
              Defines MXFP4, MXFP6, MXFP8 with block scaling (32 elements per block)<br>
              <a href="https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf" target="_blank" rel="noopener" style="color: var(--accent-blue);">OCP MX Specification (PDF)</a>
            </li>
            <li style="margin-bottom: var(--space-sm);">
              <strong>NVIDIA Blackwell Architecture - NVFP4</strong><br>
              NVIDIA's FP4 format with 16-element blocks and E4M3 scaling factors<br>
              <a href="https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/" target="_blank" rel="noopener" style="color: var(--accent-blue);">nvidia.com/blackwell-architecture</a>
            </li>
          </ol>
        </div>

        <div class="card" style="margin-top: var(--space-lg);">
          <h4>Key Facts with Sources</h4>
          <table style="width: 100%; font-size: 0.85rem; border-collapse: collapse;">
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <th style="text-align: left; padding: var(--space-sm);">Fact</th>
              <th style="text-align: left; padding: var(--space-sm);">Value</th>
              <th style="text-align: left; padding: var(--space-sm);">Source</th>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">Attention formula</td>
              <td style="padding: var(--space-sm);">softmax(QK^T/sqrt(d))V</td>
              <td style="padding: var(--space-sm);"><a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">Vaswani et al., 2017, Eq. 1</a></td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">FP16 max value</td>
              <td style="padding: var(--space-sm);">65504</td>
              <td style="padding: var(--space-sm);"><a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format" target="_blank" rel="noopener">IEEE 754-2019</a></td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">FP8 E4M3 max value</td>
              <td style="padding: var(--space-sm);">448</td>
              <td style="padding: var(--space-sm);"><a href="https://arxiv.org/abs/2209.05433" target="_blank" rel="noopener">Micikevicius et al., 2022</a></td>
            </tr>
            <tr style="border-bottom: 1px solid var(--border-subtle);">
              <td style="padding: var(--space-sm);">FlashAttention memory</td>
              <td style="padding: var(--space-sm);">O(N) not O(N^2)</td>
              <td style="padding: var(--space-sm);"><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">Dao et al., 2022, Theorem 2</a></td>
            </tr>
            <tr>
              <td style="padding: var(--space-sm);">Online softmax</td>
              <td style="padding: var(--space-sm);">Incremental max tracking</td>
              <td style="padding: var(--space-sm);"><a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">Dao et al., 2022, Section 3.1</a></td>
            </tr>
          </table>
        </div>
      </section>
    </div>

    <script src="scripts/components.js"></script>
    <script>
      // Dot Product Interactive
      new GPULearning.LiveComputation({
        inputs: ["q0", "q1", "q2", "q3", "k0", "k1", "k2", "k3"],
        compute: (values) => {
          const q = values.slice(0, 4);
          const k = values.slice(4, 8);
          return q.reduce((sum, qi, i) => sum + qi * k[i], 0);
        },
        render: (result, values) => {
          const q = values.slice(0, 4);
          const k = values.slice(4, 8);
          document.getElementById("dot-result").textContent = result.toFixed(2);
          const terms = q.map((qi, i) => `(${qi}√ó${k[i]})`).join(" + ");
          document.getElementById("dot-formula").textContent =
            `${terms} = ${result.toFixed(2)}`;
        },
      });

      // Softmax Visualization
      new GPULearning.SoftmaxViz({
        inputs: ["s0", "s1", "s2", "s3", "s4"],
        bars: ["bar0", "bar1", "bar2", "bar3", "bar4"],
        values: ["prob0", "prob1", "prob2", "prob3", "prob4"],
      });

      // Online Softmax Simulation
      new GPULearning.OnlineSoftmaxSim({
        display: "stream-display",
        maxDisplay: "state-max",
        sumDisplay: "state-sum",
        countDisplay: "state-count",
        insightDisplay: "online-insight",
        addBtn: "add-block-btn",
        resetBtn: "reset-btn",
      });

      // Attention Canvas Visualization
      const canvas = document.getElementById("attention-canvas");
      const ctx = canvas.getContext("2d");

      const colors = {
        bg: "#1a1a24",
        text: "#e8e6e3",
        textMuted: "#6a6a6a",
        blue: "#4d9fff",
        purple: "#a78bfa",
        green: "#34d399",
        orange: "#fb923c",
      };

      const vizData = {
        Q: [0.8, 0.2, -0.5, 0.3],
        K: [
          [0.5, 0.3, -0.2, 0.1],
          [0.9, 0.1, -0.4, 0.2],
          [0.1, 0.8, 0.3, -0.1],
          [0.4, 0.5, -0.3, 0.6],
        ],
        V: [
          [1.0, 0.2],
          [0.3, 0.8],
          [0.5, 0.5],
          [0.7, 0.1],
        ],
      };

      let currentStep = 0;

      function drawAttention(step) {
        ctx.fillStyle = colors.bg;
        ctx.fillRect(0, 0, canvas.width, canvas.height);

        const padding = 30;
        const boxW = 55;
        const boxH = 28;

        ctx.fillStyle = colors.blue;
        ctx.font = "11px IBM Plex Mono";
        ctx.fillText("Q (query)", padding, 25);
        ctx.strokeStyle = colors.blue;
        ctx.lineWidth = 2;
        ctx.strokeRect(padding, 35, boxW * 2, boxH);
        ctx.fillStyle = colors.textMuted;
        ctx.font = "9px IBM Plex Mono";
        ctx.fillText("[0.8, 0.2, ...]", padding + 5, 53);

        ctx.fillStyle = colors.orange;
        ctx.font = "11px IBM Plex Mono";
        ctx.fillText("K cache", padding + 180, 25);
        for (let i = 0; i < 4; i++) {
          ctx.strokeStyle = step >= 1 ? colors.orange : colors.textMuted;
          ctx.strokeRect(padding + 180, 35 + i * 32, boxW * 2, boxH);
          ctx.fillStyle = colors.textMuted;
          ctx.font = "9px IBM Plex Mono";
          ctx.fillText(`k${i}`, padding + 185, 53 + i * 32);
        }

        ctx.fillStyle = colors.green;
        ctx.font = "11px IBM Plex Mono";
        ctx.fillText("V cache", padding + 330, 25);
        for (let i = 0; i < 4; i++) {
          ctx.strokeStyle = step >= 4 ? colors.green : colors.textMuted;
          ctx.strokeRect(padding + 330, 35 + i * 32, boxW, boxH);
          ctx.fillStyle = colors.textMuted;
          ctx.font = "9px IBM Plex Mono";
          ctx.fillText(`v${i}`, padding + 335, 53 + i * 32);
        }

        ctx.fillStyle = colors.purple;
        ctx.font = "11px IBM Plex Mono";
        ctx.fillText(step >= 3 ? "Weights" : "Scores", padding + 440, 25);

        let scores = vizData.K.map((k) =>
          vizData.Q.reduce((sum, q, i) => sum + q * k[i], 0),
        );

        if (step >= 2) {
          const scale = Math.sqrt(vizData.Q.length);
          scores = scores.map((s) => s / scale);
        }

        let weights = scores;
        if (step >= 3) {
          const maxScore = Math.max(...scores);
          const exps = scores.map((s) => Math.exp(s - maxScore));
          const sumExp = exps.reduce((a, b) => a + b, 0);
          weights = exps.map((e) => e / sumExp);
        }

        for (let i = 0; i < 4; i++) {
          ctx.strokeStyle = step >= 1 ? colors.purple : colors.textMuted;
          ctx.strokeRect(padding + 440, 35 + i * 32, boxW, boxH);

          if (step >= 1) {
            ctx.fillStyle = colors.text;
            ctx.font = "9px IBM Plex Mono";
            const val =
              step >= 3 ? weights[i].toFixed(3) : scores[i].toFixed(2);
            ctx.fillText(val, padding + 450, 53 + i * 32);
          }
        }

        if (step >= 4) {
          ctx.fillStyle = colors.green;
          ctx.font = "11px IBM Plex Mono";
          ctx.fillText("Output", padding + 560, 25);
          ctx.strokeStyle = colors.green;
          ctx.lineWidth = 3;
          ctx.strokeRect(padding + 560, 60, boxW * 1.3, boxH * 1.5);

          const output = [0, 0];
          vizData.V.forEach((v, i) => {
            output[0] += weights[i] * v[0];
            output[1] += weights[i] * v[1];
          });

          ctx.fillStyle = colors.text;
          ctx.font = "10px IBM Plex Mono";
          ctx.fillText(`[${output[0].toFixed(2)},`, padding + 568, 78);
          ctx.fillText(` ${output[1].toFixed(2)}]`, padding + 568, 92);
        }
      }

      const stepDescs = [
        "Click a step to see what happens at each stage of attention.",
        "Step 1: Compute dot products QK^T. Each score measures how relevant that cached token is to our query.",
        "Step 2: Scale by ‚àöd (here ‚àö4 = 2). This prevents dot products from getting too large and saturating softmax.",
        "Step 3: Apply softmax. Scores become probabilities that sum to 1. Notice how the highest score dominates.",
        "Step 4: Compute weighted sum of V vectors. Each V contributes proportionally to its attention weight.",
      ];

      document.getElementById("step-qk").onclick = () => {
        currentStep = 1;
        drawAttention(1);
        document.getElementById("attention-step-desc").textContent =
          stepDescs[1];
      };
      document.getElementById("step-scale").onclick = () => {
        currentStep = 2;
        drawAttention(2);
        document.getElementById("attention-step-desc").textContent =
          stepDescs[2];
      };
      document.getElementById("step-softmax").onclick = () => {
        currentStep = 3;
        drawAttention(3);
        document.getElementById("attention-step-desc").textContent =
          stepDescs[3];
      };
      document.getElementById("step-output").onclick = () => {
        currentStep = 4;
        drawAttention(4);
        document.getElementById("attention-step-desc").textContent =
          stepDescs[4];
      };
      document.getElementById("step-reset").onclick = () => {
        currentStep = 0;
        drawAttention(0);
        document.getElementById("attention-step-desc").textContent =
          stepDescs[0];
      };

      drawAttention(0);
    </script>
  </body>
</html>
