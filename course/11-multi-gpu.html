<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chapter 11: Multi-GPU Programming - GPU Learning</title>
    <link rel="icon" type="image/x-icon" href="../favicon.ico" />
    <link rel="icon" type="image/svg+xml" href="../favicon.svg" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../base.css" />
    <style>
      body { display: block; }

      /* Topology diagrams */
      .topology-diagram {
        display: flex;
        flex-direction: column;
        gap: var(--space-md);
        margin: var(--space-lg) 0;
        padding: var(--space-lg);
        background: var(--bg-secondary);
        border-radius: var(--radius-lg);
      }

      .topology-diagram__gpus {
        display: flex;
        justify-content: center;
        gap: var(--space-lg);
        flex-wrap: wrap;
      }

      .topology-diagram__gpu {
        width: 80px;
        height: 60px;
        background: var(--glow-blue);
        border: 2px solid var(--accent-blue);
        border-radius: var(--radius-md);
        display: flex;
        align-items: center;
        justify-content: center;
        font-family: var(--font-mono);
        font-weight: 600;
        font-size: var(--text-sm);
      }

      .topology-diagram__switch {
        width: 120px;
        height: 40px;
        background: var(--glow-purple);
        border: 2px solid var(--accent-purple);
        border-radius: var(--radius-md);
        display: flex;
        align-items: center;
        justify-content: center;
        font-family: var(--font-mono);
        font-size: var(--text-xs);
        margin: 0 auto;
      }

      .topology-diagram__label {
        text-align: center;
        font-size: var(--text-sm);
        color: var(--text-muted);
      }

      .topology-diagram__bandwidth {
        font-family: var(--font-mono);
        color: var(--accent-green);
      }

      /* Parallelism comparison */
      .parallel-comparison {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
        gap: var(--space-md);
        margin: var(--space-lg) 0;
      }

      .parallel-card {
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-lg);
        overflow: hidden;
      }

      .parallel-card__header {
        padding: var(--space-md);
        font-weight: 600;
        font-size: var(--text-sm);
      }

      .parallel-card__header--data {
        background: var(--glow-blue);
        color: var(--accent-blue);
      }

      .parallel-card__header--tensor {
        background: var(--glow-purple);
        color: var(--accent-purple);
      }

      .parallel-card__header--pipeline {
        background: var(--glow-green);
        color: var(--accent-green);
      }

      .parallel-card__content {
        padding: var(--space-md);
      }

      .parallel-card__diagram {
        display: flex;
        gap: 4px;
        margin: var(--space-sm) 0;
      }

      .parallel-card__block {
        flex: 1;
        height: 40px;
        border-radius: var(--radius-sm);
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: var(--text-xs);
        font-family: var(--font-mono);
      }

      .parallel-card__block--gpu0 { background: var(--accent-blue); color: white; }
      .parallel-card__block--gpu1 { background: var(--accent-purple); color: white; }
      .parallel-card__block--gpu2 { background: var(--accent-green); color: white; }
      .parallel-card__block--gpu3 { background: var(--accent-orange); color: white; }

      /* NCCL operations visualization */
      .nccl-viz {
        margin: var(--space-lg) 0;
        padding: var(--space-md);
        background: var(--bg-secondary);
        border-radius: var(--radius-md);
      }

      .nccl-viz__title {
        font-weight: 600;
        margin-bottom: var(--space-md);
        font-size: var(--text-sm);
      }

      .nccl-viz__row {
        display: flex;
        align-items: center;
        margin-bottom: var(--space-sm);
      }

      .nccl-viz__label {
        width: 80px;
        font-size: var(--text-xs);
        color: var(--text-muted);
      }

      .nccl-viz__gpus {
        display: flex;
        gap: 4px;
        flex: 1;
      }

      .nccl-viz__cell {
        width: 48px;
        height: 32px;
        border-radius: var(--radius-sm);
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: var(--text-xs);
        font-family: var(--font-mono);
      }

      .nccl-viz__cell--data { background: var(--accent-blue); color: white; }
      .nccl-viz__cell--empty { background: var(--bg-tertiary); border: 1px dashed var(--border-subtle); }
      .nccl-viz__cell--partial { background: var(--accent-purple); color: white; }
      .nccl-viz__cell--sum { background: var(--accent-green); color: white; }

      .nccl-viz__arrow {
        margin: var(--space-sm) 0;
        text-align: center;
        color: var(--text-muted);
      }

      /* Code blocks */
      .code-block {
        background: var(--bg-tertiary);
        border-radius: var(--radius-md);
        padding: var(--space-md);
        overflow-x: auto;
        font-family: 'IBM Plex Mono', monospace;
        font-size: var(--text-sm);
        line-height: 1.6;
        margin: var(--space-md) 0;
      }

      .code-block__header {
        font-size: var(--text-xs);
        color: var(--text-muted);
        margin-bottom: var(--space-sm);
        font-weight: 600;
      }

      .code-block .comment { color: var(--text-muted); }
      .code-block .keyword { color: var(--accent-purple); }
      .code-block .function { color: var(--accent-blue); }
      .code-block .string { color: var(--accent-green); }
      .code-block .number { color: var(--accent-orange); }
      .code-block .decorator { color: var(--accent-cyan); }

      /* Bandwidth table */
      .bandwidth-table {
        width: 100%;
        border-collapse: collapse;
        margin: var(--space-lg) 0;
        font-size: var(--text-sm);
      }

      .bandwidth-table th,
      .bandwidth-table td {
        padding: var(--space-sm) var(--space-md);
        text-align: left;
        border-bottom: 1px solid var(--border-subtle);
      }

      .bandwidth-table th {
        background: var(--bg-secondary);
        font-weight: 600;
      }

      .bandwidth-table code {
        background: var(--bg-tertiary);
        padding: 2px 6px;
        border-radius: var(--radius-sm);
        font-family: var(--font-mono);
      }

      /* ZeRO visualization */
      .zero-viz {
        display: grid;
        grid-template-columns: repeat(4, 1fr);
        gap: var(--space-sm);
        margin: var(--space-lg) 0;
        padding: var(--space-md);
        background: var(--bg-secondary);
        border-radius: var(--radius-md);
      }

      .zero-viz__gpu {
        display: flex;
        flex-direction: column;
        gap: 4px;
      }

      .zero-viz__header {
        font-size: var(--text-xs);
        font-family: var(--font-mono);
        text-align: center;
        margin-bottom: var(--space-xs);
      }

      .zero-viz__block {
        height: 24px;
        border-radius: var(--radius-sm);
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: 9px;
        font-family: var(--font-mono);
      }

      .zero-viz__block--weights { background: var(--accent-blue); color: white; }
      .zero-viz__block--grads { background: var(--accent-orange); color: white; }
      .zero-viz__block--opt { background: var(--accent-purple); color: white; }
      .zero-viz__block--partial { background: var(--bg-tertiary); border: 1px dashed var(--border-subtle); color: var(--text-muted); }
      .zero-viz__block--empty { background: transparent; }

      /* Metric cards */
      .metric-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
        gap: var(--space-md);
        margin: var(--space-lg) 0;
      }

      .metric-card {
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        padding: var(--space-md);
        text-align: center;
      }

      .metric-card__label {
        font-size: var(--text-xs);
        color: var(--text-muted);
        margin-bottom: var(--space-xs);
      }

      .metric-card__value {
        font-family: var(--font-mono);
        font-size: var(--text-lg);
        font-weight: 600;
        color: var(--accent-blue);
      }

      .metric-card__unit {
        font-size: var(--text-sm);
        color: var(--text-secondary);
      }

      @media (max-width: 600px) {
        .zero-viz { grid-template-columns: repeat(2, 1fr); }
        .parallel-comparison { grid-template-columns: 1fr; }
      }
    </style>
  </head>
  <body>
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <main class="chapter-container" id="main-content">

      <header class="chapter-header">
        <div class="chapter-header__label"><a href="../index.html">Chapter 11</a></div>
        <h1 class="chapter-header__title">Multi-GPU Programming</h1>
        <p class="chapter-header__desc">
          Scaling training across multiple GPUs with data parallelism, tensor parallelism,
          and communication primitives. Understanding the topology and trade-offs that
          determine whether scaling actually speeds things up.
        </p>
      </header>

      <div class="learning-objectives">
        <div class="learning-objectives__title">What You'll Learn</div>
        <ol class="learning-objectives__list">
          <li class="learning-objectives__item">Explain data, tensor, and pipeline parallelism trade-offs</li>
          <li class="learning-objectives__item">Use NCCL primitives for GPU-to-GPU communication</li>
          <li class="learning-objectives__item">Understand NVLink/NVSwitch topology and bandwidth implications</li>
          <li class="learning-objectives__item">Implement basic distributed data parallel training</li>
          <li class="learning-objectives__item">Describe ZeRO optimization stages and their memory savings</li>
        </ol>
      </div>

      <!-- Section 1: Why Multi-GPU -->
      <section class="section" id="section-0">
        <div class="section__number">01 - WHY MULTI-GPU</div>
        <h2 class="section__title">Scaling Beyond One GPU</h2>

        <p>
          Large models don't fit on a single GPU. Even when they do, training is too slow.
          Multi-GPU training addresses both problems, but introduces communication overhead.
        </p>

        <h3>The Scale Problem</h3>

        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-card__label">LLaMA-70B Weights</div>
            <div class="metric-card__value">140</div>
            <div class="metric-card__unit">GB (FP16)</div>
          </div>
          <div class="metric-card">
            <div class="metric-card__label">H100 Memory</div>
            <div class="metric-card__value">80</div>
            <div class="metric-card__unit">GB HBM3</div>
          </div>
          <div class="metric-card">
            <div class="metric-card__label">Training Memory</div>
            <div class="metric-card__value">~4-8x</div>
            <div class="metric-card__unit">of weights</div>
          </div>
          <div class="metric-card">
            <div class="metric-card__label">GPUs Needed</div>
            <div class="metric-card__value">8+</div>
            <div class="metric-card__unit">for LLaMA-70B</div>
          </div>
        </div>

        <p>
          The <a href="https://arxiv.org/abs/2302.13971" target="_blank" rel="noopener">LLaMA paper</a>
          trained 65B parameters on 2048 A100 GPUs for 21 days. Scaling efficiently
          across thousands of GPUs is essential for frontier model training.
        </p>

        <h3>Scaling Efficiency</h3>

        <p>
          Perfect scaling means N GPUs give N× throughput. Reality is worse due to:
        </p>

        <ul>
          <li><strong>Communication overhead</strong> — time spent synchronizing gradients</li>
          <li><strong>Bubble time</strong> — GPUs waiting for data from other GPUs</li>
          <li><strong>Memory overhead</strong> — duplicate buffers, communication buffers</li>
          <li><strong>Load imbalance</strong> — some GPUs finish before others</li>
        </ul>

        <div class="callout info">
          <div class="callout-title">Scaling Efficiency Formula</div>
          <p class="mb-0">
            Efficiency = (N × single_GPU_throughput) / actual_throughput<br>
            Good systems achieve 90%+ efficiency at 100s of GPUs.
            <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">Megatron-LM</a>
            reports 52% efficiency at 3072 GPUs for GPT-3 scale models.
          </p>
        </div>

        <div class="quiz quiz--micro" id="quiz-scale">
          <div class="quiz-q">A 70B parameter model in FP16 needs ~140GB for weights alone. With Adam optimizer states (2x weights) and gradients, what's the minimum memory for training?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>280 GB (weights + gradients)</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>560 GB (weights + gradients + 2x optimizer states)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>140 GB (just weights)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 2: Parallelism Strategies -->
      <section class="section" id="section-1">
        <div class="section__number">02 - PARALLELISM TYPES</div>
        <h2 class="section__title">Data, Tensor, and Pipeline</h2>

        <p>
          There are three fundamental ways to distribute work across GPUs.
          Modern large-scale training combines all three in what's called
          <a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener">3D parallelism</a>.
        </p>

        <div class="parallel-comparison">
          <div class="parallel-card">
            <div class="parallel-card__header parallel-card__header--data">Data Parallelism</div>
            <div class="parallel-card__content">
              <p style="font-size: var(--text-sm); margin-bottom: var(--space-sm);">
                Same model on each GPU, different data batches.
                Gradients synchronized after backward.
              </p>
              <div class="parallel-card__diagram">
                <div class="parallel-card__block parallel-card__block--gpu0">Batch 0</div>
                <div class="parallel-card__block parallel-card__block--gpu1">Batch 1</div>
                <div class="parallel-card__block parallel-card__block--gpu2">Batch 2</div>
                <div class="parallel-card__block parallel-card__block--gpu3">Batch 3</div>
              </div>
              <p style="font-size: var(--text-xs); color: var(--text-muted);">
                <strong>Pro:</strong> Simple, scales batch size<br>
                <strong>Con:</strong> Model must fit on one GPU
              </p>
            </div>
          </div>

          <div class="parallel-card">
            <div class="parallel-card__header parallel-card__header--tensor">Tensor Parallelism</div>
            <div class="parallel-card__content">
              <p style="font-size: var(--text-sm); margin-bottom: var(--space-sm);">
                Split individual layers across GPUs.
                Each GPU computes part of each layer.
              </p>
              <div class="parallel-card__diagram">
                <div class="parallel-card__block parallel-card__block--gpu0">Col 0</div>
                <div class="parallel-card__block parallel-card__block--gpu1">Col 1</div>
                <div class="parallel-card__block parallel-card__block--gpu2">Col 2</div>
                <div class="parallel-card__block parallel-card__block--gpu3">Col 3</div>
              </div>
              <p style="font-size: var(--text-xs); color: var(--text-muted);">
                <strong>Pro:</strong> Splits large layers<br>
                <strong>Con:</strong> High communication every layer
              </p>
            </div>
          </div>

          <div class="parallel-card">
            <div class="parallel-card__header parallel-card__header--pipeline">Pipeline Parallelism</div>
            <div class="parallel-card__content">
              <p style="font-size: var(--text-sm); margin-bottom: var(--space-sm);">
                Different layers on different GPUs.
                Data flows through GPUs sequentially.
              </p>
              <div class="parallel-card__diagram">
                <div class="parallel-card__block parallel-card__block--gpu0">L0-L7</div>
                <div class="parallel-card__block parallel-card__block--gpu1">L8-L15</div>
                <div class="parallel-card__block parallel-card__block--gpu2">L16-L23</div>
                <div class="parallel-card__block parallel-card__block--gpu3">L24-L31</div>
              </div>
              <p style="font-size: var(--text-xs); color: var(--text-muted);">
                <strong>Pro:</strong> Low communication<br>
                <strong>Con:</strong> Pipeline bubbles, memory imbalance
              </p>
            </div>
          </div>
        </div>

        <h3>When to Use Each</h3>

        <table class="bandwidth-table">
          <thead>
            <tr>
              <th>Strategy</th>
              <th>Best For</th>
              <th>Communication Pattern</th>
              <th>Typical Scale</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Data Parallel</td>
              <td>Model fits on 1 GPU</td>
              <td>AllReduce after each step</td>
              <td>Any number of GPUs</td>
            </tr>
            <tr>
              <td>Tensor Parallel</td>
              <td>Large layers (big hidden dim)</td>
              <td>AllReduce after each layer</td>
              <td>2-8 GPUs (NVLink required)</td>
            </tr>
            <tr>
              <td>Pipeline Parallel</td>
              <td>Many layers</td>
              <td>P2P between stages</td>
              <td>4-64 GPUs per pipeline</td>
            </tr>
          </tbody>
        </table>

        <div class="callout info">
          <div class="callout-title">3D Parallelism</div>
          <p class="mb-0">
            Large-scale training uses all three: tensor parallelism within a node (NVLink),
            pipeline parallelism across nodes, and data parallelism across replicas.
            <a href="https://arxiv.org/abs/2104.04473" target="_blank" rel="noopener">Megatron-Turing NLG</a>
            uses (TP=8) × (PP=8) × (DP=48) = 3072 GPUs.
          </p>
        </div>

        <div class="quiz quiz--micro" id="quiz-parallel">
          <div class="quiz-q">Tensor parallelism requires AllReduce after every layer. Why is NVLink important for tensor parallelism?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>NVLink provides ~10x more bandwidth than PCIe for frequent communication</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>NVLink is required for multi-GPU—PCIe doesn't support it</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Tensor parallelism only works with NVLink hardware</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 3: NCCL Primitives -->
      <section class="section" id="section-2">
        <div class="section__number">03 - NCCL PRIMITIVES</div>
        <h2 class="section__title">Collective Communication Operations</h2>

        <p>
          <a href="https://developer.nvidia.com/nccl" target="_blank" rel="noopener">NCCL</a>
          (NVIDIA Collective Communications Library) provides optimized primitives for
          GPU-to-GPU communication. Understanding these operations is essential for
          distributed training.
        </p>

        <h3>AllReduce</h3>

        <p>
          The most common operation: sum gradients across all GPUs, result available on all GPUs.
        </p>

        <div class="nccl-viz">
          <div class="nccl-viz__title">AllReduce (Sum)</div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label">Before</div>
            <div class="nccl-viz__gpus">
              <div class="nccl-viz__cell nccl-viz__cell--data">g0</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">g1</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">g2</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">g3</div>
            </div>
          </div>
          <div class="nccl-viz__arrow">↓ AllReduce ↓</div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label">After</div>
            <div class="nccl-viz__gpus">
              <div class="nccl-viz__cell nccl-viz__cell--sum">Σg</div>
              <div class="nccl-viz__cell nccl-viz__cell--sum">Σg</div>
              <div class="nccl-viz__cell nccl-viz__cell--sum">Σg</div>
              <div class="nccl-viz__cell nccl-viz__cell--sum">Σg</div>
            </div>
          </div>
        </div>

        <h3>AllGather</h3>

        <p>
          Concatenate data from all GPUs onto all GPUs. Used in tensor parallelism to reconstruct tensors.
        </p>

        <div class="nccl-viz">
          <div class="nccl-viz__title">AllGather</div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label">Before</div>
            <div class="nccl-viz__gpus">
              <div class="nccl-viz__cell nccl-viz__cell--data">A</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">B</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">C</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">D</div>
            </div>
          </div>
          <div class="nccl-viz__arrow">↓ AllGather ↓</div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label">After</div>
            <div class="nccl-viz__gpus">
              <div class="nccl-viz__cell nccl-viz__cell--sum" style="width: 100%;">ABCD</div>
            </div>
          </div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label"></div>
            <div style="font-size: var(--text-xs); color: var(--text-muted);">Each GPU has the complete concatenated result</div>
          </div>
        </div>

        <h3>ReduceScatter</h3>

        <p>
          Reduce then scatter: each GPU gets a different portion of the reduced result.
          This is the key operation in ZeRO Stage 2.
        </p>

        <div class="nccl-viz">
          <div class="nccl-viz__title">ReduceScatter</div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label">Before</div>
            <div class="nccl-viz__gpus">
              <div class="nccl-viz__cell nccl-viz__cell--data">g0</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">g1</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">g2</div>
              <div class="nccl-viz__cell nccl-viz__cell--data">g3</div>
            </div>
          </div>
          <div class="nccl-viz__arrow">↓ ReduceScatter ↓</div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label">After</div>
            <div class="nccl-viz__gpus">
              <div class="nccl-viz__cell nccl-viz__cell--partial">Σ[0]</div>
              <div class="nccl-viz__cell nccl-viz__cell--partial">Σ[1]</div>
              <div class="nccl-viz__cell nccl-viz__cell--partial">Σ[2]</div>
              <div class="nccl-viz__cell nccl-viz__cell--partial">Σ[3]</div>
            </div>
          </div>
          <div class="nccl-viz__row">
            <div class="nccl-viz__label"></div>
            <div style="font-size: var(--text-xs); color: var(--text-muted);">Each GPU has 1/N of the summed result</div>
          </div>
        </div>

        <h3>Communication Cost</h3>

        <table class="bandwidth-table">
          <thead>
            <tr>
              <th>Operation</th>
              <th>Data Transferred per GPU</th>
              <th>Use Case</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>AllReduce</code></td>
              <td>2 × data_size × (N-1)/N</td>
              <td>Gradient sync (DDP)</td>
            </tr>
            <tr>
              <td><code>AllGather</code></td>
              <td>data_size × (N-1)/N</td>
              <td>Tensor parallel gather</td>
            </tr>
            <tr>
              <td><code>ReduceScatter</code></td>
              <td>data_size × (N-1)/N</td>
              <td>ZeRO gradient reduce</td>
            </tr>
            <tr>
              <td><code>Broadcast</code></td>
              <td>data_size</td>
              <td>Send from one to all</td>
            </tr>
          </tbody>
        </table>

        <div class="callout info">
          <div class="callout-title">Ring AllReduce</div>
          <p class="mb-0">
            NCCL implements AllReduce using a <a href="https://tech.preferred.jp/en/blog/technologies-behind-distributed-deep-learning-allreduce/" target="_blank" rel="noopener">ring algorithm</a>
            that achieves optimal bandwidth utilization. Each GPU sends/receives 2 × data_size total,
            regardless of the number of GPUs—making it highly scalable.
          </p>
        </div>

        <div class="quiz quiz--micro" id="quiz-nccl">
          <div class="quiz-q">AllReduce transfers 2 × data_size × (N-1)/N per GPU. For large N, what does this approach?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>2 × data_size (constant, independent of N)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>N × data_size (linear in N)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>data_size (just the gradient size)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 4: Hardware Topology -->
      <section class="section" id="section-3">
        <div class="section__number">04 - HARDWARE TOPOLOGY</div>
        <h2 class="section__title">NVLink, NVSwitch, and Interconnects</h2>

        <p>
          Communication bandwidth determines which parallelism strategies are practical.
          GPU interconnect technology varies dramatically in bandwidth.
        </p>

        <h3>Interconnect Bandwidth</h3>

        <table class="bandwidth-table">
          <thead>
            <tr>
              <th>Interconnect</th>
              <th>Bandwidth (bidirectional)</th>
              <th>Typical Use</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><a href="https://www.nvidia.com/en-us/data-center/nvlink/" target="_blank" rel="noopener">NVLink 4.0</a> (H100)</td>
              <td><strong>900 GB/s</strong></td>
              <td>Intra-node GPU-to-GPU</td>
            </tr>
            <tr>
              <td><a href="https://www.nvidia.com/en-us/data-center/nvlink/" target="_blank" rel="noopener">NVLink 3.0</a> (A100)</td>
              <td><strong>600 GB/s</strong></td>
              <td>Intra-node GPU-to-GPU</td>
            </tr>
            <tr>
              <td>PCIe 5.0 x16</td>
              <td>~64 GB/s</td>
              <td>GPU to CPU, older systems</td>
            </tr>
            <tr>
              <td><a href="https://www.nvidia.com/en-us/networking/products/infiniband/" target="_blank" rel="noopener">InfiniBand HDR</a></td>
              <td>~50 GB/s</td>
              <td>Inter-node</td>
            </tr>
            <tr>
              <td>100 GbE</td>
              <td>~12.5 GB/s</td>
              <td>Ethernet clusters</td>
            </tr>
          </tbody>
        </table>

        <h3>DGX H100 Topology</h3>

        <p>
          The <a href="https://resources.nvidia.com/en-us-dgx-systems/ai-enterprise-dgx" target="_blank" rel="noopener">DGX H100</a>
          uses NVSwitch to provide full-bisection bandwidth between all 8 GPUs:
        </p>

        <div class="topology-diagram">
          <div class="topology-diagram__gpus">
            <div class="topology-diagram__gpu">GPU 0</div>
            <div class="topology-diagram__gpu">GPU 1</div>
            <div class="topology-diagram__gpu">GPU 2</div>
            <div class="topology-diagram__gpu">GPU 3</div>
          </div>
          <div class="topology-diagram__switch">NVSwitch</div>
          <div class="topology-diagram__gpus">
            <div class="topology-diagram__gpu">GPU 4</div>
            <div class="topology-diagram__gpu">GPU 5</div>
            <div class="topology-diagram__gpu">GPU 6</div>
            <div class="topology-diagram__gpu">GPU 7</div>
          </div>
          <div class="topology-diagram__label">
            Any-to-any: <span class="topology-diagram__bandwidth">900 GB/s per GPU</span>
          </div>
        </div>

        <div class="callout warning">
          <div class="callout-title">Topology Matters</div>
          <p class="mb-0">
            Without NVSwitch, GPU pairs connect via NVLink but some pairs must route through intermediate GPUs.
            This creates <strong>topology-dependent performance</strong>—tensor parallel degree should match
            the number of GPUs with direct NVLink connections.
          </p>
        </div>

        <h3>Multi-Node Considerations</h3>

        <p>
          Inter-node bandwidth (InfiniBand/Ethernet) is 10-50× slower than NVLink.
          This constrains which parallelism works across nodes:
        </p>

        <ul>
          <li><strong>Data parallelism</strong> — works well (gradient sync once per step)</li>
          <li><strong>Pipeline parallelism</strong> — works well (point-to-point only)</li>
          <li><strong>Tensor parallelism</strong> — avoid across nodes (communication every layer)</li>
        </ul>

        <div class="quiz quiz--micro" id="quiz-topology">
          <div class="quiz-q">NVLink provides 900 GB/s, InfiniBand provides ~50 GB/s. Why avoid tensor parallelism across nodes?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Tensor parallelism doesn't work over network—only NVLink</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>~18x lower bandwidth makes per-layer AllReduce too slow</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>InfiniBand doesn't support collective operations</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 5: Distributed Data Parallel -->
      <section class="section" id="section-4">
        <div class="section__number">05 - DATA PARALLEL</div>
        <h2 class="section__title">Implementing DDP</h2>

        <p>
          <a href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html" target="_blank" rel="noopener">DistributedDataParallel</a>
          (DDP) is the workhorse of multi-GPU training. Each GPU has a full model copy,
          processes different data, and synchronizes gradients.
        </p>

        <h3>Basic DDP Setup</h3>

        <div class="code-block">
          <div class="code-block__header">PyTorch DDP</div>
<pre><span class="keyword">import</span> torch.distributed <span class="keyword">as</span> dist
<span class="keyword">from</span> torch.nn.parallel <span class="keyword">import</span> DistributedDataParallel <span class="keyword">as</span> DDP

<span class="comment"># Initialize process group (called once per process)</span>
dist.init_process_group(backend=<span class="string">"nccl"</span>)

<span class="comment"># Get local rank (which GPU this process uses)</span>
local_rank = int(os.environ[<span class="string">"LOCAL_RANK"</span>])
torch.cuda.set_device(local_rank)

<span class="comment"># Create model and wrap with DDP</span>
model = MyModel().cuda()
model = DDP(model, device_ids=[local_rank])

<span class="comment"># Create distributed sampler for data</span>
train_sampler = DistributedSampler(train_dataset)
train_loader = DataLoader(
    train_dataset,
    sampler=train_sampler,
    batch_size=per_gpu_batch_size
)

<span class="comment"># Training loop (gradients auto-synced by DDP)</span>
<span class="keyword">for</span> epoch <span class="keyword">in</span> range(epochs):
    train_sampler.set_epoch(epoch)  <span class="comment"># Shuffle per epoch</span>
    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:
        loss = model(batch)
        loss.backward()  <span class="comment"># DDP syncs gradients here</span>
        optimizer.step()
        optimizer.zero_grad()</pre>
        </div>

        <h3>How DDP Works</h3>

        <ol>
          <li><strong>Bucket gradients</strong> — group gradients into ~25MB buckets for efficient AllReduce</li>
          <li><strong>Overlap compute and communication</strong> — AllReduce starts as soon as bucket is ready</li>
          <li><strong>Broadcast initial weights</strong> — rank 0 broadcasts weights to all ranks at start</li>
        </ol>

        <div class="code-block">
          <div class="code-block__header">Launch with torchrun</div>
<pre><span class="comment"># Single node, 8 GPUs</span>
torchrun --nproc_per_node=8 train.py

<span class="comment"># Multi-node (run on each node)</span>
torchrun --nnodes=2 --nproc_per_node=8 \
    --rdzv_id=job1 --rdzv_backend=c10d \
    --rdzv_endpoint=master_ip:29500 train.py</pre>
        </div>

        <div class="callout info">
          <div class="callout-title">Gradient Bucketing</div>
          <p class="mb-0">
            DDP groups gradients into <a href="https://pytorch.org/docs/stable/notes/ddp.html#internal-design" target="_blank" rel="noopener">buckets</a>
            for efficient AllReduce. Small AllReduces have high overhead—bucketing amortizes this.
            Default bucket size is 25MB, tunable via <code>bucket_cap_mb</code>.
          </p>
        </div>

        <h3>Scaling Efficiency</h3>

        <p>
          DDP efficiency depends on the ratio of compute to communication:
        </p>

        <table class="bandwidth-table">
          <thead>
            <tr>
              <th>Model Size</th>
              <th>Gradient Size (FP16)</th>
              <th>AllReduce Time (8×H100)</th>
              <th>Typical Efficiency</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1B params</td>
              <td>2 GB</td>
              <td>~4 ms</td>
              <td>&gt;95%</td>
            </tr>
            <tr>
              <td>7B params</td>
              <td>14 GB</td>
              <td>~31 ms</td>
              <td>~90%</td>
            </tr>
            <tr>
              <td>70B params</td>
              <td>140 GB</td>
              <td>~311 ms</td>
              <td>Needs ZeRO</td>
            </tr>
          </tbody>
        </table>

        <div class="quiz quiz--micro" id="quiz-ddp">
          <div class="quiz-q">DDP overlaps gradient AllReduce with backward computation. What enables this overlap?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>GPU has separate compute and memory units</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Gradient bucketing—AllReduce starts on filled buckets while backward continues</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>NCCL uses a separate CPU thread for communication</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 6: ZeRO -->
      <section class="section" id="section-5">
        <div class="section__number">06 - ZERO OPTIMIZER</div>
        <h2 class="section__title">Memory-Efficient Data Parallelism</h2>

        <p>
          <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">ZeRO</a>
          (Zero Redundancy Optimizer) eliminates memory redundancy in data parallelism.
          Instead of each GPU storing full optimizer states, gradients, and parameters—they're partitioned.
        </p>

        <h3>ZeRO Stages</h3>

        <p><strong>Baseline DDP</strong> — each GPU stores everything:</p>

        <div class="zero-viz">
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 0</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Optimizer</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 1</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Optimizer</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 2</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Optimizer</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 3</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Optimizer</div>
          </div>
        </div>

        <p><strong>ZeRO Stage 1</strong> — partition optimizer states:</p>

        <div class="zero-viz">
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 0</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 0</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 1</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 1</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 2</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 2</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 3</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Gradients</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 3</div>
          </div>
        </div>

        <p><strong>ZeRO Stage 2</strong> — partition gradients (ReduceScatter instead of AllReduce):</p>

        <div class="zero-viz">
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 0</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Grad 0</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 0</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 1</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Grad 1</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 1</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 2</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Grad 2</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 2</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 3</div>
            <div class="zero-viz__block zero-viz__block--weights">Weights</div>
            <div class="zero-viz__block zero-viz__block--grads">Grad 3</div>
            <div class="zero-viz__block zero-viz__block--opt">Opt 3</div>
          </div>
        </div>

        <p><strong>ZeRO Stage 3</strong> — partition parameters (AllGather before each forward/backward):</p>

        <div class="zero-viz">
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 0</div>
            <div class="zero-viz__block zero-viz__block--partial">W 0</div>
            <div class="zero-viz__block zero-viz__block--partial">Grad 0</div>
            <div class="zero-viz__block zero-viz__block--partial">Opt 0</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 1</div>
            <div class="zero-viz__block zero-viz__block--partial">W 1</div>
            <div class="zero-viz__block zero-viz__block--partial">Grad 1</div>
            <div class="zero-viz__block zero-viz__block--partial">Opt 1</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 2</div>
            <div class="zero-viz__block zero-viz__block--partial">W 2</div>
            <div class="zero-viz__block zero-viz__block--partial">Grad 2</div>
            <div class="zero-viz__block zero-viz__block--partial">Opt 2</div>
          </div>
          <div class="zero-viz__gpu">
            <div class="zero-viz__header">GPU 3</div>
            <div class="zero-viz__block zero-viz__block--partial">W 3</div>
            <div class="zero-viz__block zero-viz__block--partial">Grad 3</div>
            <div class="zero-viz__block zero-viz__block--partial">Opt 3</div>
          </div>
        </div>

        <h3>Memory Savings</h3>

        <table class="bandwidth-table">
          <thead>
            <tr>
              <th>Stage</th>
              <th>Memory per GPU (N GPUs, FP16+FP32 opt)</th>
              <th>Communication</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Baseline DDP</td>
              <td>2Ψ + 2Ψ + 12Ψ = 16Ψ</td>
              <td>AllReduce</td>
            </tr>
            <tr>
              <td>ZeRO-1</td>
              <td>2Ψ + 2Ψ + 12Ψ/N = 4Ψ + 12Ψ/N</td>
              <td>AllReduce</td>
            </tr>
            <tr>
              <td>ZeRO-2</td>
              <td>2Ψ + 2Ψ/N + 12Ψ/N = 2Ψ + 14Ψ/N</td>
              <td>ReduceScatter + AllGather</td>
            </tr>
            <tr>
              <td>ZeRO-3</td>
              <td>16Ψ/N</td>
              <td>AllGather (2x per layer)</td>
            </tr>
          </tbody>
        </table>

        <p style="font-size: var(--text-sm); color: var(--text-muted);">
          Ψ = model parameters. Adam FP32 optimizer states = 12 bytes/param (momentum + variance + master weight).
        </p>

        <div class="code-block">
          <div class="code-block__header">DeepSpeed ZeRO Stage 2</div>
<pre><span class="keyword">import</span> deepspeed

<span class="comment"># Config enables ZeRO Stage 2</span>
ds_config = {
    <span class="string">"zero_optimization"</span>: {
        <span class="string">"stage"</span>: <span class="number">2</span>,
        <span class="string">"offload_optimizer"</span>: {<span class="string">"device"</span>: <span class="string">"cpu"</span>},  <span class="comment"># Optional CPU offload</span>
        <span class="string">"contiguous_gradients"</span>: <span class="keyword">True</span>,
        <span class="string">"overlap_comm"</span>: <span class="keyword">True</span>
    },
    <span class="string">"fp16"</span>: {<span class="string">"enabled"</span>: <span class="keyword">True</span>},
    <span class="string">"train_batch_size"</span>: <span class="number">32</span>
}

model, optimizer, _, _ = deepspeed.initialize(
    model=model,
    config=ds_config
)</pre>
        </div>

        <div class="callout info">
          <div class="callout-title">ZeRO-Offload</div>
          <p class="mb-0">
            <a href="https://arxiv.org/abs/2101.06840" target="_blank" rel="noopener">ZeRO-Offload</a>
            extends ZeRO by offloading optimizer states and computation to CPU memory.
            This enables training 10B+ models on a single GPU, at the cost of CPU-GPU transfer overhead.
          </p>
        </div>

        <div class="quiz quiz--micro" id="quiz-zero">
          <div class="quiz-q">ZeRO Stage 3 reduces memory to 16Ψ/N per GPU. What's the trade-off?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Lower throughput due to CPU offload</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>AllGather needed before each layer's forward and backward</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Only works with specific model architectures</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 7: Tensor Parallelism -->
      <section class="section" id="section-6">
        <div class="section__number">07 - TENSOR PARALLELISM</div>
        <h2 class="section__title">Splitting Layers Across GPUs</h2>

        <p>
          <a href="https://arxiv.org/abs/1909.08053" target="_blank" rel="noopener">Megatron-LM style tensor parallelism</a>
          splits individual linear layers across GPUs. This is essential for models where
          a single layer doesn't fit in GPU memory.
        </p>

        <h3>Column-Parallel Linear</h3>

        <p>
          Split the weight matrix along the output dimension. Each GPU computes part of the output.
        </p>

        <div class="code-block">
          <div class="code-block__header">Column Parallel (A = XW, W split by columns)</div>
<pre><span class="comment"># Weight W shape: (in_features, out_features)</span>
<span class="comment"># Split into W = [W_0, W_1] along columns</span>

<span class="comment"># GPU 0: A_0 = X @ W_0   (all of X, part of W)</span>
<span class="comment"># GPU 1: A_1 = X @ W_1   (all of X, part of W)</span>

<span class="comment"># Result: A = [A_0, A_1] (concatenated)</span>
<span class="comment"># Requires AllGather to get full A on all GPUs (if needed)</span></pre>
        </div>

        <h3>Row-Parallel Linear</h3>

        <p>
          Split the weight matrix along the input dimension. Requires input to be partitioned.
        </p>

        <div class="code-block">
          <div class="code-block__header">Row Parallel (B = AW, W split by rows, A partitioned)</div>
<pre><span class="comment"># Weight W shape: (in_features, out_features)</span>
<span class="comment"># Split into W = [W_0; W_1] along rows</span>
<span class="comment"># Input A = [A_0, A_1] partitioned across GPUs</span>

<span class="comment"># GPU 0: B_0 = A_0 @ W_0  (partial product)</span>
<span class="comment"># GPU 1: B_1 = A_1 @ W_1  (partial product)</span>

<span class="comment"># Result: B = B_0 + B_1 (requires AllReduce)</span></pre>
        </div>

        <h3>Transformer Tensor Parallelism</h3>

        <p>
          In transformers, column-parallel and row-parallel layers are combined to minimize
          communication:
        </p>

        <table class="bandwidth-table">
          <thead>
            <tr>
              <th>Layer</th>
              <th>Parallelism Type</th>
              <th>Communication</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>QKV projection</td>
              <td>Column parallel</td>
              <td>None (input broadcast)</td>
            </tr>
            <tr>
              <td>Attention output</td>
              <td>Row parallel</td>
              <td>AllReduce after</td>
            </tr>
            <tr>
              <td>FFN first linear</td>
              <td>Column parallel</td>
              <td>None</td>
            </tr>
            <tr>
              <td>FFN second linear</td>
              <td>Row parallel</td>
              <td>AllReduce after</td>
            </tr>
          </tbody>
        </table>

        <p>
          This pattern requires only <strong>2 AllReduces per transformer layer</strong>
          (one after attention, one after FFN).
        </p>

        <div class="callout warning">
          <div class="callout-title">Tensor Parallelism Degree</div>
          <p class="mb-0">
            Tensor parallel degree is typically 2, 4, or 8—matching GPUs connected by NVLink.
            Higher degrees have diminishing returns: more communication, smaller per-GPU matrices
            (worse GPU utilization).
          </p>
        </div>

        <div class="quiz quiz--micro" id="quiz-tensor">
          <div class="quiz-q">Megatron-style tensor parallelism requires 2 AllReduces per transformer layer. Where do they occur?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>After attention output projection and after FFN</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Before and after each linear layer</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Only during backward pass for gradient sync</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Summary -->
      <section class="section" id="section-7">
        <div class="section__number">08 - SUMMARY</div>
        <h2 class="section__title">Key Takeaways</h2>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--blue">Parallelism Strategies</div>
          </div>
          <ul>
            <li><strong>Data parallel</strong> — simplest, scales batch size, model must fit on 1 GPU</li>
            <li><strong>Tensor parallel</strong> — splits layers, requires NVLink, typically 2-8 GPUs</li>
            <li><strong>Pipeline parallel</strong> — splits by layers, works across nodes, has bubble overhead</li>
            <li><strong>3D parallelism</strong> — combine all three for maximum scale</li>
          </ul>
        </div>

        <div class="card" style="margin-top: var(--space-md);">
          <div class="card__header">
            <div class="card__icon card__icon--green">NCCL Operations</div>
          </div>
          <ul>
            <li><strong>AllReduce</strong> — sum to all, used in DDP gradient sync</li>
            <li><strong>AllGather</strong> — concat to all, used in tensor parallel and ZeRO-3</li>
            <li><strong>ReduceScatter</strong> — sum and partition, used in ZeRO-2</li>
          </ul>
        </div>

        <div class="card" style="margin-top: var(--space-md);">
          <div class="card__header">
            <div class="card__icon card__icon--purple">ZeRO Memory Efficiency</div>
          </div>
          <ul>
            <li><strong>ZeRO-1</strong> — partition optimizer states (~4x memory savings)</li>
            <li><strong>ZeRO-2</strong> — + partition gradients (~8x savings)</li>
            <li><strong>ZeRO-3</strong> — + partition parameters (~N× savings, more communication)</li>
          </ul>
        </div>
      </section>

      <!-- Chapter navigation -->
      <nav class="chapter-nav">
        <a href="10-training.html" class="chapter-nav__link">
          <span class="chapter-nav__arrow">&lt;</span>
          <span class="chapter-nav__title">Training Kernels</span>
        </a>
        <a href="13-production.html" class="chapter-nav__link">
          <span class="chapter-nav__title">Production Systems</span>
          <span class="chapter-nav__arrow">&gt;</span>
        </a>
      </nav>
    </main>

    <script>
      // Quiz functionality
      document.querySelectorAll('.quiz-opt').forEach(opt => {
        opt.addEventListener('click', function() {
          const quiz = this.closest('.quiz');
          if (quiz.classList.contains('answered')) return;
          
          quiz.classList.add('answered');
          const isCorrect = this.dataset.correct === 'true';
          this.classList.add(isCorrect ? 'correct' : 'incorrect');
          
          if (!isCorrect) {
            quiz.querySelector('[data-correct="true"]').classList.add('correct');
          }
          
          const fb = quiz.querySelector('.quiz-fb');
          fb.textContent = isCorrect 
            ? 'Correct!' 
            : 'Not quite. The correct answer is highlighted.';
          fb.className = 'quiz-fb ' + (isCorrect ? 'quiz-fb--correct' : 'quiz-fb--incorrect');
        });
        
        opt.addEventListener('keydown', function(e) {
          if (e.key === 'Enter' || e.key === ' ') {
            e.preventDefault();
            this.click();
          }
        });
      });
    </script>
  </body>
</html>
