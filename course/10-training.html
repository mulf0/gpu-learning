<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Chapter 10: Training Kernels - GPU Learning</title>
    <link rel="icon" type="image/x-icon" href="../favicon.ico" />
    <link rel="icon" type="image/svg+xml" href="../favicon.svg" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono:wght@400;500;600&family=Source+Serif+4:ital,opsz,wght@0,8..60,300;0,8..60,400;0,8..60,600;1,8..60,400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="../base.css" />
    <style>
      body { display: block; }

      /* Memory diagram */
      .memory-diagram {
        display: flex;
        gap: var(--space-md);
        margin: var(--space-lg) 0;
        flex-wrap: wrap;
        justify-content: center;
      }

      .memory-diagram__block {
        padding: var(--space-md) var(--space-lg);
        border-radius: var(--radius-md);
        text-align: center;
        min-width: 120px;
      }

      .memory-diagram__block--weights {
        background: var(--glow-blue);
        border: 1px solid var(--accent-blue);
      }

      .memory-diagram__block--gradients {
        background: var(--glow-orange);
        border: 1px solid var(--accent-orange);
      }

      .memory-diagram__block--optimizer {
        background: var(--glow-purple);
        border: 1px solid var(--accent-purple);
      }

      .memory-diagram__block--activations {
        background: var(--glow-green);
        border: 1px solid var(--accent-green);
      }

      .memory-diagram__label {
        font-weight: 600;
        font-size: var(--text-sm);
        margin-bottom: var(--space-xs);
      }

      .memory-diagram__size {
        font-family: var(--font-mono);
        font-size: var(--text-xs);
        color: var(--text-muted);
      }

      /* Code blocks */
      .code-block {
        background: var(--bg-tertiary);
        border-radius: var(--radius-md);
        padding: var(--space-md);
        overflow-x: auto;
        font-family: 'IBM Plex Mono', monospace;
        font-size: var(--text-sm);
        line-height: 1.6;
        margin: var(--space-md) 0;
      }

      .code-block__header {
        font-size: var(--text-xs);
        color: var(--text-muted);
        margin-bottom: var(--space-sm);
        font-weight: 600;
      }

      .code-block .comment { color: var(--text-muted); }
      .code-block .keyword { color: var(--accent-purple); }
      .code-block .function { color: var(--accent-blue); }
      .code-block .string { color: var(--accent-green); }
      .code-block .number { color: var(--accent-orange); }
      .code-block .decorator { color: var(--accent-cyan); }

      /* Comparison panels */
      .compare-panels {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: var(--space-md);
        margin: var(--space-lg) 0;
      }

      @media (max-width: 700px) {
        .compare-panels { grid-template-columns: 1fr; }
      }

      .compare-panel {
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        overflow: hidden;
      }

      .compare-panel__header {
        padding: var(--space-sm) var(--space-md);
        font-weight: 600;
        font-size: var(--text-sm);
      }

      .compare-panel__header--baseline {
        background: var(--glow-orange);
        color: var(--accent-orange);
      }

      .compare-panel__header--optimized {
        background: var(--glow-green);
        color: var(--accent-green);
      }

      .compare-panel__content {
        padding: var(--space-md);
      }

      /* Precision table */
      .precision-table {
        width: 100%;
        border-collapse: collapse;
        margin: var(--space-lg) 0;
        font-size: var(--text-sm);
      }

      .precision-table th,
      .precision-table td {
        padding: var(--space-sm) var(--space-md);
        text-align: left;
        border-bottom: 1px solid var(--border-subtle);
      }

      .precision-table th {
        background: var(--bg-secondary);
        font-weight: 600;
      }

      .precision-table code {
        background: var(--bg-tertiary);
        padding: 2px 6px;
        border-radius: var(--radius-sm);
        font-family: var(--font-mono);
      }

      /* Checkpointing visualization */
      .checkpoint-viz {
        display: flex;
        flex-direction: column;
        gap: var(--space-sm);
        margin: var(--space-lg) 0;
        padding: var(--space-md);
        background: var(--bg-secondary);
        border-radius: var(--radius-md);
      }

      .checkpoint-viz__row {
        display: flex;
        align-items: center;
        gap: var(--space-xs);
      }

      .checkpoint-viz__label {
        width: 80px;
        font-size: var(--text-xs);
        color: var(--text-muted);
      }

      .checkpoint-viz__blocks {
        display: flex;
        gap: 2px;
        flex: 1;
      }

      .checkpoint-viz__block {
        height: 24px;
        flex: 1;
        border-radius: var(--radius-sm);
        display: flex;
        align-items: center;
        justify-content: center;
        font-size: var(--text-xs);
        font-family: var(--font-mono);
      }

      .checkpoint-viz__block--saved {
        background: var(--accent-blue);
        color: white;
      }

      .checkpoint-viz__block--recompute {
        background: var(--bg-tertiary);
        border: 1px dashed var(--border-subtle);
        color: var(--text-muted);
      }

      .checkpoint-viz__block--checkpoint {
        background: var(--accent-green);
        color: white;
      }

      /* Timeline for gradient accumulation */
      .timeline {
        margin: var(--space-lg) 0;
        padding: var(--space-md);
        background: var(--bg-secondary);
        border-radius: var(--radius-md);
      }

      .timeline__row {
        display: flex;
        align-items: center;
        margin-bottom: var(--space-sm);
      }

      .timeline__label {
        width: 100px;
        font-size: var(--text-xs);
        color: var(--text-muted);
      }

      .timeline__steps {
        display: flex;
        gap: 2px;
        flex: 1;
      }

      .timeline__step {
        padding: var(--space-xs) var(--space-sm);
        border-radius: var(--radius-sm);
        font-size: var(--text-xs);
        font-family: var(--font-mono);
      }

      .timeline__step--forward {
        background: var(--glow-blue);
        border: 1px solid var(--accent-blue);
      }

      .timeline__step--backward {
        background: var(--glow-orange);
        border: 1px solid var(--accent-orange);
      }

      .timeline__step--update {
        background: var(--glow-green);
        border: 1px solid var(--accent-green);
      }

      /* Metric cards */
      .metric-grid {
        display: grid;
        grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));
        gap: var(--space-md);
        margin: var(--space-lg) 0;
      }

      .metric-card {
        background: var(--bg-secondary);
        border: 1px solid var(--border-subtle);
        border-radius: var(--radius-md);
        padding: var(--space-md);
        text-align: center;
      }

      .metric-card__label {
        font-size: var(--text-xs);
        color: var(--text-muted);
        margin-bottom: var(--space-xs);
      }

      .metric-card__value {
        font-family: var(--font-mono);
        font-size: var(--text-lg);
        font-weight: 600;
        color: var(--accent-blue);
      }

      .metric-card__unit {
        font-size: var(--text-sm);
        color: var(--text-secondary);
      }
    </style>
  </head>
  <body>
    <a href="#main-content" class="skip-link">Skip to main content</a>

    <main class="chapter-container" id="main-content">
      <header class="chapter-header">
        <div class="chapter-header__label">Chapter 10</div>
        <h1 class="chapter-header__title">Training Kernels</h1>
        <p class="chapter-header__desc">
          Understanding backward passes, mixed-precision training, and memory optimization techniques
          that make training large models possible on limited GPU memory.
        </p>
      </header>

      <div class="learning-objectives">
        <div class="learning-objectives__title">What You'll Learn</div>
        <ol class="learning-objectives__list">
          <li class="learning-objectives__item">Implement backward passes for common operations</li>
          <li class="learning-objectives__item">Apply mixed-precision training with loss scaling</li>
          <li class="learning-objectives__item">Use activation checkpointing to trade compute for memory</li>
          <li class="learning-objectives__item">Implement gradient accumulation for larger effective batch sizes</li>
          <li class="learning-objectives__item">Debug training-specific numerical issues</li>
        </ol>
      </div>

      <!-- Section 1: Forward vs Backward -->
      <section class="section" id="section-0">
        <div class="section__number">01 - FORWARD VS BACKWARD</div>
        <h2 class="section__title">The Computational Graph</h2>

        <p>
          Training neural networks requires computing gradients via 
          <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank" rel="noopener">automatic differentiation</a>.
          For every forward operation, there's a corresponding backward operation that propagates gradients.
        </p>

        <h3>Memory Requirements</h3>

        <p>
          Training consumes far more memory than inference because you must store:
        </p>

        <div class="memory-diagram">
          <div class="memory-diagram__block memory-diagram__block--weights">
            <div class="memory-diagram__label">Weights</div>
            <div class="memory-diagram__size">Model params</div>
          </div>
          <div class="memory-diagram__block memory-diagram__block--gradients">
            <div class="memory-diagram__label">Gradients</div>
            <div class="memory-diagram__size">Same as weights</div>
          </div>
          <div class="memory-diagram__block memory-diagram__block--optimizer">
            <div class="memory-diagram__label">Optimizer States</div>
            <div class="memory-diagram__size">2x weights (Adam)</div>
          </div>
          <div class="memory-diagram__block memory-diagram__block--activations">
            <div class="memory-diagram__label">Activations</div>
            <div class="memory-diagram__size">Batch × Layers</div>
          </div>
        </div>

        <p>
          For a 7B parameter model in FP32, the <a href="https://arxiv.org/abs/1910.02054" target="_blank" rel="noopener">ZeRO paper</a> 
          breaks down memory consumption:
        </p>

        <div class="metric-grid">
          <div class="metric-card">
            <div class="metric-card__label">Weights</div>
            <div class="metric-card__value">28</div>
            <div class="metric-card__unit">GB</div>
          </div>
          <div class="metric-card">
            <div class="metric-card__label">Gradients</div>
            <div class="metric-card__value">28</div>
            <div class="metric-card__unit">GB</div>
          </div>
          <div class="metric-card">
            <div class="metric-card__label">Adam States</div>
            <div class="metric-card__value">56</div>
            <div class="metric-card__unit">GB</div>
          </div>
          <div class="metric-card">
            <div class="metric-card__label">Total (min)</div>
            <div class="metric-card__value">112</div>
            <div class="metric-card__unit">GB</div>
          </div>
        </div>

        <div class="callout info">
          <div class="callout-title">Why Activations Matter</div>
          <p class="mb-0">
            Activations scale with batch size and sequence length. For transformers, 
            attention activations grow as O(seq_len²) per layer unless using 
            <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention</a> 
            which doesn't materialize the full attention matrix.
          </p>
        </div>

        <h3>Forward vs Backward Compute</h3>

        <p>
          A common rule of thumb: <strong>backward pass costs ~2x the forward pass</strong>.
          This comes from computing both the gradient with respect to inputs and weights.
          For a linear layer Y = XW:
        </p>

        <div class="compare-panels">
          <div class="compare-panel">
            <div class="compare-panel__header compare-panel__header--baseline">Forward Pass</div>
            <div class="compare-panel__content">
              <div class="code-block">
<pre><span class="comment"># Forward: 1 matmul</span>
Y = X @ W  <span class="comment"># (B, in) @ (in, out) = (B, out)</span></pre>
              </div>
            </div>
          </div>
          <div class="compare-panel">
            <div class="compare-panel__header compare-panel__header--optimized">Backward Pass</div>
            <div class="compare-panel__content">
              <div class="code-block">
<pre><span class="comment"># Backward: 2 matmuls</span>
dX = dY @ W.T  <span class="comment"># gradient w.r.t. input</span>
dW = X.T @ dY  <span class="comment"># gradient w.r.t. weights</span></pre>
              </div>
            </div>
          </div>
        </div>

        <div class="quiz quiz--micro" id="quiz-memory">
          <div class="quiz-q">A 13B parameter model trained with Adam in FP32 needs at minimum how much memory for weights + optimizer states?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>52 GB (4 bytes × 13B)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>104 GB (weights + gradients)</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>208 GB (weights + gradients + 2x optimizer states)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 2: Backward Pass Kernels -->
      <section class="section" id="section-1">
        <div class="section__number">02 - BACKWARD KERNELS</div>
        <h2 class="section__title">Writing Backward Passes</h2>

        <p>
          Understanding backward kernels helps you write custom operations and debug gradient issues.
          Each operation must define how gradients flow backwards.
        </p>

        <h3>Linear Layer Backward</h3>

        <p>
          The <a href="https://pytorch.org/docs/stable/generated/torch.nn.Linear.html" target="_blank" rel="noopener">linear layer</a>
          backward pass requires saving the input tensor from the forward pass:
        </p>

        <div class="code-block">
          <div class="code-block__header">Triton Linear Backward</div>
<pre><span class="decorator">@triton.jit</span>
<span class="keyword">def</span> <span class="function">linear_backward_dx</span>(
    dY_ptr, W_ptr, dX_ptr,
    M, N, K,  <span class="comment"># M=batch, N=in_features, K=out_features</span>
    stride_dy_m, stride_dy_k,
    stride_w_n, stride_w_k,
    stride_dx_m, stride_dx_n,
    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr
):
    <span class="string">"""dX = dY @ W.T"""</span>
    pid_m = tl.program_id(<span class="number">0</span>)
    pid_n = tl.program_id(<span class="number">1</span>)
    
    <span class="comment"># Accumulate dX[m, n] = sum_k(dY[m, k] * W[n, k])</span>
    offs_m = pid_m * BLOCK_M + tl.arange(<span class="number">0</span>, BLOCK_M)
    offs_n = pid_n * BLOCK_N + tl.arange(<span class="number">0</span>, BLOCK_N)
    offs_k = tl.arange(<span class="number">0</span>, BLOCK_K)
    
    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)
    
    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">0</span>, K, BLOCK_K):
        dy = tl.load(dY_ptr + offs_m[:, <span class="keyword">None</span>] * stride_dy_m + 
                     (k + offs_k[<span class="keyword">None</span>, :]) * stride_dy_k)
        w = tl.load(W_ptr + offs_n[:, <span class="keyword">None</span>] * stride_w_n + 
                    (k + offs_k[<span class="keyword">None</span>, :]) * stride_w_k)
        acc += tl.dot(dy, tl.trans(w))
    
    tl.store(dX_ptr + offs_m[:, <span class="keyword">None</span>] * stride_dx_m + 
             offs_n[<span class="keyword">None</span>, :] * stride_dx_n, acc)</pre>
        </div>

        <h3>LayerNorm Backward</h3>

        <p>
          <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noopener">LayerNorm</a> 
          backward is more complex because normalization creates dependencies between elements.
          The gradients must account for the mean and variance computations:
        </p>

        <div class="code-block">
          <div class="code-block__header">LayerNorm Gradient Equations</div>
<pre><span class="comment"># Forward: y = (x - mean) / std * gamma + beta</span>
<span class="comment"># Where std = sqrt(var + eps)</span>

<span class="comment"># Backward requires 3 passes:</span>
<span class="comment"># 1. Compute gradient contributions</span>
dx_hat = dY * gamma                    <span class="comment"># (B, D)</span>
dvar = sum(dx_hat * (x - mean) * -0.5 * std^-3)
dmean = sum(dx_hat * -1/std) + dvar * sum(-2 * (x - mean)) / D

<span class="comment"># 2. Compute dx</span>
dx = dx_hat / std + dvar * 2 * (x - mean) / D + dmean / D

<span class="comment"># 3. Compute dgamma, dbeta</span>
dgamma = sum(dY * (x - mean) / std, dim=0)  <span class="comment"># reduce over batch</span>
dbeta = sum(dY, dim=0)</pre>
        </div>

        <div class="callout warning">
          <div class="callout-title">Numerical Stability in Backward</div>
          <p class="mb-0">
            Division by std can cause issues when variance is very small.
            Always use the same epsilon in backward as forward, and consider using 
            <a href="https://docs.nvidia.com/deeplearning/cudnn/developer-guide/index.html#batch-normalization" target="_blank" rel="noopener">numerically stable formulations</a>.
          </p>
        </div>

        <h3>Attention Backward Memory</h3>

        <p>
          Standard attention backward is memory-intensive because it requires the full attention matrix.
          For sequence length L and batch B with H heads:
        </p>

        <table class="precision-table">
          <thead>
            <tr>
              <th>What to Store</th>
              <th>Size</th>
              <th>L=2048, B=8, H=32</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Q, K, V tensors</td>
              <td>3 × B × H × L × d</td>
              <td>~3 GB</td>
            </tr>
            <tr>
              <td>Attention matrix</td>
              <td>B × H × L × L</td>
              <td>~8 GB</td>
            </tr>
            <tr>
              <td>Softmax output</td>
              <td>B × H × L × L</td>
              <td>~8 GB</td>
            </tr>
          </tbody>
        </table>

        <p>
          This is why <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener">FlashAttention</a>
          recomputes attention during backward instead of storing it—trading compute for memory.
        </p>

        <div class="quiz quiz--micro" id="quiz-backward">
          <div class="quiz-q">Why does FlashAttention recompute the attention matrix during backward pass?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Recomputation is faster than memory access</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Storing O(L²) attention matrix is prohibitively expensive for long sequences</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>The attention matrix changes between forward and backward</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 3: Mixed Precision Training -->
      <section class="section" id="section-2">
        <div class="section__number">03 - MIXED PRECISION</div>
        <h2 class="section__title">FP16/BF16 Training with Loss Scaling</h2>

        <p>
          <a href="https://arxiv.org/abs/1710.03740" target="_blank" rel="noopener">Mixed-precision training</a>
          uses lower precision (FP16 or BF16) for forward/backward passes while maintaining
          FP32 master weights. This reduces memory and leverages Tensor Cores for speed.
        </p>

        <h3>The Precision Hierarchy</h3>

        <table class="precision-table">
          <thead>
            <tr>
              <th>Format</th>
              <th>Bits</th>
              <th>Dynamic Range</th>
              <th>Use Case</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td><code>FP32</code></td>
              <td>32</td>
              <td>~10<sup>38</sup></td>
              <td>Master weights, optimizer states</td>
            </tr>
            <tr>
              <td><code>FP16</code></td>
              <td>16</td>
              <td>~65504</td>
              <td>Forward/backward (requires loss scaling)</td>
            </tr>
            <tr>
              <td><code>BF16</code></td>
              <td>16</td>
              <td>~10<sup>38</sup></td>
              <td>Forward/backward (no loss scaling needed)</td>
            </tr>
            <tr>
              <td><code>TF32</code></td>
              <td>19 (internal)</td>
              <td>~10<sup>38</sup></td>
              <td>Tensor Core matmuls on Ampere+</td>
            </tr>
          </tbody>
        </table>

        <h3>Loss Scaling</h3>

        <p>
          FP16 can't represent gradients smaller than ~6×10<sup>-8</sup>. Small gradients
          underflow to zero, causing training to stall. 
          <a href="https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html#lossscaling" target="_blank" rel="noopener">Loss scaling</a>
          multiplies the loss by a large factor, scaling up all gradients:
        </p>

        <div class="code-block">
          <div class="code-block__header">PyTorch AMP with Loss Scaling</div>
<pre><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler

scaler = GradScaler()

<span class="keyword">for</span> inputs, targets <span class="keyword">in</span> dataloader:
    optimizer.zero_grad()
    
    <span class="comment"># Forward in FP16</span>
    <span class="keyword">with</span> autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
    
    <span class="comment"># Backward with scaled loss</span>
    scaler.scale(loss).backward()  <span class="comment"># loss × scale_factor</span>
    
    <span class="comment"># Unscale gradients, check for inf/nan, step</span>
    scaler.step(optimizer)  <span class="comment"># only steps if grads valid</span>
    scaler.update()  <span class="comment"># adjust scale factor</span></pre>
        </div>

        <h3>Dynamic Loss Scaling</h3>

        <p>
          The scale factor is adjusted dynamically:
        </p>

        <ul>
          <li><strong>No overflow for N steps</strong> → increase scale (typically 2x)</li>
          <li><strong>Overflow detected</strong> → decrease scale (typically 0.5x), skip update</li>
        </ul>

        <div class="callout info">
          <div class="callout-title">BF16 Advantage</div>
          <p class="mb-0">
            <a href="https://cloud.google.com/tpu/docs/bfloat16" target="_blank" rel="noopener">BF16</a> 
            has the same exponent range as FP32, so gradients rarely underflow.
            Most modern training uses BF16 without loss scaling when hardware supports it
            (Ampere+ GPUs, TPUs).
          </p>
        </div>

        <h3>What Stays in FP32</h3>

        <p>
          Even in mixed-precision training, some operations must remain in FP32:
        </p>

        <ul>
          <li><strong>Softmax</strong> — accumulation over many elements</li>
          <li><strong>LayerNorm variance</strong> — requires high precision for stability</li>
          <li><strong>Loss computation</strong> — small loss values can underflow</li>
          <li><strong>Optimizer states</strong> — momentum/variance accumulate over many steps</li>
        </ul>

        <div class="quiz quiz--micro" id="quiz-precision">
          <div class="quiz-q">Why does BF16 training usually not require loss scaling?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>BF16 has more mantissa bits than FP16</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>BF16 has the same exponent range as FP32, so gradients don't underflow</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>BF16 uses stochastic rounding</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 4: Activation Checkpointing -->
      <section class="section" id="section-3">
        <div class="section__number">04 - CHECKPOINTING</div>
        <h2 class="section__title">Trading Compute for Memory</h2>

        <p>
          <a href="https://arxiv.org/abs/1604.06174" target="_blank" rel="noopener">Activation checkpointing</a>
          (gradient checkpointing) saves memory by not storing all intermediate activations.
          Instead, it recomputes them during the backward pass.
        </p>

        <h3>How It Works</h3>

        <p>
          Without checkpointing, all activations are saved for backward:
        </p>

        <div class="checkpoint-viz">
          <div class="checkpoint-viz__row">
            <div class="checkpoint-viz__label">No checkpoint</div>
            <div class="checkpoint-viz__blocks">
              <div class="checkpoint-viz__block checkpoint-viz__block--saved">L1</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--saved">L2</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--saved">L3</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--saved">L4</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--saved">L5</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--saved">L6</div>
            </div>
          </div>
          <div class="checkpoint-viz__row">
            <div class="checkpoint-viz__label">Memory</div>
            <div class="checkpoint-viz__blocks">
              <div style="font-size: var(--text-xs); color: var(--accent-orange);">O(n) activations stored</div>
            </div>
          </div>
        </div>

        <p>
          With checkpointing every 2 layers:
        </p>

        <div class="checkpoint-viz">
          <div class="checkpoint-viz__row">
            <div class="checkpoint-viz__label">Checkpointed</div>
            <div class="checkpoint-viz__blocks">
              <div class="checkpoint-viz__block checkpoint-viz__block--checkpoint">L1</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--recompute">L2</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--checkpoint">L3</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--recompute">L4</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--checkpoint">L5</div>
              <div class="checkpoint-viz__block checkpoint-viz__block--recompute">L6</div>
            </div>
          </div>
          <div class="checkpoint-viz__row">
            <div class="checkpoint-viz__label">Memory</div>
            <div class="checkpoint-viz__blocks">
              <div style="font-size: var(--text-xs); color: var(--accent-green);">O(sqrt(n)) activations stored</div>
            </div>
          </div>
        </div>

        <h3>Implementation</h3>

        <div class="code-block">
          <div class="code-block__header">PyTorch Checkpointing</div>
<pre><span class="keyword">from</span> torch.utils.checkpoint <span class="keyword">import</span> checkpoint

<span class="keyword">class</span> <span class="function">CheckpointedTransformer</span>(nn.Module):
    <span class="keyword">def</span> <span class="function">__init__</span>(self, n_layers):
        <span class="keyword">super</span>().__init__()
        self.layers = nn.ModuleList([
            TransformerBlock() <span class="keyword">for</span> _ <span class="keyword">in</span> range(n_layers)
        ])
    
    <span class="keyword">def</span> <span class="function">forward</span>(self, x):
        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:
            <span class="comment"># checkpoint() doesn't save activations for this segment</span>
            <span class="comment"># Instead, recomputes them during backward</span>
            x = checkpoint(layer, x, use_reentrant=<span class="keyword">False</span>)
        <span class="keyword">return</span> x</pre>
        </div>

        <h3>Memory vs Compute Tradeoff</h3>

        <table class="precision-table">
          <thead>
            <tr>
              <th>Strategy</th>
              <th>Memory</th>
              <th>Compute Overhead</th>
              <th>When to Use</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>No checkpointing</td>
              <td>O(n)</td>
              <td>0%</td>
              <td>Memory not a constraint</td>
            </tr>
            <tr>
              <td>Checkpoint every k layers</td>
              <td>O(n/k)</td>
              <td>~(k-1)/k × forward</td>
              <td>Moderate memory pressure</td>
            </tr>
            <tr>
              <td>Checkpoint every layer</td>
              <td>O(1) per layer</td>
              <td>~100% (2x forward)</td>
              <td>Severe memory constraints</td>
            </tr>
            <tr>
              <td>Selective checkpointing</td>
              <td>Variable</td>
              <td>Variable</td>
              <td>Checkpoint attention, keep FFN</td>
            </tr>
          </tbody>
        </table>

        <div class="callout info">
          <div class="callout-title">Selective Checkpointing</div>
          <p class="mb-0">
            Not all operations benefit equally from checkpointing. Attention has O(L²) activations
            but O(L²) recompute cost. FFN has O(L×d) activations but cheap recompute.
            <a href="https://arxiv.org/abs/2205.05198" target="_blank" rel="noopener">Selective checkpointing</a>
            only checkpoints the most memory-intensive operations.
          </p>
        </div>

        <div class="quiz quiz--micro" id="quiz-checkpoint">
          <div class="quiz-q">Checkpointing every layer reduces activation memory to O(1) per layer, but at what compute cost?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>No extra compute—just memory savings</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>~50% more forward compute</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>~100% more forward compute (recompute entire forward during backward)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 5: Gradient Accumulation -->
      <section class="section" id="section-4">
        <div class="section__number">05 - GRADIENT ACCUMULATION</div>
        <h2 class="section__title">Simulating Larger Batch Sizes</h2>

        <p>
          <a href="https://pytorch.org/docs/stable/notes/amp_examples.html#gradient-accumulation" target="_blank" rel="noopener">Gradient accumulation</a>
          lets you train with effective batch sizes larger than what fits in memory
          by accumulating gradients over multiple forward-backward passes before updating weights.
        </p>

        <h3>The Pattern</h3>

        <div class="timeline">
          <div class="timeline__row">
            <div class="timeline__label">Micro-batch 1</div>
            <div class="timeline__steps">
              <div class="timeline__step timeline__step--forward">FWD</div>
              <div class="timeline__step timeline__step--backward">BWD</div>
            </div>
          </div>
          <div class="timeline__row">
            <div class="timeline__label">Micro-batch 2</div>
            <div class="timeline__steps">
              <div class="timeline__step timeline__step--forward">FWD</div>
              <div class="timeline__step timeline__step--backward">BWD (accumulate)</div>
            </div>
          </div>
          <div class="timeline__row">
            <div class="timeline__label">Micro-batch 3</div>
            <div class="timeline__steps">
              <div class="timeline__step timeline__step--forward">FWD</div>
              <div class="timeline__step timeline__step--backward">BWD (accumulate)</div>
            </div>
          </div>
          <div class="timeline__row">
            <div class="timeline__label">Micro-batch 4</div>
            <div class="timeline__steps">
              <div class="timeline__step timeline__step--forward">FWD</div>
              <div class="timeline__step timeline__step--backward">BWD (accumulate)</div>
              <div class="timeline__step timeline__step--update">UPDATE</div>
            </div>
          </div>
        </div>

        <p>
          With 4 accumulation steps and micro-batch size 8, effective batch size = 32.
        </p>

        <h3>Implementation</h3>

        <div class="code-block">
          <div class="code-block__header">Gradient Accumulation Loop</div>
<pre>accumulation_steps = <span class="number">4</span>
optimizer.zero_grad()

<span class="keyword">for</span> i, (inputs, targets) <span class="keyword">in</span> enumerate(dataloader):
    <span class="comment"># Forward + backward (gradients accumulate)</span>
    <span class="keyword">with</span> autocast():
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss = loss / accumulation_steps  <span class="comment"># Scale loss</span>
    
    scaler.scale(loss).backward()
    
    <span class="comment"># Only update every N steps</span>
    <span class="keyword">if</span> (i + <span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:
        scaler.step(optimizer)
        scaler.update()
        optimizer.zero_grad()</pre>
        </div>

        <div class="callout warning">
          <div class="callout-title">Divide Loss by Accumulation Steps</div>
          <p class="mb-0">
            Since gradients are summed (not averaged) across accumulation steps, you must divide
            the loss by the number of accumulation steps to get correct gradient magnitudes.
            Alternatively, ensure your loss is already a mean over the batch.
          </p>
        </div>

        <h3>BatchNorm Interaction</h3>

        <p>
          <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">BatchNorm</a>
          computes statistics over micro-batches, not the effective batch.
          This can cause training instability with small micro-batches.
          Solutions:
        </p>

        <ul>
          <li><strong>SyncBatchNorm</strong> — synchronize statistics across GPUs</li>
          <li><strong>GroupNorm/LayerNorm</strong> — statistics independent of batch size</li>
          <li><strong>Ghost BatchNorm</strong> — compute stats over virtual sub-batches</li>
        </ul>

        <div class="quiz quiz--micro" id="quiz-accumulation">
          <div class="quiz-q">With gradient accumulation over 8 steps and micro-batch size 16, what is the effective batch size?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>16 (micro-batch only)</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>128 (8 × 16)</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>8 (accumulation steps only)</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Section 6: Training Numerics -->
      <section class="section" id="section-5">
        <div class="section__number">06 - TRAINING NUMERICS</div>
        <h2 class="section__title">Stability and Debugging</h2>

        <p>
          Training instability manifests as loss spikes, NaN losses, or gradients that explode/vanish.
          Understanding the numerical causes helps you fix them.
        </p>

        <h3>Gradient Clipping</h3>

        <p>
          <a href="https://arxiv.org/abs/1211.5063" target="_blank" rel="noopener">Gradient clipping</a>
          prevents exploding gradients by capping their magnitude:
        </p>

        <div class="code-block">
          <div class="code-block__header">Gradient Clipping in PyTorch</div>
<pre><span class="comment"># After backward, before optimizer step</span>
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">1.0</span>)

<span class="comment"># Or clip by value</span>
torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=<span class="number">0.5</span>)</pre>
        </div>

        <p>
          Common max_norm values: 1.0 for transformers, 0.1-0.5 for RNNs.
          Monitor gradient norms—if clipping activates frequently, investigate the cause.
        </p>

        <h3>Loss Spikes</h3>

        <p>
          Sudden loss increases often indicate numerical issues:
        </p>

        <table class="precision-table">
          <thead>
            <tr>
              <th>Symptom</th>
              <th>Likely Cause</th>
              <th>Fix</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Spike then recovery</td>
              <td>Outlier batch, FP16 overflow</td>
              <td>Skip batch, adjust loss scale</td>
            </tr>
            <tr>
              <td>Spike then NaN</td>
              <td>Gradient explosion</td>
              <td>Gradient clipping, lower LR</td>
            </tr>
            <tr>
              <td>Gradual divergence</td>
              <td>LR too high, bad hyperparams</td>
              <td>LR warmup, hyperparameter search</td>
            </tr>
            <tr>
              <td>NaN from start</td>
              <td>Weight init, input data issues</td>
              <td>Check data pipeline, init scale</td>
            </tr>
          </tbody>
        </table>

        <h3>Debugging NaN Gradients</h3>

        <div class="code-block">
          <div class="code-block__header">NaN Detection Hook</div>
<pre><span class="keyword">def</span> <span class="function">check_nan_hook</span>(module, grad_input, grad_output):
    <span class="keyword">for</span> i, grad <span class="keyword">in</span> enumerate(grad_output):
        <span class="keyword">if</span> grad <span class="keyword">is not None</span> <span class="keyword">and</span> torch.isnan(grad).any():
            <span class="keyword">print</span>(f<span class="string">"NaN in {module.__class__.__name__} grad_output[{i}]"</span>)
            <span class="keyword">print</span>(f<span class="string">"  grad stats: min={grad.min()}, max={grad.max()}"</span>)

<span class="comment"># Register on all modules</span>
<span class="keyword">for</span> module <span class="keyword">in</span> model.modules():
    module.register_backward_hook(check_nan_hook)</pre>
        </div>

        <div class="callout info">
          <div class="callout-title">Anomaly Detection Mode</div>
          <p class="mb-0">
            PyTorch's <a href="https://pytorch.org/docs/stable/autograd.html#anomaly-detection" target="_blank" rel="noopener">anomaly detection</a>
            tracks which operation produced NaN: <code>torch.autograd.set_detect_anomaly(True)</code>.
            Slows training significantly—use only for debugging.
          </p>
        </div>

        <h3>Weight Initialization</h3>

        <p>
          Proper initialization prevents vanishing/exploding gradients at training start.
          Common schemes:
        </p>

        <ul>
          <li><strong><a href="https://arxiv.org/abs/1502.01852" target="_blank" rel="noopener">He/Kaiming</a></strong> — 
            std = sqrt(2/fan_in), good for ReLU</li>
          <li><strong><a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf" target="_blank" rel="noopener">Xavier/Glorot</a></strong> — 
            std = sqrt(2/(fan_in + fan_out)), good for tanh/sigmoid</li>
          <li><strong>Small init for residuals</strong> — scale final layer of residual blocks by 1/sqrt(n_layers)</li>
        </ul>

        <p>
          GPT-style models often scale residual path weights by <code>1/sqrt(2 * n_layers)</code> 
          as noted in the <a href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" target="_blank" rel="noopener">GPT-2 paper</a>.
        </p>

        <div class="quiz quiz--micro" id="quiz-numerics">
          <div class="quiz-q">You see loss = NaN after a few hundred steps. Gradient norms were growing before the crash. Most likely fix?</div>
          <div class="quiz-opts">
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Increase learning rate</span>
            </div>
            <div class="quiz-opt" data-correct="true" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Add gradient clipping</span>
            </div>
            <div class="quiz-opt" data-correct="false" tabindex="0">
              <div class="quiz-mark"></div>
              <span>Remove loss scaling</span>
            </div>
          </div>
          <div class="quiz-fb"></div>
        </div>
      </section>

      <hr class="chunk-divider">

      <!-- Summary and Links -->
      <section class="section" id="section-6">
        <div class="section__number">07 - SUMMARY</div>
        <h2 class="section__title">Key Takeaways</h2>

        <div class="card">
          <div class="card__header">
            <div class="card__icon card__icon--blue">Training Memory Breakdown</div>
          </div>
          <ul>
            <li>Training needs ~4x model weights for Adam (weights + gradients + 2x optimizer states)</li>
            <li>Activations scale with batch size, sequence length, and model depth</li>
            <li>Backward pass costs ~2x forward due to computing gradients for both inputs and weights</li>
          </ul>
        </div>

        <div class="card" style="margin-top: var(--space-md);">
          <div class="card__header">
            <div class="card__icon card__icon--green">Memory Optimization Techniques</div>
          </div>
          <ul>
            <li><strong>Mixed precision</strong> — FP16/BF16 halves memory for activations and gradients</li>
            <li><strong>Activation checkpointing</strong> — trade compute for memory by recomputing activations</li>
            <li><strong>Gradient accumulation</strong> — simulate larger batches without more memory</li>
          </ul>
        </div>

        <div class="card" style="margin-top: var(--space-md);">
          <div class="card__header">
            <div class="card__icon card__icon--orange">Numerical Stability</div>
          </div>
          <ul>
            <li>Use loss scaling with FP16 (not needed for BF16)</li>
            <li>Gradient clipping prevents explosion—common max_norm = 1.0</li>
            <li>Proper initialization scales with network depth</li>
          </ul>
        </div>
      </section>

      <!-- Chapter navigation -->
      <nav class="chapter-nav">
        <div class="chapter-nav__controls">
          <a href="09-quantization.html" class="chapter-nav__link">&lt; Prev</a>
          <span class="chapter-nav__sep">|</span>
          <a href="../index.html" class="chapter-nav__link">Home</a>
          <span class="chapter-nav__sep">|</span>
          <a href="11-multi-gpu.html" class="chapter-nav__link">Next &gt;</a>
        </div>
        <div class="chapter-nav__current">Chapter 10: Training Kernels</div>
      </nav>
    </main>

    <script>
      // Quiz functionality
      document.querySelectorAll('.quiz-opt').forEach(opt => {
        opt.addEventListener('click', function() {
          const quiz = this.closest('.quiz');
          if (quiz.classList.contains('answered')) return;
          
          quiz.classList.add('answered');
          const isCorrect = this.dataset.correct === 'true';
          this.classList.add(isCorrect ? 'correct' : 'incorrect');
          
          if (!isCorrect) {
            quiz.querySelector('[data-correct="true"]').classList.add('correct');
          }
          
          const fb = quiz.querySelector('.quiz-fb');
          fb.textContent = isCorrect 
            ? 'Correct!' 
            : 'Not quite. The correct answer is highlighted.';
          fb.className = 'quiz-fb ' + (isCorrect ? 'quiz-fb--correct' : 'quiz-fb--incorrect');
        });
        
        opt.addEventListener('keydown', function(e) {
          if (e.key === 'Enter' || e.key === ' ') {
            e.preventDefault();
            this.click();
          }
        });
      });
    </script>
  </body>
</html>
