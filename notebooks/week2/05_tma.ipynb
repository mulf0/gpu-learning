{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 5: TMA — Tensor Memory Accelerator\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Learn to use Hopper's TMA for efficient bulk data transfers.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Even with pipelining, SMs spend cycles computing addresses for memory loads. On Hopper (H100), the **Tensor Memory Accelerator (TMA)** offloads this entirely — the SM just says \"load this tile\" and moves on.\n",
    "\n",
    "**Note:** TMA requires Hopper (SM90) or newer. This notebook covers concepts and shows the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.testing import do_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "### Traditional Memory Loads\n",
    "\n",
    "```\n",
    "SM Work for Loading a 64x64 Tile:\n",
    "\n",
    "1. Calculate 4096 addresses (64*64)\n",
    "2. Issue 4096 load instructions  \n",
    "3. Wait for data\n",
    "4. Repeat for next tile\n",
    "\n",
    "→ SM is busy with bookkeeping, not compute!\n",
    "```\n",
    "\n",
    "### TMA Loads\n",
    "\n",
    "```\n",
    "SM Work:\n",
    "\n",
    "1. Tell TMA: \"Load tile at (row, col)\" (one instruction)\n",
    "2. Go do compute\n",
    "3. TMA handles everything in the background\n",
    "\n",
    "→ SM focuses on math!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU capability\n",
    "def check_tma_support():\n",
    "    \"\"\"Check if current GPU supports TMA.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return False, \"No CUDA available\"\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    sm_major = props.major\n",
    "    sm_minor = props.minor\n",
    "    \n",
    "    # TMA requires SM90 (Hopper) or newer\n",
    "    has_tma = sm_major >= 9\n",
    "    \n",
    "    return has_tma, f\"SM{sm_major}{sm_minor} ({props.name})\"\n",
    "\n",
    "has_tma, gpu_info = check_tma_support()\n",
    "print(f\"GPU: {gpu_info}\")\n",
    "print(f\"TMA Support: {'Yes' if has_tma else 'No (requires Hopper/SM90+)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore (15 min)\n",
    "\n",
    "### TMA Key Features\n",
    "\n",
    "| Feature | Benefit |\n",
    "|---------|--------|\n",
    "| **Descriptor-based** | Define tensor layout once, reuse |\n",
    "| **2D/3D tiles** | Native multidimensional support |\n",
    "| **Multicast** | Same data to multiple SMs |\n",
    "| **Async** | Fully non-blocking |\n",
    "| **Address calculation** | Done by TMA unit, not SM |\n",
    "\n",
    "### TMA Descriptor\n",
    "\n",
    "A TMA descriptor contains:\n",
    "- Tensor base address\n",
    "- Tensor dimensions (global shape)\n",
    "- Tile dimensions (what to load)\n",
    "- Data type\n",
    "- Swizzle pattern (for bank conflict avoidance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual TMA descriptor (actual API is more complex)\n",
    "class TMAConcept:\n",
    "    \"\"\"Conceptual representation of TMA operations.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_descriptor(tensor, tile_shape):\n",
    "        \"\"\"Create a TMA descriptor for a tensor.\n",
    "        \n",
    "        In real Triton/CUDA:\n",
    "        - Uses cuTensorMapEncode() to create descriptor\n",
    "        - Descriptor lives in constant memory\n",
    "        - Kernel receives pointer to descriptor\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'base_ptr': tensor.data_ptr(),\n",
    "            'global_shape': tensor.shape,\n",
    "            'tile_shape': tile_shape,\n",
    "            'dtype': tensor.dtype,\n",
    "            'strides': tensor.stride(),\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def async_load(desc, smem, tile_coords):\n",
    "        \"\"\"Conceptual async TMA load.\n",
    "        \n",
    "        Real instruction: cp.async.bulk.tensor\n",
    "        \n",
    "        SM just specifies:\n",
    "        - Which tile (by coordinates)\n",
    "        - Where in SMEM to put it\n",
    "        \n",
    "        TMA unit handles:\n",
    "        - Address calculation\n",
    "        - Memory transactions\n",
    "        - Swizzling\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Example setup\n",
    "tensor = torch.randn(1024, 1024, device='cuda', dtype=torch.float16)\n",
    "tile_shape = (64, 64)\n",
    "\n",
    "desc = TMAConcept.create_descriptor(tensor, tile_shape)\n",
    "print(\"Conceptual TMA Descriptor:\")\n",
    "for k, v in desc.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept (10 min)\n",
    "\n",
    "### TMA vs Traditional Loads\n",
    "\n",
    "```\n",
    "Traditional Load (per tile):\n",
    "┌───────────┐    ┌───────────┐    ┌───────────┐\n",
    "│ Addr calc │ →  │   Load    │ →  │  Barrier  │\n",
    "│ (SM busy) │    │ (SM busy) │    │  (wait)   │\n",
    "└───────────┘    └───────────┘    └───────────┘\n",
    "\n",
    "TMA Load (per tile):\n",
    "┌───────────┐    \n",
    "│TMA trigger│  → SM does compute while TMA loads!\n",
    "│(1 instr)  │    \n",
    "└───────────┘    \n",
    "```\n",
    "\n",
    "### Multicast\n",
    "\n",
    "TMA can send the same data to multiple SMs simultaneously:\n",
    "\n",
    "```\n",
    "Without Multicast:                With Multicast:\n",
    "HBM → SM0                         HBM ─┬→ SM0\n",
    "HBM → SM1   (3 separate loads)         ├→ SM1  (1 load, 3 destinations)\n",
    "HBM → SM2                              └→ SM2\n",
    "```\n",
    "\n",
    "This is especially useful for broadcast patterns like attention's K/V sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triton TMA API (Hopper+)\n",
    "# Note: This requires Hopper GPU to actually run\n",
    "\n",
    "if has_tma:\n",
    "    @triton.jit\n",
    "    def matmul_tma(\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        M, N, K,\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn, \n",
    "        stride_cm, stride_cn,\n",
    "        BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "    ):\n",
    "        \"\"\"Matmul using TMA for data loading.\n",
    "        \n",
    "        On Hopper, Triton can use TMA automatically when:\n",
    "        - Tensor layouts match TMA requirements\n",
    "        - Using tl.load with proper masking\n",
    "        \n",
    "        For explicit TMA control, use experimental APIs.\n",
    "        \"\"\"\n",
    "        pid_m = tl.program_id(0)\n",
    "        pid_n = tl.program_id(1)\n",
    "        \n",
    "        offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "        offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "        offs_k = tl.arange(0, BLOCK_K)\n",
    "        \n",
    "        a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "        b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "        \n",
    "        acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "        \n",
    "        for k in range(0, K, BLOCK_K):\n",
    "            # On Hopper, these loads may use TMA automatically\n",
    "            a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "            b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "            \n",
    "            acc += tl.dot(a, b)\n",
    "            \n",
    "            a_ptrs += BLOCK_K * stride_ak\n",
    "            b_ptrs += BLOCK_K * stride_bk\n",
    "        \n",
    "        c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "        tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n",
    "    \n",
    "    print(\"TMA matmul kernel defined (requires Hopper to run)\")\n",
    "else:\n",
    "    print(\"TMA not available on this GPU - showing concepts only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It (30 min)\n",
    "\n",
    "### TMA Benefits Simulation\n",
    "\n",
    "Let's simulate the performance benefit of TMA by comparing:\n",
    "1. Traditional loads (SM does address calculation)\n",
    "2. Simulated TMA (address calculation \"free\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_tma_benefit(M, N, K, BLOCK_M=64, BLOCK_N=64, BLOCK_K=32):\n",
    "    \"\"\"Estimate potential benefit of TMA vs traditional loads.\n",
    "    \n",
    "    This is a simplified model - real benefits depend on many factors.\n",
    "    \"\"\"\n",
    "    # Number of tiles\n",
    "    num_tiles_m = (M + BLOCK_M - 1) // BLOCK_M\n",
    "    num_tiles_n = (N + BLOCK_N - 1) // BLOCK_N\n",
    "    num_tiles_k = (K + BLOCK_K - 1) // BLOCK_K\n",
    "    \n",
    "    # Elements per tile\n",
    "    a_tile_elems = BLOCK_M * BLOCK_K\n",
    "    b_tile_elems = BLOCK_K * BLOCK_N\n",
    "    \n",
    "    # Traditional: SM calculates addresses\n",
    "    # Rough estimate: ~4 cycles per address calculation\n",
    "    addr_calc_cycles = (a_tile_elems + b_tile_elems) * 4\n",
    "    \n",
    "    # Compute cycles per tile\n",
    "    # FMA throughput: ~256 FMAs per cycle (simplified for FP16)\n",
    "    compute_flops = BLOCK_M * BLOCK_N * BLOCK_K * 2\n",
    "    compute_cycles = compute_flops / 256\n",
    "    \n",
    "    # Memory latency\n",
    "    mem_latency_cycles = 400  # Approximate\n",
    "    \n",
    "    # Total per tile (simplified)\n",
    "    traditional_cycles = addr_calc_cycles + max(compute_cycles, mem_latency_cycles)\n",
    "    tma_cycles = compute_cycles + mem_latency_cycles / 4  # TMA overlaps better\n",
    "    \n",
    "    return {\n",
    "        'tiles': num_tiles_m * num_tiles_n * num_tiles_k,\n",
    "        'traditional_cycles_per_tile': traditional_cycles,\n",
    "        'tma_cycles_per_tile': tma_cycles,\n",
    "        'estimated_speedup': traditional_cycles / tma_cycles,\n",
    "        'addr_calc_overhead': addr_calc_cycles / traditional_cycles * 100,\n",
    "    }\n",
    "\n",
    "print(\"Estimated TMA Benefit\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for size in [1024, 2048, 4096, 8192]:\n",
    "    est = estimate_tma_benefit(size, size, size)\n",
    "    print(f\"\\nSize {size}x{size}:\")\n",
    "    print(f\"  Tiles: {est['tiles']:,}\")\n",
    "    print(f\"  Address calculation overhead: {est['addr_calc_overhead']:.1f}%\")\n",
    "    print(f\"  Estimated TMA speedup: {est['estimated_speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When TMA Helps Most\n",
    "\n",
    "TMA benefits are highest when:\n",
    "1. **Small tiles** (more address calculations per byte)\n",
    "2. **Complex layouts** (strided access, padding)\n",
    "3. **Multicast patterns** (same data to many SMs)\n",
    "4. **High occupancy** (more concurrent operations)\n",
    "\n",
    "### TMA Limitations\n",
    "\n",
    "- Only available on Hopper (H100) and newer\n",
    "- Tile sizes must be powers of 2 or specific shapes\n",
    "- Requires proper tensor alignment\n",
    "- Descriptor setup has overhead (but amortized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real benchmark (if on Hopper)\n",
    "if has_tma:\n",
    "    def benchmark_tma(M, N, K):\n",
    "        a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "        b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "        c = torch.empty(M, N, device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32\n",
    "        grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "        \n",
    "        ms = do_bench(lambda: matmul_tma[grid](\n",
    "            a, b, c, M, N, K,\n",
    "            a.stride(0), a.stride(1),\n",
    "            b.stride(0), b.stride(1),\n",
    "            c.stride(0), c.stride(1),\n",
    "            BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "        ))\n",
    "        \n",
    "        flops = 2 * M * N * K\n",
    "        tflops = flops / (ms * 1e-3) / 1e12\n",
    "        \n",
    "        ms_torch = do_bench(lambda: torch.mm(a, b))\n",
    "        tflops_torch = flops / (ms_torch * 1e-3) / 1e12\n",
    "        \n",
    "        return {'triton_tflops': tflops, 'torch_tflops': tflops_torch}\n",
    "    \n",
    "    print(\"TMA Matmul Benchmark (Hopper)\")\n",
    "    print(\"=\" * 50)\n",
    "    for size in [2048, 4096]:\n",
    "        result = benchmark_tma(size, size, size)\n",
    "        print(f\"Size {size}x{size}: Triton={result['triton_tflops']:.1f} TFLOPS, PyTorch={result['torch_tflops']:.1f} TFLOPS\")\n",
    "else:\n",
    "    print(\"Skipping TMA benchmark - requires Hopper GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify (10 min)\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**Q1:** What does TMA offload from the SM?\n",
    "\n",
    "A) Floating point computation  \n",
    "B) Address calculation for memory loads  \n",
    "C) Thread synchronization  \n",
    "D) Register allocation\n",
    "\n",
    "**Q2:** What GPU architecture introduced TMA?\n",
    "\n",
    "A) Ampere (A100)  \n",
    "B) Hopper (H100)  \n",
    "C) Turing  \n",
    "D) Volta\n",
    "\n",
    "**Q3:** What is TMA multicast useful for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quiz Answers\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Q1: B) Address calculation for memory loads\")\n",
    "print(\"    TMA handles address calculation, boundary checks, and\")\n",
    "print(\"    memory transactions - SM just triggers the load.\")\n",
    "print()\n",
    "print(\"Q2: B) Hopper (H100)\")\n",
    "print(\"    TMA is a Hopper-specific feature (SM90+).\")\n",
    "print()\n",
    "print(\"Q3: Broadcast patterns\")\n",
    "print(\"    When multiple SMs need the same data (like K/V in attention),\")\n",
    "print(\"    TMA can send it to all of them with a single memory read.\")\n",
    "print(\"    This saves memory bandwidth significantly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **TMA = Tensor Memory Accelerator** (Hopper+ only)\n",
    "2. **Offloads address calculation** from SM to dedicated unit\n",
    "3. **Descriptor-based**: define tensor once, load tiles by coordinates\n",
    "4. **Multicast**: same data to multiple SMs efficiently\n",
    "5. **Works with pipelining** for even better latency hiding\n",
    "\n",
    "### TMA vs Traditional\n",
    "\n",
    "| Aspect | Traditional | TMA |\n",
    "|--------|------------|-----|\n",
    "| Address calc | SM cycles | TMA unit (free) |\n",
    "| Tile loads | Many instructions | One instruction |\n",
    "| Multicast | N loads | 1 load, N destinations |\n",
    "| Swizzling | Manual | Automatic |\n",
    "\n",
    "### Tomorrow: Tensor Cores\n",
    "\n",
    "Now that we can load data efficiently, let's use the fastest compute units: **Tensor Cores**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
