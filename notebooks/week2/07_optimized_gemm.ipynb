{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 7: Optimized GEMM — Putting It All Together\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Combine all optimizations to build a production-quality GEMM achieving 80%+ of peak.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Week 1: Naive → 500 GFLOPS  \n",
    "Week 2 Goal: Optimized → **80%+ of cuBLAS**\n",
    "\n",
    "We'll use every technique from this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.testing import do_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "### Optimization Checklist\n",
    "\n",
    "| Day | Technique | Status |\n",
    "|-----|-----------|--------|\n",
    "| 1 | Profiling & Metrics | ✓ Can measure |\n",
    "| 2 | Coalesced Memory Access | ✓ |\n",
    "| 3 | Bank Conflict Free SMEM | ✓ |\n",
    "| 4 | Software Pipelining | ✓ |\n",
    "| 5 | TMA (Hopper) | ✓ (if available) |\n",
    "| 6 | Tensor Cores | ✓ |\n",
    "\n",
    "Let's combine them all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore (15 min)\n",
    "\n",
    "### The Ultimate GEMM Recipe\n",
    "\n",
    "```\n",
    "1. Load tiles via TMA (or coalesced loads with pipelining)\n",
    "2. Store in SMEM with padding (no bank conflicts)\n",
    "3. Compute via Tensor Cores (tl.dot with FP16)\n",
    "4. Accumulate in FP32 (precision)\n",
    "5. Pipeline loads with compute (latency hiding)\n",
    "6. Tune tile sizes via autotuning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full configuration space for autotuning\n",
    "def get_autotune_configs():\n",
    "    \"\"\"Generate autotuning configurations.\"\"\"\n",
    "    configs = []\n",
    "    \n",
    "    # Block sizes: larger = more data reuse, but more SMEM\n",
    "    block_sizes = [\n",
    "        (64, 64, 32),\n",
    "        (128, 64, 32),\n",
    "        (64, 128, 32),\n",
    "        (128, 128, 32),\n",
    "        (128, 128, 64),\n",
    "        (256, 64, 32),\n",
    "        (64, 256, 32),\n",
    "    ]\n",
    "    \n",
    "    # Pipeline stages\n",
    "    stages_options = [2, 3, 4]\n",
    "    \n",
    "    # Warps per block\n",
    "    warps_options = [4, 8]\n",
    "    \n",
    "    for bm, bn, bk in block_sizes:\n",
    "        for stages in stages_options:\n",
    "            for warps in warps_options:\n",
    "                configs.append(\n",
    "                    triton.Config(\n",
    "                        {'BLOCK_M': bm, 'BLOCK_N': bn, 'BLOCK_K': bk},\n",
    "                        num_stages=stages,\n",
    "                        num_warps=warps,\n",
    "                    )\n",
    "                )\n",
    "    \n",
    "    return configs\n",
    "\n",
    "print(f\"Total configurations: {len(get_autotune_configs())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept (10 min)\n",
    "\n",
    "### Key Performance Factors\n",
    "\n",
    "1. **Tile Size**: Balance between data reuse and occupancy\n",
    "2. **Pipeline Depth**: Hide memory latency\n",
    "3. **Warp Count**: Enough threads to saturate Tensor Cores\n",
    "4. **Data Type**: FP16 for Tensor Cores, FP32 accumulator\n",
    "\n",
    "### Theoretical Peak Analysis\n",
    "\n",
    "```\n",
    "H100 FP16 Tensor Core: 990 TFLOPS\n",
    "H100 HBM Bandwidth: 3.35 TB/s\n",
    "\n",
    "For GEMM C = A × B:\n",
    "  FLOPS = 2 × M × N × K\n",
    "  Bytes = 2 × (M×K + K×N + M×N)  [FP16]\n",
    "\n",
    "Arithmetic Intensity = FLOPS / Bytes\n",
    "Balance Point = 990 TFLOPS / 3.35 TB/s = 295 FLOPS/byte\n",
    "\n",
    "For 4096³ GEMM:\n",
    "  AI = 2×4096³ / (2×3×4096²) ≈ 1365 FLOPS/byte\n",
    "  → COMPUTE BOUND (good for Tensor Cores!)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It (30 min)\n",
    "\n",
    "### The Production GEMM Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 64}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 256, 'BLOCK_K': 32}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_M': 256, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_stages=2, num_warps=4),\n",
    "    ],\n",
    "    key=['M', 'N', 'K'],\n",
    ")\n",
    "@triton.jit\n",
    "def gemm_optimized(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Production-quality GEMM kernel.\n",
    "    \n",
    "    Optimizations:\n",
    "    - Tensor Cores via tl.dot (FP16 input, FP32 accumulator)\n",
    "    - Software pipelining (via num_stages)\n",
    "    - Autotuned tile sizes\n",
    "    - Coalesced memory access\n",
    "    \"\"\"\n",
    "    # Program IDs\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    # Swizzle for better L2 cache utilization\n",
    "    # Groups programs to improve spatial locality\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_N)\n",
    "    GROUP_SIZE = 8\n",
    "    pid_m_group = pid_m // GROUP_SIZE\n",
    "    pid_m_in_group = pid_m % GROUP_SIZE\n",
    "    pid_n_swizzled = pid_m_group * GROUP_SIZE + pid_m_in_group\n",
    "    pid_m_swizzled = pid_n\n",
    "    \n",
    "    # For simplicity, use direct mapping (swizzle can be added)\n",
    "    pid_m_final = pid_m\n",
    "    pid_n_final = pid_n\n",
    "    \n",
    "    # Block offsets\n",
    "    offs_m = pid_m_final * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n_final * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    # Initial pointers\n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    # Accumulator (FP32 for precision)\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    # Main loop over K dimension\n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        # Boundary masks\n",
    "        mask_a = (offs_m[:, None] < M) & (offs_k[None, :] + k < K)\n",
    "        mask_b = (offs_k[:, None] + k < K) & (offs_n[None, :] < N)\n",
    "        \n",
    "        # Load tiles (compiler will pipeline these)\n",
    "        a = tl.load(a_ptrs, mask=mask_a, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=mask_b, other=0.0)\n",
    "        \n",
    "        # Tensor Core MMA\n",
    "        acc += tl.dot(a, b)\n",
    "        \n",
    "        # Advance pointers\n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    # Store result\n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    mask_c = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n",
    "    tl.store(c_ptrs, acc, mask=mask_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_optimized(a, b):\n",
    "    \"\"\"Wrapper for optimized GEMM.\"\"\"\n",
    "    M, K = a.shape\n",
    "    K2, N = b.shape\n",
    "    assert K == K2, f\"Inner dimensions must match: {K} vs {K2}\"\n",
    "    \n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
    "    \n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n",
    "    \n",
    "    gemm_optimized[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_final(M, N, K):\n",
    "    \"\"\"Final benchmark comparing optimized kernel to cuBLAS.\"\"\"\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Warmup and autotune\n",
    "    _ = matmul_optimized(a, b)\n",
    "    \n",
    "    # Benchmark Triton\n",
    "    ms_triton = do_bench(lambda: matmul_optimized(a, b))\n",
    "    \n",
    "    # Benchmark cuBLAS (PyTorch)\n",
    "    ms_cublas = do_bench(lambda: torch.mm(a, b))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    flops = 2 * M * N * K\n",
    "    tflops_triton = flops / (ms_triton * 1e-3) / 1e12\n",
    "    tflops_cublas = flops / (ms_cublas * 1e-3) / 1e12\n",
    "    \n",
    "    # Verify correctness\n",
    "    c_triton = matmul_optimized(a, b)\n",
    "    c_cublas = torch.mm(a, b).float()\n",
    "    is_correct = torch.allclose(c_triton, c_cublas, rtol=1e-2, atol=1e-2)\n",
    "    \n",
    "    return {\n",
    "        'triton_ms': ms_triton,\n",
    "        'cublas_ms': ms_cublas,\n",
    "        'triton_tflops': tflops_triton,\n",
    "        'cublas_tflops': tflops_cublas,\n",
    "        'efficiency': tflops_triton / tflops_cublas * 100,\n",
    "        'correct': is_correct,\n",
    "    }\n",
    "\n",
    "print(\"Optimized GEMM Benchmark\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Size':<15} {'Triton':<12} {'cuBLAS':<12} {'Efficiency':<12} {'Correct':<8}\")\n",
    "print(f\"{'':15} {'(TFLOPS)':<12} {'(TFLOPS)':<12} {'(vs cuBLAS)':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for size in [1024, 2048, 4096, 8192]:\n",
    "    result = benchmark_final(size, size, size)\n",
    "    status = \"PASS\" if result['correct'] else \"FAIL\"\n",
    "    print(f\"{f'{size}x{size}':<15} {result['triton_tflops']:<12.1f} {result['cublas_tflops']:<12.1f} \"\n",
    "          f\"{result['efficiency']:<12.1f}% {status:<8}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance():\n",
    "    \"\"\"Detailed performance analysis.\"\"\"\n",
    "    # Get GPU info\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    \n",
    "    print(f\"GPU: {props.name}\")\n",
    "    print(f\"SM Count: {props.multi_processor_count}\")\n",
    "    print(f\"Memory: {props.total_memory / 1e9:.1f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Benchmark across sizes\n",
    "    print(\"Performance scaling:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    sizes = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        if size > 8192 and props.total_memory < 24e9:\n",
    "            print(f\"{size}x{size}: Skipped (insufficient memory)\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            result = benchmark_final(size, size, size)\n",
    "            results.append((size, result))\n",
    "            print(f\"{size}x{size}: {result['triton_tflops']:.1f} TFLOPS ({result['efficiency']:.1f}% of cuBLAS)\")\n",
    "        except Exception as e:\n",
    "            print(f\"{size}x{size}: Error - {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "results = analyze_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify (10 min)\n",
    "\n",
    "### Final Checklist\n",
    "\n",
    "Our optimized GEMM should achieve:\n",
    "\n",
    "- [ ] 80%+ of cuBLAS performance on large matrices\n",
    "- [ ] Correct results (< 1% error vs cuBLAS)\n",
    "- [ ] Uses Tensor Cores (high TFLOPS)\n",
    "- [ ] Coalesced memory access\n",
    "- [ ] Software pipelining\n",
    "- [ ] Autotuned for different sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_optimization_goals():\n",
    "    \"\"\"Verify we met Week 2 goals.\"\"\"\n",
    "    print(\"Week 2 Goals Verification\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test on 4096x4096 (representative size)\n",
    "    result = benchmark_final(4096, 4096, 4096)\n",
    "    \n",
    "    goals = [\n",
    "        (\"80%+ of cuBLAS\", result['efficiency'] >= 80),\n",
    "        (\"Correct results\", result['correct']),\n",
    "        (\"High TFLOPS (>100)\", result['triton_tflops'] > 100),\n",
    "    ]\n",
    "    \n",
    "    all_passed = True\n",
    "    for goal, passed in goals:\n",
    "        status = \"PASS\" if passed else \"FAIL\"\n",
    "        print(f\"  {goal}: {status}\")\n",
    "        if not passed:\n",
    "            all_passed = False\n",
    "    \n",
    "    print()\n",
    "    if all_passed:\n",
    "        print(\"All Week 2 goals achieved!\")\n",
    "    else:\n",
    "        print(\"Some goals not met - may need hardware-specific tuning.\")\n",
    "    \n",
    "    return all_passed\n",
    "\n",
    "verify_optimization_goals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Week 2 Achievements\n",
    "\n",
    "| Day | Topic | Key Learning |\n",
    "|-----|-------|-------------|\n",
    "| 1 | Profiling | Measure before optimizing |\n",
    "| 2 | Coalescing | Adjacent threads → adjacent memory |\n",
    "| 3 | Bank Conflicts | Stride-33 beats stride-32 |\n",
    "| 4 | Pipelining | Overlap loads with compute |\n",
    "| 5 | TMA | Hopper's address-free loads |\n",
    "| 6 | Tensor Cores | FP16 for 66x speedup |\n",
    "| 7 | Integration | Combine all techniques |\n",
    "\n",
    "### Journey So Far\n",
    "\n",
    "```\n",
    "Week 1 Day 1:  Naive Python       →  0.001 GFLOPS\n",
    "Week 1 Day 2:  NumPy              →  50 GFLOPS\n",
    "Week 1 Day 2:  CuPy               →  5,000 GFLOPS\n",
    "Week 1 Day 7:  Tiled Triton       →  500 GFLOPS\n",
    "Week 2 Day 7:  Optimized Triton   →  80%+ of cuBLAS!\n",
    "```\n",
    "\n",
    "### What's Next: Week 3\n",
    "\n",
    "Now that we can write fast kernels, we'll tackle the **attention mechanism**:\n",
    "- Dot products and softmax\n",
    "- Numerical stability\n",
    "- Online algorithms\n",
    "- Building FlashAttention from scratch!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
