{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1, Day 3: GPU Architecture\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Understand how GPUs execute code - SMs, warps, and the SIMT model.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "CuPy gave us 50-100x speedup \"for free.\" But to write our own fast kernels, we need to understand:\n",
    "1. How GPU hardware is organized\n",
    "2. How thousands of threads execute in parallel\n",
    "3. What makes GPU code fast (or slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"CuPy not available - conceptual content still applies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "**Question:** How can a GPU execute millions of threads efficiently?\n",
    "\n",
    "Let's query our GPU to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    props = cp.cuda.runtime.getDeviceProperties(0)\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"GPU SPECIFICATIONS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Name: {props['name'].decode()}\")\n",
    "    print(f\"Compute Capability: {props['major']}.{props['minor']}\")\n",
    "    print(f\"\\nStreaming Multiprocessors (SMs): {props['multiProcessorCount']}\")\n",
    "    print(f\"Max threads per SM: {props['maxThreadsPerMultiProcessor']}\")\n",
    "    print(f\"Max threads per block: {props['maxThreadsPerBlock']}\")\n",
    "    print(f\"\\nTotal memory: {props['totalGlobalMem'] / 1e9:.1f} GB\")\n",
    "    print(f\"Shared memory per block: {props['sharedMemPerBlock'] / 1024:.0f} KB\")\n",
    "    print(f\"Registers per block: {props['regsPerBlock']}\")\n",
    "    print(f\"\\nWarp size: {props['warpSize']} threads\")\n",
    "else:\n",
    "    print(\"Example: NVIDIA H100 GPU\")\n",
    "    print(\"  SMs: 132\")\n",
    "    print(\"  Max threads per SM: 2048\")\n",
    "    print(\"  Warp size: 32 threads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore - The GPU Hierarchy (15 min)\n",
    "\n",
    "### GPU Organization\n",
    "\n",
    "```\n",
    "GPU Device\n",
    "├── SM 0 (Streaming Multiprocessor)\n",
    "│   ├── Warp 0 (32 threads)\n",
    "│   ├── Warp 1 (32 threads)\n",
    "│   ├── ...\n",
    "│   └── Shared Memory (per-SM)\n",
    "├── SM 1\n",
    "├── ...\n",
    "└── Global Memory (HBM)\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "| Level | What it is | Size |\n",
    "|-------|-----------|------|\n",
    "| GPU | The whole device | 1 |\n",
    "| SM | Streaming Multiprocessor | ~100-200 per GPU |\n",
    "| Warp | 32 threads in lockstep | 64 per SM (max) |\n",
    "| Thread | Individual execution context | 1000s per SM |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive visualization concept\n",
    "print(\"GPU HIERARCHY VISUALIZATION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\")\n",
    "print(\"  [GPU Device]\")\n",
    "print(\"  │\")\n",
    "print(\"  ├── [SM 0] ─── [Warp 0] ─ T0  T1  T2  ... T31\")\n",
    "print(\"  │         │── [Warp 1] ─ T32 T33 T34 ... T63\")\n",
    "print(\"  │         │── ...\")\n",
    "print(\"  │         └── [Shared Memory: 48-228 KB]\")\n",
    "print(\"  │\")\n",
    "print(\"  ├── [SM 1] ─── [Warp 0] ─ ...\")\n",
    "print(\"  │         └── ...\")\n",
    "print(\"  │\")\n",
    "print(\"  └── [Global Memory: 40-192 GB HBM]\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"For interactive visualization, see:\")\n",
    "print(\"  ../lessons/gpu-architecture.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Warp: The Most Important Concept\n",
    "\n",
    "**A warp is 32 threads that execute the SAME instruction at the SAME time.**\n",
    "\n",
    "This is called SIMT (Single Instruction, Multiple Threads).\n",
    "\n",
    "Think of it like:\n",
    "- 32 soldiers marching in perfect sync\n",
    "- Each soldier carries different data\n",
    "- But they all do the same action at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate warp execution\n",
    "def simulate_warp_execution(instructions, data_per_thread):\n",
    "    \"\"\"Simulate how a warp executes instructions.\"\"\"\n",
    "    num_threads = 32\n",
    "    thread_data = data_per_thread.copy()\n",
    "    \n",
    "    print(f\"Warp with {num_threads} threads\")\n",
    "    print(f\"Initial data: {thread_data[:8]}... (showing first 8 threads)\")\n",
    "    print()\n",
    "    \n",
    "    for inst in instructions:\n",
    "        print(f\"All 32 threads execute: {inst}\")\n",
    "        if inst == \"MULTIPLY BY 2\":\n",
    "            thread_data = [x * 2 for x in thread_data]\n",
    "        elif inst == \"ADD 10\":\n",
    "            thread_data = [x + 10 for x in thread_data]\n",
    "        print(f\"  Result: {thread_data[:8]}...\")\n",
    "        print()\n",
    "    \n",
    "    return thread_data\n",
    "\n",
    "# Example: Each thread starts with its thread ID\n",
    "initial_data = list(range(32))\n",
    "instructions = [\"MULTIPLY BY 2\", \"ADD 10\"]\n",
    "\n",
    "result = simulate_warp_execution(instructions, initial_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept - Warp Divergence (10 min)\n",
    "\n",
    "### What happens when threads take different paths?\n",
    "\n",
    "If threads in a warp hit an `if-else`, the warp must execute BOTH paths:\n",
    "1. Execute the `if` branch (threads in `else` wait)\n",
    "2. Execute the `else` branch (threads in `if` wait)\n",
    "\n",
    "This is called **warp divergence** and it kills performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_divergence(thread_ids, condition_fn):\n",
    "    \"\"\"Simulate warp divergence.\"\"\"\n",
    "    # Evaluate condition for each thread\n",
    "    conditions = [condition_fn(tid) for tid in thread_ids]\n",
    "    \n",
    "    true_threads = [tid for tid, c in zip(thread_ids, conditions) if c]\n",
    "    false_threads = [tid for tid, c in zip(thread_ids, conditions) if not c]\n",
    "    \n",
    "    print(\"Warp executes if-else statement\")\n",
    "    print(f\"Condition: thread_id % 2 == 0\")\n",
    "    print()\n",
    "    print(f\"Step 1: Execute IF branch\")\n",
    "    print(f\"  Active threads ({len(true_threads)}): {true_threads[:8]}...\")\n",
    "    print(f\"  Waiting threads ({len(false_threads)}): {false_threads[:8]}...\")\n",
    "    print()\n",
    "    print(f\"Step 2: Execute ELSE branch\")\n",
    "    print(f\"  Active threads ({len(false_threads)}): {false_threads[:8]}...\")\n",
    "    print(f\"  Waiting threads ({len(true_threads)}): {true_threads[:8]}...\")\n",
    "    print()\n",
    "    print(f\"Total cycles: 2 (50% efficiency)\")\n",
    "    print(f\"Without divergence: 1 cycle (100% efficiency)\")\n",
    "\n",
    "# Half the threads go one way, half go another\n",
    "thread_ids = list(range(32))\n",
    "simulate_divergence(thread_ids, lambda tid: tid % 2 == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoiding Divergence\n",
    "\n",
    "**Bad:** Condition based on thread ID within warp\n",
    "```python\n",
    "if thread_id % 2 == 0:  # Half diverge!\n",
    "    do_something()\n",
    "else:\n",
    "    do_other()\n",
    "```\n",
    "\n",
    "**Good:** Condition based on data (all threads likely same path)\n",
    "```python\n",
    "if input_value > threshold:  # Usually all same\n",
    "    do_something()\n",
    "```\n",
    "\n",
    "**Better:** No branches at all\n",
    "```python\n",
    "result = a * mask + b * (1 - mask)  # Branchless\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It - Blocks and Grids (30 min)\n",
    "\n",
    "### Programming Model: Blocks and Grids\n",
    "\n",
    "When you launch a GPU kernel, you specify:\n",
    "- **Grid**: How many blocks to launch\n",
    "- **Block**: How many threads per block\n",
    "\n",
    "```\n",
    "Grid (your problem)\n",
    "├── Block 0 (e.g., 256 threads = 8 warps)\n",
    "│   ├── Warp 0: threads 0-31\n",
    "│   ├── Warp 1: threads 32-63\n",
    "│   └── ...\n",
    "├── Block 1\n",
    "└── Block N-1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kernel_config(problem_size, threads_per_block=256):\n",
    "    \"\"\"Calculate how to map a problem to GPU threads.\"\"\"\n",
    "    \n",
    "    # Number of blocks needed (round up)\n",
    "    num_blocks = (problem_size + threads_per_block - 1) // threads_per_block\n",
    "    \n",
    "    # Warps per block\n",
    "    warps_per_block = threads_per_block // 32\n",
    "    \n",
    "    # Total threads launched\n",
    "    total_threads = num_blocks * threads_per_block\n",
    "    \n",
    "    print(f\"Problem size: {problem_size:,} elements\")\n",
    "    print(f\"Threads per block: {threads_per_block}\")\n",
    "    print(f\"Warps per block: {warps_per_block}\")\n",
    "    print(f\"Number of blocks: {num_blocks:,}\")\n",
    "    print(f\"Total threads: {total_threads:,}\")\n",
    "    print(f\"Extra threads (wasted): {total_threads - problem_size}\")\n",
    "    \n",
    "    return num_blocks, threads_per_block\n",
    "\n",
    "# Example: Processing a 1M element vector\n",
    "calculate_kernel_config(1_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 2D problem (matrix)\n",
    "def calculate_2d_config(rows, cols, block_x=16, block_y=16):\n",
    "    \"\"\"Calculate grid configuration for 2D problem.\"\"\"\n",
    "    \n",
    "    grid_x = (cols + block_x - 1) // block_x\n",
    "    grid_y = (rows + block_y - 1) // block_y\n",
    "    \n",
    "    threads_per_block = block_x * block_y\n",
    "    total_blocks = grid_x * grid_y\n",
    "    total_threads = total_blocks * threads_per_block\n",
    "    \n",
    "    print(f\"Matrix size: {rows} x {cols} = {rows*cols:,} elements\")\n",
    "    print(f\"Block size: {block_x} x {block_y} = {threads_per_block} threads\")\n",
    "    print(f\"Grid size: {grid_x} x {grid_y} = {total_blocks:,} blocks\")\n",
    "    print(f\"Total threads: {total_threads:,}\")\n",
    "    \n",
    "    return (grid_x, grid_y), (block_x, block_y)\n",
    "\n",
    "# Example: 1024x1024 matrix\n",
    "calculate_2d_config(1024, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread Indexing\n",
    "\n",
    "Each thread knows its position:\n",
    "- `threadIdx.x`: Position within block (0 to blockDim.x-1)\n",
    "- `blockIdx.x`: Which block (0 to gridDim.x-1)\n",
    "- Global index: `blockIdx.x * blockDim.x + threadIdx.x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_thread_indexing(grid_dim, block_dim):\n",
    "    \"\"\"Simulate how threads calculate their global index.\"\"\"\n",
    "    \n",
    "    print(f\"Grid: {grid_dim} blocks, Block: {block_dim} threads\")\n",
    "    print()\n",
    "    \n",
    "    for block_idx in range(min(grid_dim, 3)):  # Show first 3 blocks\n",
    "        print(f\"Block {block_idx}:\")\n",
    "        for thread_idx in range(min(block_dim, 8)):  # Show first 8 threads\n",
    "            global_idx = block_idx * block_dim + thread_idx\n",
    "            print(f\"  threadIdx={thread_idx}, blockIdx={block_idx} -> global={global_idx}\")\n",
    "        if block_dim > 8:\n",
    "            print(f\"  ...\")\n",
    "        print()\n",
    "\n",
    "simulate_thread_indexing(grid_dim=4, block_dim=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupancy\n",
    "\n",
    "**Occupancy** = active warps / maximum warps per SM\n",
    "\n",
    "Higher occupancy often (but not always) means better performance.\n",
    "\n",
    "Occupancy is limited by:\n",
    "1. Registers per thread\n",
    "2. Shared memory per block\n",
    "3. Threads per block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_occupancy(threads_per_block, regs_per_thread, smem_per_block,\n",
    "                       max_warps_per_sm=64, max_regs_per_sm=65536, max_smem_per_sm=102400):\n",
    "    \"\"\"Calculate theoretical occupancy.\"\"\"\n",
    "    \n",
    "    warps_per_block = threads_per_block // 32\n",
    "    regs_per_block = threads_per_block * regs_per_thread\n",
    "    \n",
    "    # Blocks limited by warps\n",
    "    blocks_by_warps = max_warps_per_sm // warps_per_block\n",
    "    \n",
    "    # Blocks limited by registers\n",
    "    blocks_by_regs = max_regs_per_sm // regs_per_block if regs_per_block > 0 else 999\n",
    "    \n",
    "    # Blocks limited by shared memory\n",
    "    blocks_by_smem = max_smem_per_sm // smem_per_block if smem_per_block > 0 else 999\n",
    "    \n",
    "    # Actual blocks limited by most constrained resource\n",
    "    active_blocks = min(blocks_by_warps, blocks_by_regs, blocks_by_smem)\n",
    "    active_warps = active_blocks * warps_per_block\n",
    "    occupancy = active_warps / max_warps_per_sm * 100\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Threads/block: {threads_per_block}\")\n",
    "    print(f\"  Registers/thread: {regs_per_thread}\")\n",
    "    print(f\"  Shared memory/block: {smem_per_block} bytes\")\n",
    "    print()\n",
    "    print(f\"Limits:\")\n",
    "    print(f\"  By warps: {blocks_by_warps} blocks\")\n",
    "    print(f\"  By registers: {blocks_by_regs} blocks\")\n",
    "    print(f\"  By shared memory: {blocks_by_smem} blocks\")\n",
    "    print()\n",
    "    print(f\"Result:\")\n",
    "    print(f\"  Active blocks/SM: {active_blocks}\")\n",
    "    print(f\"  Active warps/SM: {active_warps}\")\n",
    "    print(f\"  Occupancy: {occupancy:.0f}%\")\n",
    "    \n",
    "    return occupancy\n",
    "\n",
    "# Example configurations\n",
    "print(\"=\" * 50)\n",
    "print(\"High occupancy config:\")\n",
    "print(\"=\" * 50)\n",
    "calculate_occupancy(256, 32, 0)\n",
    "\n",
    "print()\n",
    "print(\"=\" * 50)\n",
    "print(\"Register-limited config:\")\n",
    "print(\"=\" * 50)\n",
    "calculate_occupancy(256, 128, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify - Quiz (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: How many threads in a warp?\n",
    "print(\"Q1: A warp contains 32 threads\")\n",
    "print(\"    This is fixed for all NVIDIA GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: What's the global index formula?\n",
    "print(\"Q2: global_idx = blockIdx.x * blockDim.x + threadIdx.x\")\n",
    "print(\"\")\n",
    "print(\"For 2D:\")\n",
    "print(\"  row = blockIdx.y * blockDim.y + threadIdx.y\")\n",
    "print(\"  col = blockIdx.x * blockDim.x + threadIdx.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: What happens with divergence?\n",
    "print(\"Q3: When threads in a warp diverge (if-else):\")\n",
    "print(\"    Both paths execute serially\")\n",
    "print(\"    Inactive threads are masked\")\n",
    "print(\"    Throughput is reduced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: What can threads in the same block do?\n",
    "print(\"Q4: Threads in the same block can:\")\n",
    "print(\"    - Share data via shared memory\")\n",
    "print(\"    - Synchronize with __syncthreads()\")\n",
    "print(\"\")\n",
    "print(\"    Threads in different blocks CANNOT:\")\n",
    "print(\"    - Share data directly\")\n",
    "print(\"    - Synchronize with each other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Point |\n",
    "|---------|----------|\n",
    "| SM | Independent execution unit on GPU |\n",
    "| Warp | 32 threads executing in lockstep |\n",
    "| Block | Group of warps sharing resources |\n",
    "| Grid | All blocks for your kernel |\n",
    "| Divergence | Kills performance - avoid if possible |\n",
    "| Occupancy | More active warps = better latency hiding |\n",
    "\n",
    "### Interactive Resources\n",
    "\n",
    "For interactive visualizations of these concepts, see:\n",
    "- [GPU Architecture Lesson](../lessons/gpu-architecture.html) - Warp execution demo, occupancy calculator\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "Now that we understand GPU architecture, let's write our first kernel!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: Day 4 - Your First Triton Kernel\n",
    "\n",
    "Tomorrow we'll write vector addition in Triton and understand index arithmetic.\n",
    "\n",
    "[Continue to 04_first_triton_kernel.ipynb](./04_first_triton_kernel.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
