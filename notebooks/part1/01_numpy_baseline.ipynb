{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1, Day 1: NumPy Baseline\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Establish a CPU baseline for matrix multiplication and learn to measure performance.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Matrix multiplication is THE operation in deep learning. Today we'll:\n",
    "1. Implement matmul from scratch\n",
    "2. Compare against NumPy's optimized version\n",
    "3. Learn to measure GFLOPS (billions of floating-point operations per second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "**Question:** How fast can we multiply two 1024x1024 matrices?\n",
    "\n",
    "Let's start with a naive Python implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_naive(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Naive triple-loop matrix multiplication.\n",
    "    \n",
    "    C[i,j] = sum_k(A[i,k] * B[k,j])\n",
    "    \"\"\"\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "    assert K == K2, f\"Inner dimensions must match: {K} != {K2}\"\n",
    "    \n",
    "    C = np.zeros((M, N), dtype=A.dtype)\n",
    "    \n",
    "    for i in range(M):\n",
    "        for j in range(N):\n",
    "            for k in range(K):\n",
    "                C[i, j] += A[i, k] * B[k, j]\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test correctness on small matrices\n",
    "A_small = np.array([[1, 2], [3, 4]], dtype=np.float32)\n",
    "B_small = np.array([[5, 6], [7, 8]], dtype=np.float32)\n",
    "\n",
    "C_naive = matmul_naive(A_small, B_small)\n",
    "C_numpy = A_small @ B_small\n",
    "\n",
    "print(\"A:\")\n",
    "print(A_small)\n",
    "print(\"\\nB:\")\n",
    "print(B_small)\n",
    "print(\"\\nC = A @ B (naive):\")\n",
    "print(C_naive)\n",
    "print(\"\\nC = A @ B (numpy):\")\n",
    "print(C_numpy)\n",
    "print(f\"\\nResults match: {np.allclose(C_naive, C_numpy)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time the naive implementation on a small matrix\n",
    "# WARNING: This is SLOW. We use 64x64 to keep it reasonable.\n",
    "N = 64\n",
    "A = np.random.randn(N, N).astype(np.float32)\n",
    "B = np.random.randn(N, N).astype(np.float32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "C = matmul_naive(A, B)\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "print(f\"Matrix size: {N}x{N}\")\n",
    "print(f\"Time: {elapsed*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore - Measuring Performance (15 min)\n",
    "\n",
    "### What is GFLOPS?\n",
    "\n",
    "**FLOPS** = Floating-point Operations Per Second\n",
    "\n",
    "**GFLOPS** = Billions of FLOPS\n",
    "\n",
    "For matrix multiplication C = A @ B where A is MxK and B is KxN:\n",
    "- Each element of C requires K multiplications and K-1 additions\n",
    "- Total operations ≈ 2 * M * N * K (we count each multiply-add as 2 ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gflops(M: int, N: int, K: int, time_seconds: float) -> float:\n",
    "    \"\"\"Calculate GFLOPS for matrix multiplication.\n",
    "    \n",
    "    FLOPS = 2 * M * N * K (multiply-add counted as 2 ops)\n",
    "    \"\"\"\n",
    "    flops = 2 * M * N * K\n",
    "    gflops = flops / (time_seconds * 1e9)\n",
    "    return gflops\n",
    "\n",
    "# Calculate GFLOPS for our naive implementation\n",
    "gflops = calculate_gflops(N, N, N, elapsed)\n",
    "print(f\"Naive Python matmul: {gflops:.3f} GFLOPS\")\n",
    "print(f\"\\nFor reference:\")\n",
    "print(f\"  Modern CPU single core: ~50-100 GFLOPS\")\n",
    "print(f\"  NVIDIA H100 GPU: ~2000 TFLOPS (FP16 Tensor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is it so slow?\n",
    "\n",
    "Python's interpreter overhead dominates. Each loop iteration:\n",
    "1. Fetches the next bytecode instruction\n",
    "2. Interprets it\n",
    "3. Performs the operation\n",
    "\n",
    "For a 1024x1024 matmul, that's 1024^3 ≈ 1 billion loop iterations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept - NumPy's Secret (10 min)\n",
    "\n",
    "NumPy calls highly optimized libraries written in C/Fortran:\n",
    "- **BLAS** (Basic Linear Algebra Subprograms)\n",
    "- Often linked to **Intel MKL** or **OpenBLAS**\n",
    "\n",
    "These libraries:\n",
    "1. Use SIMD instructions (process multiple values per instruction)\n",
    "2. Optimize cache usage (tiling/blocking)\n",
    "3. Use multiple CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which BLAS NumPy is using\n",
    "np.show_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It - Benchmarking NumPy (30 min)\n",
    "\n",
    "Let's create a proper benchmarking function and measure NumPy's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(\n",
    "    matmul_fn,\n",
    "    M: int,\n",
    "    N: int,\n",
    "    K: int,\n",
    "    warmup: int = 2,\n",
    "    repeat: int = 5,\n",
    "    dtype=np.float32\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Benchmark a matrix multiplication function.\n",
    "    \n",
    "    Returns:\n",
    "        (mean_time_ms, gflops)\n",
    "    \"\"\"\n",
    "    A = np.random.randn(M, K).astype(dtype)\n",
    "    B = np.random.randn(K, N).astype(dtype)\n",
    "    \n",
    "    # Warmup runs (important for JIT compilation, cache warmup)\n",
    "    for _ in range(warmup):\n",
    "        _ = matmul_fn(A, B)\n",
    "    \n",
    "    # Timed runs\n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        start = time.perf_counter()\n",
    "        C = matmul_fn(A, B)\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    gflops = calculate_gflops(M, N, K, mean_time)\n",
    "    \n",
    "    return mean_time * 1000, gflops  # Convert to ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark NumPy for various sizes\n",
    "sizes = [128, 256, 512, 1024, 2048]\n",
    "\n",
    "print(f\"{'Size':>8} {'Time (ms)':>12} {'GFLOPS':>10}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "numpy_results = []\n",
    "for size in sizes:\n",
    "    time_ms, gflops = benchmark_matmul(np.matmul, size, size, size)\n",
    "    numpy_results.append((size, time_ms, gflops))\n",
    "    print(f\"{size:>8} {time_ms:>12.2f} {gflops:>10.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    sizes_arr = [r[0] for r in numpy_results]\n",
    "    times_arr = [r[1] for r in numpy_results]\n",
    "    gflops_arr = [r[2] for r in numpy_results]\n",
    "    \n",
    "    ax1.plot(sizes_arr, times_arr, 'o-', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Matrix Size (N)')\n",
    "    ax1.set_ylabel('Time (ms)')\n",
    "    ax1.set_title('NumPy Matmul Time')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(sizes_arr, gflops_arr, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    ax2.set_xlabel('Matrix Size (N)')\n",
    "    ax2.set_ylabel('GFLOPS')\n",
    "    ax2.set_title('NumPy Matmul Throughput')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"matplotlib not available, skipping visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Notice that:\n",
    "1. GFLOPS increases with matrix size (better cache utilization)\n",
    "2. NumPy achieves tens to hundreds of GFLOPS on CPU\n",
    "3. This is already ~1000x faster than naive Python!\n",
    "\n",
    "But we can do much better with a GPU..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify - Understanding the Math (10 min)\n",
    "\n",
    "### Quiz: Matrix Multiplication\n",
    "\n",
    "Test your understanding before moving to GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: What's the output shape of (100, 200) @ (200, 50)?\n",
    "A = np.random.randn(100, 200)\n",
    "B = np.random.randn(200, 50)\n",
    "C = A @ B\n",
    "print(f\"Q1: Shape of (100, 200) @ (200, 50) = {C.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: How many FLOPS in a 1000x1000 matmul?\n",
    "M, N, K = 1000, 1000, 1000\n",
    "flops = 2 * M * N * K\n",
    "print(f\"Q2: FLOPS in 1000x1000 matmul = {flops:,} = {flops/1e9:.1f} GFLOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: If NumPy achieves 100 GFLOPS, how long for 1000x1000?\n",
    "gflops_target = 100\n",
    "time_seconds = (flops / 1e9) / gflops_target\n",
    "print(f\"Q3: At 100 GFLOPS, 1000x1000 takes {time_seconds*1000:.1f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Element C[i,j] is computed from which elements?\n",
    "print(\"Q4: C[i,j] = sum over k of A[i,k] * B[k,j]\")\n",
    "print(\"    = dot product of row i of A with column j of B\")\n",
    "\n",
    "# Demonstrate\n",
    "i, j = 1, 0\n",
    "A_demo = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B_demo = np.array([[7, 8], [9, 10], [11, 12]])\n",
    "C_demo = A_demo @ B_demo\n",
    "\n",
    "print(f\"\\nA = \\n{A_demo}\")\n",
    "print(f\"\\nB = \\n{B_demo}\")\n",
    "print(f\"\\nC[{i},{j}] = A[{i},:] dot B[:,{j}]\")\n",
    "print(f\"        = {A_demo[i,:]} dot {B_demo[:,j]}\")\n",
    "print(f\"        = {A_demo[i,0]}*{B_demo[0,j]} + {A_demo[i,1]}*{B_demo[1,j]} + {A_demo[i,2]}*{B_demo[2,j]}\")\n",
    "print(f\"        = {A_demo[i,0]*B_demo[0,j]} + {A_demo[i,1]*B_demo[1,j]} + {A_demo[i,2]*B_demo[2,j]}\")\n",
    "print(f\"        = {C_demo[i,j]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| Implementation | Performance | Notes |\n",
    "|---------------|-------------|-------|\n",
    "| Naive Python | ~0.001 GFLOPS | Interpreter overhead |\n",
    "| NumPy (BLAS) | ~50-200 GFLOPS | Optimized C, SIMD, multi-core |\n",
    "| GPU (coming) | ~1000+ GFLOPS | Massive parallelism |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Matmul FLOPS** = 2 * M * N * K\n",
    "2. **GFLOPS** = FLOPS / (time * 10^9)\n",
    "3. NumPy is ~1000x faster than naive Python\n",
    "4. GPUs can be another 10-100x faster than NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: Day 2 - CuPy Introduction\n",
    "\n",
    "Tomorrow we'll see how CuPy achieves 10-100x speedup over NumPy with almost zero code changes.\n",
    "\n",
    "[Continue to 02_cupy_intro.ipynb](./02_cupy_intro.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
