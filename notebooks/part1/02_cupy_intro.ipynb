{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1, Day 2: CuPy - Instant GPU Speedup\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Get 50-100x speedup over NumPy with minimal code changes.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Yesterday we measured NumPy's CPU performance. Today:\n",
    "1. Run the same matmul on GPU using CuPy\n",
    "2. Understand data transfer overhead\n",
    "3. Learn when GPU acceleration helps (and when it doesn't)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    GPU_AVAILABLE = True\n",
    "    print(f\"CuPy version: {cp.__version__}\")\n",
    "    print(f\"GPU: {cp.cuda.runtime.getDeviceProperties(0)['name'].decode()}\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"CuPy not available. This notebook requires a GPU.\")\n",
    "    print(\"Try running on Google Colab with GPU runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "CuPy is a drop-in replacement for NumPy that runs on GPU.\n",
    "\n",
    "**The magic:** Change `np` to `cp` and your code runs on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # NumPy on CPU\n",
    "    A_cpu = np.random.randn(1024, 1024).astype(np.float32)\n",
    "    B_cpu = np.random.randn(1024, 1024).astype(np.float32)\n",
    "    \n",
    "    # CuPy on GPU - same API!\n",
    "    A_gpu = cp.random.randn(1024, 1024).astype(cp.float32)\n",
    "    B_gpu = cp.random.randn(1024, 1024).astype(cp.float32)\n",
    "    \n",
    "    # Matmul - identical syntax\n",
    "    C_cpu = A_cpu @ B_cpu\n",
    "    C_gpu = A_gpu @ B_gpu\n",
    "    \n",
    "    print(f\"CPU result shape: {C_cpu.shape}, type: {type(C_cpu)}\")\n",
    "    print(f\"GPU result shape: {C_gpu.shape}, type: {type(C_gpu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore - Speedup Measurement (15 min)\n",
    "\n",
    "### Important: GPU timing requires synchronization\n",
    "\n",
    "GPU operations are asynchronous. Without `cp.cuda.Stream.null.synchronize()`, \n",
    "you might measure the time to *launch* the kernel, not *execute* it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gflops(M, N, K, time_seconds):\n",
    "    flops = 2 * M * N * K\n",
    "    return flops / (time_seconds * 1e9)\n",
    "\n",
    "def benchmark_numpy(M, N, K, warmup=2, repeat=5):\n",
    "    A = np.random.randn(M, K).astype(np.float32)\n",
    "    B = np.random.randn(K, N).astype(np.float32)\n",
    "    \n",
    "    for _ in range(warmup):\n",
    "        _ = A @ B\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        start = time.perf_counter()\n",
    "        C = A @ B\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    return mean_time * 1000, calculate_gflops(M, N, K, mean_time)\n",
    "\n",
    "def benchmark_cupy(M, N, K, warmup=2, repeat=5):\n",
    "    A = cp.random.randn(M, K).astype(cp.float32)\n",
    "    B = cp.random.randn(K, N).astype(cp.float32)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        _ = A @ B\n",
    "        cp.cuda.Stream.null.synchronize()  # Wait for GPU!\n",
    "    \n",
    "    times = []\n",
    "    for _ in range(repeat):\n",
    "        cp.cuda.Stream.null.synchronize()  # Ensure previous ops done\n",
    "        start = time.perf_counter()\n",
    "        C = A @ B\n",
    "        cp.cuda.Stream.null.synchronize()  # Wait for this op\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    mean_time = np.mean(times)\n",
    "    return mean_time * 1000, calculate_gflops(M, N, K, mean_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    sizes = [256, 512, 1024, 2048, 4096]\n",
    "    \n",
    "    print(f\"{'Size':>6} {'NumPy (ms)':>12} {'CuPy (ms)':>12} {'Speedup':>10} {'GPU GFLOPS':>12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = []\n",
    "    for size in sizes:\n",
    "        np_time, np_gflops = benchmark_numpy(size, size, size)\n",
    "        cp_time, cp_gflops = benchmark_cupy(size, size, size)\n",
    "        speedup = np_time / cp_time\n",
    "        results.append((size, np_time, cp_time, speedup, cp_gflops))\n",
    "        print(f\"{size:>6} {np_time:>12.2f} {cp_time:>12.2f} {speedup:>10.1f}x {cp_gflops:>12.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "\n",
    "Notice how:\n",
    "1. Speedup increases with matrix size\n",
    "2. Small matrices may not benefit much (kernel launch overhead)\n",
    "3. GPU achieves hundreds or thousands of GFLOPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept - Why GPUs Are Fast (10 min)\n",
    "\n",
    "### CPU vs GPU Architecture\n",
    "\n",
    "| | CPU | GPU |\n",
    "|-|----|----|\n",
    "| Cores | 8-32 large cores | 1000s of small cores |\n",
    "| Strategy | Latency-optimized | Throughput-optimized |\n",
    "| Cache | Large (MB per core) | Small (KB per core) |\n",
    "| Good for | Sequential, branchy code | Parallel, uniform code |\n",
    "\n",
    "### Why Matmul is Perfect for GPUs\n",
    "\n",
    "1. **Massive parallelism**: Each output element can be computed independently\n",
    "2. **Regular memory access**: Predictable patterns enable coalescing\n",
    "3. **High arithmetic intensity**: Many operations per byte loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # How many parallel computations in a 4096x4096 matmul?\n",
    "    N = 4096\n",
    "    output_elements = N * N\n",
    "    print(f\"Matrix size: {N}x{N}\")\n",
    "    print(f\"Output elements (each computed in parallel): {output_elements:,}\")\n",
    "    print(f\"That's {output_elements / 1e6:.1f} million parallel tasks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It - Data Transfer Overhead (30 min)\n",
    "\n",
    "### The Hidden Cost: CPU-GPU Data Transfer\n",
    "\n",
    "Data must travel over PCIe, which is much slower than GPU memory bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    def benchmark_with_transfer(M, N, K, repeat=5):\n",
    "        \"\"\"Benchmark including CPU->GPU transfer time.\"\"\"\n",
    "        A_cpu = np.random.randn(M, K).astype(np.float32)\n",
    "        B_cpu = np.random.randn(K, N).astype(np.float32)\n",
    "        \n",
    "        times = []\n",
    "        for _ in range(repeat):\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            # Transfer to GPU\n",
    "            A_gpu = cp.asarray(A_cpu)\n",
    "            B_gpu = cp.asarray(B_cpu)\n",
    "            \n",
    "            # Compute\n",
    "            C_gpu = A_gpu @ B_gpu\n",
    "            \n",
    "            # Transfer back\n",
    "            C_cpu = cp.asnumpy(C_gpu)\n",
    "            \n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "            elapsed = time.perf_counter() - start\n",
    "            times.append(elapsed)\n",
    "        \n",
    "        return np.mean(times) * 1000\n",
    "    \n",
    "    # Compare with and without transfer\n",
    "    print(f\"{'Size':>6} {'No Transfer':>14} {'With Transfer':>14} {'Overhead':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for size in [1024, 2048, 4096]:\n",
    "        time_no_transfer, _ = benchmark_cupy(size, size, size)\n",
    "        time_with_transfer = benchmark_with_transfer(size, size, size)\n",
    "        overhead = (time_with_transfer - time_no_transfer) / time_no_transfer * 100\n",
    "        print(f\"{size:>6} {time_no_transfer:>12.2f}ms {time_with_transfer:>12.2f}ms {overhead:>9.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight: Keep Data on GPU\n",
    "\n",
    "For best performance:\n",
    "1. Transfer data to GPU once\n",
    "2. Do many operations on GPU\n",
    "3. Transfer results back once\n",
    "\n",
    "Avoid: Transfer -> Compute -> Transfer -> Compute -> Transfer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # Bad pattern: Transfer every iteration\n",
    "    def bad_pattern(A_cpu, B_cpu, iterations=10):\n",
    "        for _ in range(iterations):\n",
    "            A_gpu = cp.asarray(A_cpu)\n",
    "            B_gpu = cp.asarray(B_cpu)\n",
    "            C_gpu = A_gpu @ B_gpu\n",
    "            C_cpu = cp.asnumpy(C_gpu)\n",
    "        return C_cpu\n",
    "    \n",
    "    # Good pattern: Keep data on GPU\n",
    "    def good_pattern(A_cpu, B_cpu, iterations=10):\n",
    "        A_gpu = cp.asarray(A_cpu)\n",
    "        B_gpu = cp.asarray(B_cpu)\n",
    "        for _ in range(iterations):\n",
    "            C_gpu = A_gpu @ B_gpu\n",
    "        C_cpu = cp.asnumpy(C_gpu)\n",
    "        return C_cpu\n",
    "    \n",
    "    # Benchmark\n",
    "    size = 1024\n",
    "    A = np.random.randn(size, size).astype(np.float32)\n",
    "    B = np.random.randn(size, size).astype(np.float32)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    _ = bad_pattern(A, B)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    bad_time = time.perf_counter() - start\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    _ = good_pattern(A, B)\n",
    "    cp.cuda.Stream.null.synchronize()\n",
    "    good_time = time.perf_counter() - start\n",
    "    \n",
    "    print(f\"Bad pattern (transfer each iteration): {bad_time*1000:.1f}ms\")\n",
    "    print(f\"Good pattern (keep on GPU): {good_time*1000:.1f}ms\")\n",
    "    print(f\"Speedup: {bad_time/good_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify - When to Use GPU (10 min)\n",
    "\n",
    "### Quiz: GPU Suitability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # Q1: At what matrix size does GPU become faster?\n",
    "    print(\"Q1: Testing crossover point...\")\n",
    "    \n",
    "    for size in [32, 64, 128, 256, 512]:\n",
    "        np_time, _ = benchmark_numpy(size, size, size)\n",
    "        cp_time, _ = benchmark_cupy(size, size, size)\n",
    "        winner = \"GPU\" if cp_time < np_time else \"CPU\"\n",
    "        print(f\"  {size:>4}x{size}: CPU={np_time:.2f}ms, GPU={cp_time:.2f}ms -> {winner} wins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # Q2: What about element-wise operations?\n",
    "    print(\"\\nQ2: Element-wise operations (add)...\")\n",
    "    \n",
    "    for size in [1000, 10000, 100000, 1000000]:\n",
    "        A_np = np.random.randn(size).astype(np.float32)\n",
    "        B_np = np.random.randn(size).astype(np.float32)\n",
    "        A_cp = cp.asarray(A_np)\n",
    "        B_cp = cp.asarray(B_np)\n",
    "        \n",
    "        # NumPy\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            C = A_np + B_np\n",
    "        np_time = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        # CuPy\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            C = A_cp + B_cp\n",
    "        cp.cuda.Stream.null.synchronize()\n",
    "        cp_time = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        winner = \"GPU\" if cp_time < np_time else \"CPU\"\n",
    "        print(f\"  {size:>8} elements: CPU={np_time:.3f}ms, GPU={cp_time:.3f}ms -> {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary: When to Use GPU\n",
    "\n",
    "| Scenario | Use GPU? | Why |\n",
    "|----------|----------|-----|\n",
    "| Large matmul (1000+) | Yes | Massive parallelism |\n",
    "| Small matmul (<100) | No | Kernel launch overhead |\n",
    "| Many operations in sequence | Yes | Amortize transfer cost |\n",
    "| Single small operation | No | Transfer overhead dominates |\n",
    "| Element-wise, large data | Maybe | Depends on operation complexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "| What We Learned | Key Point |\n",
    "|-----------------|----------|\n",
    "| CuPy API | Drop-in NumPy replacement |\n",
    "| GPU speedup | 10-100x for large matrices |\n",
    "| Synchronization | Must sync for accurate timing |\n",
    "| Transfer overhead | Keep data on GPU when possible |\n",
    "| Crossover point | GPU wins for large parallel tasks |\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "CuPy is fast because it uses NVIDIA's cuBLAS library. But how does cuBLAS achieve such performance? To understand that, we need to learn how GPUs actually work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next: Day 3 - GPU Architecture\n",
    "\n",
    "Tomorrow we'll peek inside the GPU and understand SMs, warps, and the SIMT execution model.\n",
    "\n",
    "[Continue to 03_gpu_architecture.ipynb](./03_gpu_architecture.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
