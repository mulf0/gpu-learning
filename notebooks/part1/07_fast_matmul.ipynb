{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1, Day 7: Your First Fast Matmul\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Implement tiled matrix multiplication and achieve significant speedup.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "This is the culmination of Week 1. We'll combine everything:\n",
    "- GPU architecture understanding (Day 3)\n",
    "- Triton kernel programming (Day 4)\n",
    "- Memory hierarchy and coalescing (Day 5)\n",
    "- Tiling for data reuse (Day 6)\n",
    "\n",
    "**Target:** Achieve 500+ GFLOPS (getting into the range of real performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import triton\n",
    "    import triton.language as tl\n",
    "    GPU_AVAILABLE = True\n",
    "    print(f\"Triton: {triton.__version__}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "except ImportError:\n",
    "    GPU_AVAILABLE = False\n",
    "    print(\"GPU libraries not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "### Matrix Multiplication: C = A @ B\n",
    "\n",
    "```\n",
    "A: M x K\n",
    "B: K x N  \n",
    "C: M x N\n",
    "\n",
    "C[i,j] = sum(A[i,k] * B[k,j] for k in range(K))\n",
    "```\n",
    "\n",
    "### FLOPS Calculation\n",
    "\n",
    "- Each output element: K multiplications + K-1 additions ≈ 2K operations\n",
    "- Total elements: M × N\n",
    "- **Total FLOPS: 2 × M × N × K**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matmul_gflops(M, N, K, time_seconds):\n",
    "    \"\"\"Calculate GFLOPS for matmul.\"\"\"\n",
    "    flops = 2 * M * N * K\n",
    "    gflops = flops / (time_seconds * 1e9)\n",
    "    return gflops\n",
    "\n",
    "# Reference: What we're aiming for\n",
    "print(\"Performance targets:\")\n",
    "print(f\"  NumPy (CPU): ~50-100 GFLOPS\")\n",
    "print(f\"  Naive GPU kernel: ~100-500 GFLOPS\")\n",
    "print(f\"  Tiled GPU kernel: ~500-2000 GFLOPS\")\n",
    "print(f\"  cuBLAS/Triton autotuned: ~2000-5000 GFLOPS\")\n",
    "print(f\"  Peak H100 (FP32): ~67 TFLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore - The Tiled Algorithm (15 min)\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "```\n",
    "For each output tile (block_m, block_n):\n",
    "    Initialize accumulator = zeros(TILE_M, TILE_N)\n",
    "    \n",
    "    For k_tile in range(0, K, TILE_K):\n",
    "        # Load tiles from global memory\n",
    "        tile_a = A[block_m*TILE_M : (block_m+1)*TILE_M, \n",
    "                   k_tile : k_tile+TILE_K]\n",
    "        tile_b = B[k_tile : k_tile+TILE_K,\n",
    "                   block_n*TILE_N : (block_n+1)*TILE_N]\n",
    "        \n",
    "        # Compute partial product (in registers/shared memory)\n",
    "        accumulator += tile_a @ tile_b\n",
    "    \n",
    "    # Write result to global memory\n",
    "    C[block_m*TILE_M : ..., block_n*TILE_N : ...] = accumulator\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tiled algorithm\n",
    "def visualize_tiled_matmul(M, N, K, TILE_M, TILE_N, TILE_K):\n",
    "    \"\"\"Show how tiled matmul processes data.\"\"\"\n",
    "    \n",
    "    num_tiles_m = (M + TILE_M - 1) // TILE_M\n",
    "    num_tiles_n = (N + TILE_N - 1) // TILE_N\n",
    "    num_tiles_k = (K + TILE_K - 1) // TILE_K\n",
    "    \n",
    "    print(f\"Matrix dimensions: A={M}x{K}, B={K}x{N}, C={M}x{N}\")\n",
    "    print(f\"Tile sizes: TILE_M={TILE_M}, TILE_N={TILE_N}, TILE_K={TILE_K}\")\n",
    "    print()\n",
    "    print(f\"Grid of output tiles: {num_tiles_m} x {num_tiles_n} = {num_tiles_m * num_tiles_n} tiles\")\n",
    "    print(f\"K-dimension tiles: {num_tiles_k} (loop iterations per output tile)\")\n",
    "    print()\n",
    "    print(\"For each output tile, we iterate {num_tiles_k} times:\")\n",
    "    print(\"  - Load a {TILE_M}x{TILE_K} tile of A\")\n",
    "    print(\"  - Load a {TILE_K}x{TILE_N} tile of B\")\n",
    "    print(\"  - Compute partial matmul and accumulate\")\n",
    "    print()\n",
    "    \n",
    "    # Data reuse calculation\n",
    "    a_loads_per_element = num_tiles_n  # Each A tile loaded for each column of C tiles\n",
    "    b_loads_per_element = num_tiles_m  # Each B tile loaded for each row of C tiles\n",
    "    \n",
    "    print(f\"Data reuse:\")\n",
    "    print(f\"  Each A tile reused {num_tiles_n} times (once per column of C tiles)\")\n",
    "    print(f\"  Each B tile reused {num_tiles_m} times (once per row of C tiles)\")\n",
    "\n",
    "visualize_tiled_matmul(1024, 1024, 1024, 64, 64, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept - Triton's Block-Level Programming (10 min)\n",
    "\n",
    "### Key Triton Operations for Matmul\n",
    "\n",
    "```python\n",
    "# 1. Load 2D tiles\n",
    "tile_a = tl.load(a_ptr + offsets, mask=mask)\n",
    "\n",
    "# 2. Matrix multiplication on tiles  \n",
    "# Triton has a built-in dot product:\n",
    "result = tl.dot(tile_a, tile_b)  # Efficient matmul!\n",
    "\n",
    "# 3. Accumulate\n",
    "accumulator += result\n",
    "```\n",
    "\n",
    "`tl.dot()` is the key - Triton compiles this to efficient Tensor Core operations on compatible hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It - Tiled Matmul Kernel (30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    @triton.jit\n",
    "    def matmul_kernel(\n",
    "        # Pointers to matrices\n",
    "        a_ptr, b_ptr, c_ptr,\n",
    "        # Matrix dimensions\n",
    "        M, N, K,\n",
    "        # Strides (elements to skip to go to next row)\n",
    "        stride_am, stride_ak,\n",
    "        stride_bk, stride_bn,\n",
    "        stride_cm, stride_cn,\n",
    "        # Tile sizes (must be compile-time constants)\n",
    "        TILE_M: tl.constexpr,\n",
    "        TILE_N: tl.constexpr,\n",
    "        TILE_K: tl.constexpr,\n",
    "    ):\n",
    "        \"\"\"Tiled matrix multiplication: C = A @ B\"\"\"\n",
    "        \n",
    "        # ===== Step 1: Identify which tile this program computes =====\n",
    "        pid_m = tl.program_id(0)  # Row tile index\n",
    "        pid_n = tl.program_id(1)  # Column tile index\n",
    "        \n",
    "        # ===== Step 2: Compute base offsets for this tile =====\n",
    "        # Rows of A and C that this tile handles\n",
    "        rm = pid_m * TILE_M + tl.arange(0, TILE_M)\n",
    "        # Columns of B and C that this tile handles\n",
    "        cn = pid_n * TILE_N + tl.arange(0, TILE_N)\n",
    "        \n",
    "        # ===== Step 3: Initialize accumulator =====\n",
    "        # This holds the partial results for this output tile\n",
    "        acc = tl.zeros((TILE_M, TILE_N), dtype=tl.float32)\n",
    "        \n",
    "        # ===== Step 4: Loop over K dimension in tiles =====\n",
    "        for k_start in range(0, K, TILE_K):\n",
    "            # K indices for this iteration\n",
    "            rk = k_start + tl.arange(0, TILE_K)\n",
    "            \n",
    "            # ----- Load tile of A -----\n",
    "            # A[rm, rk] - need 2D indexing\n",
    "            # Offsets: rm[:, None] * stride_am + rk[None, :] * stride_ak\n",
    "            a_offsets = rm[:, None] * stride_am + rk[None, :] * stride_ak\n",
    "            a_mask = (rm[:, None] < M) & (rk[None, :] < K)\n",
    "            a = tl.load(a_ptr + a_offsets, mask=a_mask, other=0.0)\n",
    "            \n",
    "            # ----- Load tile of B -----\n",
    "            # B[rk, cn]\n",
    "            b_offsets = rk[:, None] * stride_bk + cn[None, :] * stride_bn\n",
    "            b_mask = (rk[:, None] < K) & (cn[None, :] < N)\n",
    "            b = tl.load(b_ptr + b_offsets, mask=b_mask, other=0.0)\n",
    "            \n",
    "            # ----- Compute partial matmul -----\n",
    "            # This is the magic: tl.dot compiles to efficient matmul\n",
    "            acc += tl.dot(a, b)\n",
    "        \n",
    "        # ===== Step 5: Store result =====\n",
    "        c_offsets = rm[:, None] * stride_cm + cn[None, :] * stride_cn\n",
    "        c_mask = (rm[:, None] < M) & (cn[None, :] < N)\n",
    "        tl.store(c_ptr + c_offsets, acc, mask=c_mask)\n",
    "    \n",
    "    print(\"Matmul kernel compiled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    def matmul_triton(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Matrix multiplication using our Triton kernel.\"\"\"\n",
    "        \n",
    "        assert a.is_cuda and b.is_cuda\n",
    "        assert a.shape[1] == b.shape[0], f\"Shape mismatch: {a.shape} x {b.shape}\"\n",
    "        \n",
    "        M, K = a.shape\n",
    "        K, N = b.shape\n",
    "        \n",
    "        c = torch.empty((M, N), device=a.device, dtype=a.dtype)\n",
    "        \n",
    "        # Tile sizes (tune these for your GPU!)\n",
    "        TILE_M, TILE_N, TILE_K = 64, 64, 32\n",
    "        \n",
    "        # Grid: one program per output tile\n",
    "        grid = (\n",
    "            triton.cdiv(M, TILE_M),\n",
    "            triton.cdiv(N, TILE_N),\n",
    "        )\n",
    "        \n",
    "        matmul_kernel[grid](\n",
    "            a, b, c,\n",
    "            M, N, K,\n",
    "            a.stride(0), a.stride(1),\n",
    "            b.stride(0), b.stride(1),\n",
    "            c.stride(0), c.stride(1),\n",
    "            TILE_M=TILE_M,\n",
    "            TILE_N=TILE_N,\n",
    "            TILE_K=TILE_K,\n",
    "        )\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    print(\"Wrapper function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # Test correctness\n",
    "    print(\"Testing correctness...\")\n",
    "    \n",
    "    M, N, K = 512, 512, 512\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float32)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    # Our kernel\n",
    "    c_triton = matmul_triton(a, b)\n",
    "    \n",
    "    # Reference (PyTorch)\n",
    "    c_torch = a @ b\n",
    "    \n",
    "    # Check\n",
    "    max_diff = (c_triton - c_torch).abs().max().item()\n",
    "    print(f\"Max difference: {max_diff:.6f}\")\n",
    "    print(f\"Relative error: {max_diff / c_torch.abs().mean().item():.6f}\")\n",
    "    \n",
    "    # Allow small numerical differences due to float32\n",
    "    is_correct = torch.allclose(c_triton, c_torch, rtol=1e-3, atol=1e-3)\n",
    "    print(f\"Results match: {is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if GPU_AVAILABLE:\n",
    "    # Benchmark\n",
    "    def benchmark_matmul(fn, M, N, K, warmup=10, repeat=100):\n",
    "        \"\"\"Benchmark a matmul function.\"\"\"\n",
    "        a = torch.randn(M, K, device='cuda', dtype=torch.float32)\n",
    "        b = torch.randn(K, N, device='cuda', dtype=torch.float32)\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(warmup):\n",
    "            _ = fn(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(repeat):\n",
    "            c = fn(a, b)\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = (time.perf_counter() - start) / repeat\n",
    "        \n",
    "        gflops = calculate_matmul_gflops(M, N, K, elapsed)\n",
    "        return elapsed * 1000, gflops\n",
    "    \n",
    "    print(\"Benchmarking matmul implementations...\")\n",
    "    print()\n",
    "    print(f\"{'Size':>12} {'Triton (ms)':>12} {'Triton GFLOPS':>14} {'PyTorch (ms)':>13} {'Speedup':>8}\")\n",
    "    print(\"-\" * 65)\n",
    "    \n",
    "    for size in [512, 1024, 2048, 4096]:\n",
    "        triton_time, triton_gflops = benchmark_matmul(matmul_triton, size, size, size)\n",
    "        torch_time, torch_gflops = benchmark_matmul(lambda a, b: a @ b, size, size, size)\n",
    "        \n",
    "        speedup = torch_time / triton_time\n",
    "        print(f\"{size}x{size}:  {triton_time:>10.2f}  {triton_gflops:>14.0f}  {torch_time:>12.2f}  {speedup:>7.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "Your kernel likely achieves:\n",
    "- ~500-2000 GFLOPS depending on GPU and tile sizes\n",
    "- Competitive with PyTorch for medium-sized matrices\n",
    "- PyTorch may be faster due to more tuning\n",
    "\n",
    "**To go faster, we would need:**\n",
    "- Better tile sizes (autotuning)\n",
    "- Tensor Cores (requires FP16/BF16)\n",
    "- More sophisticated memory access patterns\n",
    "- Pipelining (overlap load and compute)\n",
    "\n",
    "These are topics for Week 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify - Understanding Check (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1: Why do we loop over K in tiles?\n",
    "print(\"Q1: Loop over K dimension in tiles because:\")\n",
    "print(\"    - Full K dimension doesn't fit in shared memory/registers\")\n",
    "print(\"    - Each iteration loads small tiles of A and B\")\n",
    "print(\"    - Partial products are accumulated in fast registers\")\n",
    "print(\"    - Result: global memory traffic is reduced by factor of TILE_SIZE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: What does tl.dot() do?\n",
    "print(\"Q2: tl.dot(a, b) computes tile-level matrix multiplication\")\n",
    "print(\"    - Input: two 2D tile tensors\")\n",
    "print(\"    - Output: matmul result\")\n",
    "print(\"    - On modern GPUs: compiles to Tensor Core instructions\")\n",
    "print(\"    - This is where most of the FLOPs happen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Why use 2D grid?\n",
    "print(\"Q3: 2D grid maps naturally to output matrix:\")\n",
    "print(\"    - pid_m = row tile index\")\n",
    "print(\"    - pid_n = column tile index\")\n",
    "print(\"    - Each program computes one TILE_M x TILE_N output tile\")\n",
    "print(\"    - Total programs = (M/TILE_M) x (N/TILE_N)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Where does data reuse happen?\n",
    "print(\"Q4: Data reuse in the K-loop:\")\n",
    "print(\"    - Each tile of A is loaded once, used TILE_N times\")\n",
    "print(\"    - Each tile of B is loaded once, used TILE_M times\")\n",
    "print(\"    - Reuse happens within the tl.dot() operation\")\n",
    "print(\"    - Accumulated result stays in registers until loop ends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Week 1 Summary\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "| Day | Topic | Key Takeaway |\n",
    "|-----|-------|-------------|\n",
    "| 1 | NumPy Baseline | GFLOPS measurement, CPU performance |\n",
    "| 2 | CuPy | 50-100x speedup with GPU |\n",
    "| 3 | GPU Architecture | SMs, warps, SIMT model |\n",
    "| 4 | First Kernel | Index arithmetic, program IDs |\n",
    "| 5 | Memory Hierarchy | Coalescing, arithmetic intensity |\n",
    "| 6 | Tiling | Data reuse, shared memory |\n",
    "| 7 | Fast Matmul | Putting it all together |\n",
    "\n",
    "### Performance Journey\n",
    "\n",
    "| Implementation | GFLOPS | Improvement |\n",
    "|---------------|--------|------------|\n",
    "| Naive Python | 0.001 | Baseline |\n",
    "| NumPy | 50-100 | 50,000x |\n",
    "| CuPy | 500-2000 | 10-20x over NumPy |\n",
    "| Our Triton kernel | 500-2000 | Competitive! |\n",
    "\n",
    "### What's Next (Week 2)\n",
    "\n",
    "- **Profiling** - Use Nsight Compute to find bottlenecks\n",
    "- **Autotuning** - Find optimal tile sizes automatically\n",
    "- **Tensor Cores** - FP16/BF16 for 2-4x more FLOPS\n",
    "- **Memory optimizations** - Bank conflicts, async copies\n",
    "- **Target:** 80%+ of theoretical peak performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've written your first fast GPU kernel from scratch! You now understand:\n",
    "- Why GPUs are fast (massive parallelism)\n",
    "- How to think about GPU programming (tiles, blocks, warps)\n",
    "- Why memory matters more than compute for most kernels\n",
    "- How to implement tiled algorithms for data reuse\n",
    "\n",
    "This foundation will serve you throughout your GPU programming journey.\n",
    "\n",
    "**Next:** Week 2 - The Memory Game (optimization deep dive)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
