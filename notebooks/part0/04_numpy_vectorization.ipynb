{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0.4: NumPy Vectorization as GPU Training\n",
    "\n",
    "**Chapter 0: The Parallel Mindset**\n",
    "\n",
    "NumPy's vectorized operations are excellent training for GPU thinking. This lab bridges the gap.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand why vectorized code is faster than loops\n",
    "- Practice eliminating loops with array operations\n",
    "- See how NumPy thinking translates to GPU kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loops vs Vectorization\n",
    "\n",
    "Python loops are slow. NumPy operations are fast. Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test data\n",
    "n = 1_000_000\n",
    "a = np.random.rand(n)\n",
    "b = np.random.rand(n)\n",
    "\n",
    "# Method 1: Python loop (SLOW)\n",
    "def add_loop(a, b):\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(len(a)):\n",
    "        result[i] = a[i] + b[i]\n",
    "    return result\n",
    "\n",
    "# Method 2: NumPy vectorized (FAST)\n",
    "def add_vectorized(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "result_loop = add_loop(a, b)\n",
    "loop_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "result_vec = add_vectorized(a, b)\n",
    "vec_time = time.time() - start\n",
    "\n",
    "print(f\"Loop time: {loop_time:.3f}s\")\n",
    "print(f\"Vectorized time: {vec_time:.6f}s\")\n",
    "print(f\"Speedup: {loop_time / vec_time:.0f}x\")\n",
    "print(f\"\\nResults match: {np.allclose(result_loop, result_vec)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is vectorization faster?\n",
    "\n",
    "1. **No Python overhead per element**: Loop overhead, type checking, etc.\n",
    "2. **SIMD instructions**: CPU processes multiple elements per instruction\n",
    "3. **Better cache usage**: Predictable memory access patterns\n",
    "4. **Compiled C code**: NumPy operations run optimized C, not Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Common Vectorization Patterns\n",
    "\n",
    "Learn to recognize and apply these patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 1: Element-wise operations\n",
    "x = np.random.rand(1000)\n",
    "\n",
    "# Bad (loop)\n",
    "def sigmoid_loop(x):\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        result[i] = 1 / (1 + np.exp(-x[i]))\n",
    "    return result\n",
    "\n",
    "# Good (vectorized)\n",
    "def sigmoid_vec(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "print(\"Pattern 1: Element-wise\")\n",
    "print(f\"Results match: {np.allclose(sigmoid_loop(x), sigmoid_vec(x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 2: Conditional operations with np.where\n",
    "x = np.random.randn(1000)\n",
    "\n",
    "# Bad (loop with if)\n",
    "def relu_loop(x):\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(len(x)):\n",
    "        if x[i] > 0:\n",
    "            result[i] = x[i]\n",
    "        else:\n",
    "            result[i] = 0\n",
    "    return result\n",
    "\n",
    "# Good (vectorized)\n",
    "def relu_vec(x):\n",
    "    return np.maximum(0, x)\n",
    "    # Or: return np.where(x > 0, x, 0)\n",
    "\n",
    "print(\"Pattern 2: Conditionals -> np.where / np.maximum\")\n",
    "print(f\"Results match: {np.allclose(relu_loop(x), relu_vec(x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 3: Reductions\n",
    "x = np.random.rand(1000)\n",
    "\n",
    "# Bad (loop)\n",
    "def mean_loop(x):\n",
    "    total = 0\n",
    "    for val in x:\n",
    "        total += val\n",
    "    return total / len(x)\n",
    "\n",
    "# Good (vectorized)\n",
    "def mean_vec(x):\n",
    "    return x.mean()  # or np.mean(x)\n",
    "\n",
    "print(\"Pattern 3: Reductions -> .sum(), .mean(), .max(), etc.\")\n",
    "print(f\"Results match: {np.isclose(mean_loop(x), mean_vec(x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern 4: Broadcasting\n",
    "# Add a bias to each row of a matrix\n",
    "X = np.random.rand(100, 50)  # 100 samples, 50 features\n",
    "bias = np.random.rand(50)    # 50-element bias vector\n",
    "\n",
    "# Bad (loop)\n",
    "def add_bias_loop(X, bias):\n",
    "    result = np.empty_like(X)\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            result[i, j] = X[i, j] + bias[j]\n",
    "    return result\n",
    "\n",
    "# Good (broadcasting)\n",
    "def add_bias_vec(X, bias):\n",
    "    return X + bias  # bias broadcasts across rows\n",
    "\n",
    "print(\"Pattern 4: Broadcasting\")\n",
    "print(f\"X shape: {X.shape}, bias shape: {bias.shape}\")\n",
    "print(f\"Result shape: {add_bias_vec(X, bias).shape}\")\n",
    "print(f\"Results match: {np.allclose(add_bias_loop(X, bias), add_bias_vec(X, bias))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: From NumPy to GPU Thinking\n",
    "\n",
    "NumPy vectorization teaches the right mental model for GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy way: Think about the whole array operation\n",
    "def softmax_numpy(x):\n",
    "    \"\"\"Compute softmax along last axis.\"\"\"\n",
    "    # Shift for numerical stability\n",
    "    x_shifted = x - x.max(axis=-1, keepdims=True)\n",
    "    # Exponentiate\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    # Normalize\n",
    "    return exp_x / exp_x.sum(axis=-1, keepdims=True)\n",
    "\n",
    "# GPU kernel way: Think about what ONE thread does\n",
    "# (Pseudocode - actual Triton kernel would look similar)\n",
    "def softmax_gpu_pseudocode():\n",
    "    \"\"\"\n",
    "    # Each thread block handles one row\n",
    "    row = program_id  # Which row am I processing?\n",
    "    \n",
    "    # Load row into shared memory\n",
    "    x = load(input[row, :])\n",
    "    \n",
    "    # Find max (reduction within block)\n",
    "    max_val = block_reduce_max(x)\n",
    "    \n",
    "    # Shift and exponentiate\n",
    "    exp_x = exp(x - max_val)\n",
    "    \n",
    "    # Sum (reduction within block)\n",
    "    sum_exp = block_reduce_sum(exp_x)\n",
    "    \n",
    "    # Normalize and store\n",
    "    output[row, :] = exp_x / sum_exp\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Test softmax\n",
    "x = np.random.randn(32, 128)  # Batch of 32, vocab size 128\n",
    "result = softmax_numpy(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {result.shape}\")\n",
    "print(f\"Each row sums to 1: {np.allclose(result.sum(axis=-1), 1)}\")\n",
    "print(f\"\\nNumPy and GPU kernel do the same operations,\")\n",
    "print(f\"just organized differently (whole array vs per-thread).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Vectorize These Functions\n",
    "\n",
    "Convert the loop-based implementations to vectorized NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: L2 Normalization\n",
    "def l2_normalize_loop(x):\n",
    "    \"\"\"Normalize each row to unit length.\"\"\"\n",
    "    result = np.empty_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        norm = 0\n",
    "        for j in range(x.shape[1]):\n",
    "            norm += x[i, j] ** 2\n",
    "        norm = np.sqrt(norm)\n",
    "        for j in range(x.shape[1]):\n",
    "            result[i, j] = x[i, j] / norm\n",
    "    return result\n",
    "\n",
    "# Your vectorized solution:\n",
    "def l2_normalize_vec(x):\n",
    "    # TODO: Implement without loops\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "def l2_normalize_vec(x):\n",
    "    norm = np.linalg.norm(x, axis=-1, keepdims=True)\n",
    "    return x / norm\n",
    "\n",
    "# Test\n",
    "x = np.random.randn(100, 50)\n",
    "print(f\"Results match: {np.allclose(l2_normalize_loop(x), l2_normalize_vec(x))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Pairwise distances\n",
    "def pairwise_dist_loop(X, Y):\n",
    "    \"\"\"Compute distance between each pair of points.\"\"\"\n",
    "    m, n = X.shape[0], Y.shape[0]\n",
    "    result = np.empty((m, n))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            diff = X[i] - Y[j]\n",
    "            result[i, j] = np.sqrt(np.sum(diff ** 2))\n",
    "    return result\n",
    "\n",
    "# Your vectorized solution:\n",
    "def pairwise_dist_vec(X, Y):\n",
    "    # TODO: Implement without loops\n",
    "    # Hint: Use broadcasting and the identity ||a-b||^2 = ||a||^2 + ||b||^2 - 2*a.b\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "def pairwise_dist_vec(X, Y):\n",
    "    # ||X - Y||^2 = ||X||^2 + ||Y||^2 - 2 * X @ Y.T\n",
    "    X_sq = np.sum(X ** 2, axis=1, keepdims=True)  # (m, 1)\n",
    "    Y_sq = np.sum(Y ** 2, axis=1, keepdims=True)  # (n, 1)\n",
    "    cross = X @ Y.T                                # (m, n)\n",
    "    dist_sq = X_sq + Y_sq.T - 2 * cross\n",
    "    return np.sqrt(np.maximum(dist_sq, 0))  # Clip for numerical stability\n",
    "\n",
    "# Test\n",
    "X = np.random.randn(50, 10)\n",
    "Y = np.random.randn(30, 10)\n",
    "print(f\"Results match: {np.allclose(pairwise_dist_loop(X, Y), pairwise_dist_vec(X, Y))}\")\n",
    "\n",
    "# Benchmark\n",
    "start = time.time()\n",
    "_ = pairwise_dist_loop(X, Y)\n",
    "loop_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = pairwise_dist_vec(X, Y)\n",
    "vec_time = time.time() - start\n",
    "\n",
    "print(f\"Speedup: {loop_time / vec_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Vectorization eliminates Python overhead**: 10-100x speedup is common\n",
    "2. **Think in arrays, not elements**: \"What happens to all elements?\" not \"What happens to element i?\"\n",
    "3. **Broadcasting is powerful**: Avoid explicit loops for dimension-expanding ops\n",
    "4. **NumPy -> GPU**: Same mental model, just replace array ops with kernel launches\n",
    "5. **Practice recognizing patterns**: Element-wise, reduction, broadcasting - they all have GPU equivalents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
