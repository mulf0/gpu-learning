{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 0.3: Parallel Patterns\n",
    "\n",
    "**Chapter 0: The Parallel Mindset**\n",
    "\n",
    "Learn to recognize the four fundamental parallel patterns and how they map to GPU programming.\n",
    "\n",
    "## Learning Objectives\n",
    "- Identify embarrassingly parallel, reduction, stencil, and irregular patterns\n",
    "- Understand why pattern recognition determines optimization strategy\n",
    "- Practice classifying real-world operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 1: Embarrassingly Parallel\n",
    "\n",
    "**Definition**: Each output element depends only on its corresponding input. No communication needed.\n",
    "\n",
    "**GPU sweet spot**: Maps perfectly to SIMT - one thread per element.\n",
    "\n",
    "**Examples**: Element-wise ops, activation functions, pixel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embarrassingly_parallel_examples(x):\n",
    "    \"\"\"All of these are embarrassingly parallel.\"\"\"\n",
    "    \n",
    "    # Element-wise math\n",
    "    y1 = x * 2 + 1\n",
    "    y2 = np.sin(x)\n",
    "    y3 = np.exp(-x)\n",
    "    \n",
    "    # Activation functions\n",
    "    relu = np.maximum(0, x)\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    tanh = np.tanh(x)\n",
    "    \n",
    "    # Comparisons\n",
    "    mask = x > 0\n",
    "    \n",
    "    return y1, relu, sigmoid\n",
    "\n",
    "x = np.random.randn(1000000)\n",
    "results = embarrassingly_parallel_examples(x)\n",
    "\n",
    "print(\"Embarrassingly parallel operations completed.\")\n",
    "print(\"Each output[i] depends ONLY on input[i] - perfect for GPU!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 2: Reduction\n",
    "\n",
    "**Definition**: Combine many values into one (or few) using an associative operation.\n",
    "\n",
    "**GPU approach**: Parallel tree reduction - O(log N) steps instead of O(N).\n",
    "\n",
    "**Examples**: Sum, max, min, mean, dot product, softmax denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction_examples(x):\n",
    "    \"\"\"All of these are reductions.\"\"\"\n",
    "    \n",
    "    # Basic reductions\n",
    "    total = np.sum(x)\n",
    "    maximum = np.max(x)\n",
    "    minimum = np.min(x)\n",
    "    mean = np.mean(x)\n",
    "    \n",
    "    # More complex reductions\n",
    "    norm = np.linalg.norm(x)  # sqrt(sum(x^2))\n",
    "    argmax = np.argmax(x)     # index of max\n",
    "    \n",
    "    return total, maximum, mean, norm\n",
    "\n",
    "x = np.random.randn(1000000)\n",
    "results = reduction_examples(x)\n",
    "\n",
    "print(f\"Sum: {results[0]:.4f}\")\n",
    "print(f\"Max: {results[1]:.4f}\")\n",
    "print(f\"Mean: {results[2]:.4f}\")\n",
    "print(f\"\\nReductions combine many values into one.\")\n",
    "print(\"GPU uses tree reduction: 1M elements -> 20 steps (log2(1M) ~ 20)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tree reduction\n",
    "def tree_reduction_demo(arr):\n",
    "    \"\"\"Demonstrate tree reduction for sum.\"\"\"\n",
    "    print(f\"Input: {arr}\")\n",
    "    \n",
    "    current = arr.copy()\n",
    "    step = 0\n",
    "    \n",
    "    while len(current) > 1:\n",
    "        step += 1\n",
    "        # Pair up adjacent elements and sum\n",
    "        new_len = (len(current) + 1) // 2\n",
    "        new_arr = np.zeros(new_len)\n",
    "        \n",
    "        for i in range(new_len):\n",
    "            if 2*i + 1 < len(current):\n",
    "                new_arr[i] = current[2*i] + current[2*i + 1]\n",
    "            else:\n",
    "                new_arr[i] = current[2*i]\n",
    "        \n",
    "        current = new_arr\n",
    "        print(f\"Step {step}: {current}\")\n",
    "    \n",
    "    return current[0]\n",
    "\n",
    "arr = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "result = tree_reduction_demo(arr)\n",
    "print(f\"\\nFinal sum: {result} (expected: {arr.sum()})\")\n",
    "print(f\"\\nOnly 3 steps for 8 elements (log2(8) = 3)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 3: Stencil / Neighbor\n",
    "\n",
    "**Definition**: Each output depends on nearby inputs in a fixed pattern.\n",
    "\n",
    "**GPU approach**: Load neighborhood into shared memory, compute output.\n",
    "\n",
    "**Examples**: Convolution, blur, edge detection, finite differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stencil_examples(x):\n",
    "    \"\"\"All of these are stencil operations.\"\"\"\n",
    "    \n",
    "    # 1D stencil: moving average\n",
    "    kernel = np.array([1, 1, 1]) / 3\n",
    "    moving_avg = np.convolve(x, kernel, mode='same')\n",
    "    \n",
    "    # 1D stencil: finite difference (derivative approximation)\n",
    "    derivative = np.zeros_like(x)\n",
    "    derivative[1:-1] = (x[2:] - x[:-2]) / 2  # Central difference\n",
    "    \n",
    "    # 1D stencil: Laplacian\n",
    "    laplacian = np.zeros_like(x)\n",
    "    laplacian[1:-1] = x[:-2] - 2*x[1:-1] + x[2:]\n",
    "    \n",
    "    return moving_avg, derivative, laplacian\n",
    "\n",
    "x = np.sin(np.linspace(0, 4*np.pi, 100))\n",
    "results = stencil_examples(x)\n",
    "\n",
    "print(\"Stencil operations completed.\")\n",
    "print(\"Each output[i] depends on input[i-1], input[i], input[i+1], etc.\")\n",
    "print(\"\\nGPU optimization: load neighborhood into fast shared memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D stencil example: image blur\n",
    "def blur_2d(image, kernel_size=3):\n",
    "    \"\"\"Simple box blur - a 2D stencil operation.\"\"\"\n",
    "    h, w = image.shape\n",
    "    k = kernel_size // 2\n",
    "    output = np.zeros_like(image)\n",
    "    \n",
    "    for i in range(k, h-k):\n",
    "        for j in range(k, w-k):\n",
    "            # Each output pixel is the average of its neighborhood\n",
    "            neighborhood = image[i-k:i+k+1, j-k:j+k+1]\n",
    "            output[i, j] = neighborhood.mean()\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Create a simple test image\n",
    "image = np.random.rand(100, 100)\n",
    "blurred = blur_2d(image)\n",
    "\n",
    "print(f\"Original image shape: {image.shape}\")\n",
    "print(f\"Blurred image shape: {blurred.shape}\")\n",
    "print(\"\\nEach output pixel depends on a 3x3 neighborhood of input pixels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern 4: Irregular / Sparse\n",
    "\n",
    "**Definition**: Unpredictable access patterns, data-dependent control flow.\n",
    "\n",
    "**GPU challenge**: Poor memory coalescing, thread divergence.\n",
    "\n",
    "**Examples**: Graph traversal, sparse matrix ops, tree algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def irregular_examples():\n",
    "    \"\"\"Irregular access patterns - harder to parallelize efficiently.\"\"\"\n",
    "    \n",
    "    # Sparse matrix-vector multiply\n",
    "    # Access pattern depends on where non-zeros are\n",
    "    from scipy import sparse\n",
    "    \n",
    "    # Create sparse matrix (90% zeros)\n",
    "    density = 0.1\n",
    "    A = sparse.random(1000, 1000, density=density, format='csr')\n",
    "    x = np.random.rand(1000)\n",
    "    \n",
    "    # Sparse multiply - each row accesses different columns\n",
    "    y = A.dot(x)\n",
    "    \n",
    "    return A, y\n",
    "\n",
    "A, y = irregular_examples()\n",
    "print(f\"Sparse matrix: {A.shape}, {A.nnz} non-zeros ({A.nnz / (A.shape[0]*A.shape[1]) * 100:.1f}% density)\")\n",
    "print(f\"\\nIrregular because: each row accesses different column indices.\")\n",
    "print(\"GPU threads would access scattered memory locations - poor coalescing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Classify These Operations\n",
    "\n",
    "For each operation, identify the pattern:\n",
    "- **E** = Embarrassingly Parallel\n",
    "- **R** = Reduction\n",
    "- **S** = Stencil\n",
    "- **I** = Irregular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Classify each operation\n",
    "\n",
    "operations = [\n",
    "    \"ReLU activation: max(0, x)\",\n",
    "    \"Softmax denominator: sum(exp(x))\", \n",
    "    \"2D Convolution\",\n",
    "    \"Matrix multiplication C = A @ B\",\n",
    "    \"LayerNorm: (x - mean) / std\",\n",
    "    \"Attention scores: Q @ K.T\",\n",
    "    \"Graph neural network message passing\",\n",
    "    \"Histogram computation\",\n",
    "]\n",
    "\n",
    "# Answers (try to figure them out first!)\n",
    "answers = [\n",
    "    \"E - Each element independent\",\n",
    "    \"R - Sum many values to one\",\n",
    "    \"S - Each output depends on input neighborhood\",\n",
    "    \"R + E - Dot products (reduction) for each output (parallel)\",\n",
    "    \"R + E - Compute mean/std (reduction), then normalize (parallel)\",\n",
    "    \"R + E - Dot products for each query-key pair\",\n",
    "    \"I - Neighbors depend on graph structure\",\n",
    "    \"R + I - Bin assignment irregular, bin counting is reduction\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reveal answers\n",
    "print(\"Pattern Classification:\")\n",
    "print(\"=\" * 60)\n",
    "for op, ans in zip(operations, answers):\n",
    "    print(f\"\\n{op}\")\n",
    "    print(f\"  -> {ans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Pattern recognition is the first step**: Before optimizing, identify the pattern\n",
    "2. **Embarrassingly parallel = GPU heaven**: Maximum parallelism, minimal communication\n",
    "3. **Reductions are well-understood**: Tree reduction gives O(log N) parallel steps\n",
    "4. **Stencils use shared memory**: Load neighborhood once, compute multiple outputs\n",
    "5. **Irregular patterns are hardest**: May need algorithmic changes, not just GPU porting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
