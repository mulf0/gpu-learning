{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 4: Software Pipelining\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Learn to hide memory latency by overlapping loads with computation.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Memory loads take ~400 cycles. Compute takes ~4 cycles. If we wait for loads, we waste 99% of potential compute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.testing import do_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "### Sequential Execution\n",
    "\n",
    "```\n",
    "Time →\n",
    "┌─────────┐     ┌─────────┐     ┌─────────┐\n",
    "│ Load A  │     │ Load B  │     │ Compute │     (Repeat)\n",
    "└─────────┘     └─────────┘     └─────────┘\n",
    "   400 cy          400 cy          4 cy\n",
    "\n",
    "Total: 804 cycles per iteration\n",
    "Compute utilization: 4/804 = 0.5%\n",
    "```\n",
    "\n",
    "### Pipelined Execution\n",
    "\n",
    "```\n",
    "Time →\n",
    "┌─────────┬─────────┬─────────┬─────────┬──────\n",
    "│ Load A₀ │ Load A₁ │ Load A₂ │ Load A₃ │ ...\n",
    "├─────────┼─────────┼─────────┼─────────┼──────\n",
    "│         │ Load B₀ │ Load B₁ │ Load B₂ │ ...\n",
    "├─────────┼─────────┼─────────┼─────────┼──────\n",
    "│         │         │Compute₀ │Compute₁ │ ...\n",
    "└─────────┴─────────┴─────────┴─────────┴──────\n",
    "\n",
    "After warmup: All stages run in parallel!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore (15 min)\n",
    "\n",
    "### Double Buffering\n",
    "\n",
    "The simplest pipelining technique: use 2 buffers.\n",
    "\n",
    "```\n",
    "While computing with Buffer A:\n",
    "  → Load next data into Buffer B\n",
    "\n",
    "While computing with Buffer B:\n",
    "  → Load next data into Buffer A\n",
    "```\n",
    "\n",
    "This requires:\n",
    "1. **Async memory operations** (loads that don't block)\n",
    "2. **Synchronization** (ensure load completes before use)\n",
    "3. **Multiple buffers** (to hold data for different stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_no_pipeline(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Matmul without pipelining - synchronous loads.\"\"\"\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        # Synchronous loads - kernel waits for these\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        \n",
    "        # Compute\n",
    "        acc += tl.dot(a, b)\n",
    "        \n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept (10 min)\n",
    "\n",
    "### Pipelining in Triton\n",
    "\n",
    "Triton provides `num_stages` parameter for automatic pipelining:\n",
    "\n",
    "```python\n",
    "@triton.jit\n",
    "def kernel(..., num_stages=2):  # 2-stage pipeline (double buffer)\n",
    "    ...\n",
    "```\n",
    "\n",
    "When `num_stages > 1`, Triton automatically:\n",
    "1. Uses async copy operations\n",
    "2. Manages multiple SMEM buffers\n",
    "3. Inserts proper barriers\n",
    "\n",
    "### Stage Count Tradeoffs\n",
    "\n",
    "| Stages | SMEM Usage | Latency Hiding | Complexity |\n",
    "|--------|------------|----------------|------------|\n",
    "| 1 | Minimum | None | Simple |\n",
    "| 2 | 2x | Moderate | Medium |\n",
    "| 3 | 3x | Good | Medium |\n",
    "| 4+ | 4x+ | Excellent | Higher |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_pipelined(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "    NUM_STAGES: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Matmul with software pipelining.\n",
    "    \n",
    "    Triton handles the pipelining when we use num_stages in the kernel config.\n",
    "    This version shows the concept - actual pipelining is in the autotuner.\n",
    "    \"\"\"\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    # Main loop - Triton compiler will pipeline this\n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        \n",
    "        acc += tl.dot(a, b)\n",
    "        \n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_wrapper(a, b, kernel_fn, BLOCK_M=64, BLOCK_N=64, BLOCK_K=32, num_stages=1):\n",
    "    \"\"\"Wrapper for matmul kernels.\"\"\"\n",
    "    M, K = a.shape\n",
    "    K2, N = b.shape\n",
    "    assert K == K2\n",
    "    \n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
    "    \n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "    \n",
    "    kernel_fn[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "        NUM_STAGES=num_stages,\n",
    "    )\n",
    "    return c\n",
    "\n",
    "def benchmark_pipeline_stages(M, N, K):\n",
    "    \"\"\"Benchmark matmul with different pipeline depths.\"\"\"\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for stages in [1, 2, 3, 4]:\n",
    "        ms = do_bench(lambda s=stages: matmul_wrapper(a, b, matmul_pipelined, num_stages=s))\n",
    "        flops = 2 * M * N * K\n",
    "        tflops = flops / (ms * 1e-3) / 1e12\n",
    "        results[stages] = {'ms': ms, 'tflops': tflops}\n",
    "    \n",
    "    # PyTorch baseline\n",
    "    ms_torch = do_bench(lambda: torch.mm(a, b))\n",
    "    tflops_torch = flops / (ms_torch * 1e-3) / 1e12\n",
    "    results['torch'] = {'ms': ms_torch, 'tflops': tflops_torch}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It (30 min)\n",
    "\n",
    "### Autotuning Pipeline Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triton autotuning configs\n",
    "configs = [\n",
    "    triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_stages=1),\n",
    "    triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_stages=2),\n",
    "    triton.Config({'BLOCK_M': 64, 'BLOCK_N': 64, 'BLOCK_K': 32}, num_stages=3),\n",
    "    triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=2),\n",
    "    triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': 32}, num_stages=3),\n",
    "]\n",
    "\n",
    "@triton.autotune(configs=configs, key=['M', 'N', 'K'])\n",
    "@triton.jit\n",
    "def matmul_autotuned(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Autotuned matmul - Triton finds best config including pipeline depth.\"\"\"\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        acc += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_autotuned(M, N, K):\n",
    "    \"\"\"Benchmark autotuned kernel.\"\"\"\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    c = torch.empty(M, N, device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']), triton.cdiv(N, META['BLOCK_N']))\n",
    "    \n",
    "    # Warmup + autotune\n",
    "    matmul_autotuned[grid](\n",
    "        a, b, c, M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "    )\n",
    "    \n",
    "    ms = do_bench(lambda: matmul_autotuned[grid](\n",
    "        a, b, c, M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "    ))\n",
    "    \n",
    "    flops = 2 * M * N * K\n",
    "    tflops = flops / (ms * 1e-3) / 1e12\n",
    "    \n",
    "    # Compare to PyTorch\n",
    "    ms_torch = do_bench(lambda: torch.mm(a, b))\n",
    "    tflops_torch = flops / (ms_torch * 1e-3) / 1e12\n",
    "    \n",
    "    return {\n",
    "        'triton_ms': ms,\n",
    "        'triton_tflops': tflops,\n",
    "        'torch_ms': ms_torch,\n",
    "        'torch_tflops': tflops_torch,\n",
    "        'efficiency': tflops / tflops_torch * 100,\n",
    "    }\n",
    "\n",
    "print(\"Autotuned Matmul Performance\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for size in [1024, 2048, 4096]:\n",
    "    result = benchmark_autotuned(size, size, size)\n",
    "    print(f\"\\nSize {size}x{size}:\")\n",
    "    print(f\"  Triton:    {result['triton_ms']:.3f} ms, {result['triton_tflops']:.1f} TFLOPS\")\n",
    "    print(f\"  PyTorch:   {result['torch_ms']:.3f} ms, {result['torch_tflops']:.1f} TFLOPS\")\n",
    "    print(f\"  Efficiency: {result['efficiency']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer-Consumer Pattern\n",
    "\n",
    "In CUDA, pipelining uses explicit barriers:\n",
    "\n",
    "```cpp\n",
    "// Producer (memory load)\n",
    "cp.async.cg.shared.global [smem], [gmem];  // Async copy\n",
    "cp.async.commit_group;  // Commit this batch\n",
    "\n",
    "// Consumer (compute)\n",
    "cp.async.wait_group N;  // Wait for N groups to complete\n",
    "// Now safe to use data\n",
    "```\n",
    "\n",
    "Triton abstracts this - the compiler generates proper barriers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify (10 min)\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**Q1:** What's the minimum number of pipeline stages needed to hide memory latency?\n",
    "\n",
    "A) 1 (no pipelining)  \n",
    "B) latency / compute_time  \n",
    "C) 2 (double buffering is always enough)  \n",
    "D) As many as SMEM allows\n",
    "\n",
    "**Q2:** What's the tradeoff of more pipeline stages?\n",
    "\n",
    "A) More SMEM usage  \n",
    "B) More register pressure  \n",
    "C) Longer warmup/drain  \n",
    "D) All of the above\n",
    "\n",
    "**Q3:** Why might 3-stage pipelining be worse than 2-stage on some GPUs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quiz Answers\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Q1: B) latency / compute_time\")\n",
    "print(\"    To fully hide 400-cycle latency with 100-cycle compute,\")\n",
    "print(\"    you need at least 4 stages (400/100 = 4).\")\n",
    "print()\n",
    "print(\"Q2: D) All of the above\")\n",
    "print(\"    More stages = more buffers = more SMEM and registers.\")\n",
    "print(\"    Also longer warmup (fill pipeline) and drain (empty it).\")\n",
    "print()\n",
    "print(\"Q3: SMEM limitation\")\n",
    "print(\"    If 3 buffers exceed SMEM capacity, occupancy drops.\")\n",
    "print(\"    Lower occupancy can hurt more than pipelining helps.\")\n",
    "print(\"    Always benchmark - optimal stages depend on kernel size and GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Memory latency (~400 cycles) dominates compute (~4 cycles)**\n",
    "2. **Pipelining overlaps loads with compute** using multiple buffers\n",
    "3. **Triton's `num_stages`** controls pipeline depth automatically\n",
    "4. **More stages = better latency hiding** but more SMEM usage\n",
    "5. **Autotune to find optimal** - it depends on kernel and GPU\n",
    "\n",
    "### Pipeline Depth Guidelines\n",
    "\n",
    "| GPU | Typical Optimal Stages |\n",
    "|-----|------------------------|\n",
    "| Ampere (A100) | 3-4 stages |\n",
    "| Hopper (H100) | 2-3 stages (TMA helps!) |\n",
    "\n",
    "### Tomorrow: TMA\n",
    "\n",
    "On Hopper GPUs, the Tensor Memory Accelerator (TMA) does async copies with ZERO SM involvement - even better than software pipelining!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
