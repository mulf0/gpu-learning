{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 1: Profile Like a Pro\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Learn to profile GPU kernels and identify bottlenecks using real metrics.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Your Week 1 matmul hits 500 GFLOPS. The H100 can do 990 TFLOPS (FP16 Tensor Cores).\n",
    "That's a **2000x gap**. Where's the bottleneck?\n",
    "\n",
    "Today we learn to answer: **Is my kernel compute-bound or memory-bound?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "Let's bring back our tiled matmul from Week 1 and measure its actual performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Tiled matrix multiplication kernel from Week 1.\"\"\"\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    # Block starting positions\n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    # Pointers to first block of A and B\n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    # Accumulator\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    # Main loop over K dimension\n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        # Load tiles with boundary checks\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        \n",
    "        # Accumulate\n",
    "        acc += tl.dot(a, b)\n",
    "        \n",
    "        # Advance pointers\n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    # Store result\n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n",
    "    tl.store(c_ptrs, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_triton(a, b, BLOCK_M=64, BLOCK_N=64, BLOCK_K=32):\n",
    "    \"\"\"Wrapper for Triton matmul kernel.\"\"\"\n",
    "    M, K = a.shape\n",
    "    K2, N = b.shape\n",
    "    assert K == K2\n",
    "    \n",
    "    c = torch.empty((M, N), device=a.device, dtype=torch.float32)\n",
    "    \n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "    \n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "    )\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore (15 min)\n",
    "\n",
    "### The Three Key Metrics\n",
    "\n",
    "Every GPU kernel can be characterized by three numbers:\n",
    "\n",
    "| Metric | What It Measures | Unit | How to Get It |\n",
    "|--------|-----------------|------|---------------|\n",
    "| **Achieved FLOPS** | Compute throughput | TFLOPS | `2*M*N*K / time` |\n",
    "| **Memory Bandwidth** | Data movement rate | GB/s | `bytes_moved / time` |\n",
    "| **Occupancy** | GPU utilization | % | `active_warps / max_warps` |\n",
    "\n",
    "Let's measure each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_matmul(M, N, K, num_runs=100, warmup=10):\n",
    "    \"\"\"Benchmark matmul and return performance metrics.\"\"\"\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(warmup):\n",
    "        c = matmul_triton(a, b)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(num_runs):\n",
    "        c = matmul_triton(a, b)\n",
    "    end.record()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    time_ms = start.elapsed_time(end) / num_runs\n",
    "    \n",
    "    # Calculate metrics\n",
    "    flops = 2 * M * N * K  # multiply-add = 2 ops\n",
    "    tflops = flops / (time_ms * 1e-3) / 1e12\n",
    "    \n",
    "    # Memory: read A, read B, write C\n",
    "    bytes_accessed = (M * K + K * N + M * N) * 2  # FP16 = 2 bytes\n",
    "    bandwidth_gb = bytes_accessed / (time_ms * 1e-3) / 1e9\n",
    "    \n",
    "    return {\n",
    "        'time_ms': time_ms,\n",
    "        'tflops': tflops,\n",
    "        'bandwidth_gb': bandwidth_gb,\n",
    "        'flops': flops,\n",
    "        'bytes': bytes_accessed,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark our kernel\n",
    "sizes = [(1024, 1024, 1024), (2048, 2048, 2048), (4096, 4096, 4096)]\n",
    "\n",
    "print(\"Matmul Performance\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Size':<20} {'Time (ms)':<12} {'TFLOPS':<12} {'BW (GB/s)':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for M, N, K in sizes:\n",
    "    metrics = benchmark_matmul(M, N, K)\n",
    "    print(f\"{f'{M}x{N}x{K}':<20} {metrics['time_ms']:<12.3f} {metrics['tflops']:<12.2f} {metrics['bandwidth_gb']:<12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept (10 min)\n",
    "\n",
    "### The Roofline Model\n",
    "\n",
    "The **roofline model** tells us whether a kernel is limited by compute or memory.\n",
    "\n",
    "```\n",
    "                    ┌───────────────── Compute Roof (peak TFLOPS)\n",
    "                    │\n",
    "    TFLOPS          │    ╱────────────\n",
    "      │             │   ╱\n",
    "      │             │  ╱  Memory-bound region\n",
    "      │             │ ╱   (slope = peak bandwidth)\n",
    "      │             │╱\n",
    "      └─────────────┴────────────────── Arithmetic Intensity (FLOPS/byte)\n",
    "```\n",
    "\n",
    "**Key insight:** \n",
    "- If your kernel is below the sloped line → **memory-bound** (need better access patterns)\n",
    "- If your kernel is below the flat line → **compute-bound** (need Tensor Cores)\n",
    "\n",
    "### Arithmetic Intensity\n",
    "\n",
    "$$\\text{Arithmetic Intensity} = \\frac{\\text{FLOPS}}{\\text{Bytes Accessed}}$$\n",
    "\n",
    "For matrix multiplication C = A × B:\n",
    "- FLOPS = 2 × M × N × K\n",
    "- Bytes (naive) = (M×K + K×N + M×N) × sizeof(dtype)\n",
    "- Bytes (tiled) = Much less due to data reuse!\n",
    "\n",
    "**The tiling trick:** By keeping data in shared memory, we increase arithmetic intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_arithmetic_intensity(M, N, K, dtype_bytes=2):\n",
    "    \"\"\"Calculate arithmetic intensity for matmul.\"\"\"\n",
    "    flops = 2 * M * N * K\n",
    "    \n",
    "    # Naive: every element loaded from HBM\n",
    "    bytes_naive = (M * K + K * N + M * N) * dtype_bytes\n",
    "    ai_naive = flops / bytes_naive\n",
    "    \n",
    "    # With tiling: each A element used N/TILE_N times, each B element used M/TILE_M times\n",
    "    # Effective bytes is much lower\n",
    "    TILE = 64\n",
    "    reuse_factor = TILE  # approximate\n",
    "    bytes_tiled = bytes_naive / reuse_factor\n",
    "    ai_tiled = flops / bytes_tiled\n",
    "    \n",
    "    return ai_naive, ai_tiled\n",
    "\n",
    "# Example: 4096x4096 matmul\n",
    "M = N = K = 4096\n",
    "ai_naive, ai_tiled = calculate_arithmetic_intensity(M, N, K)\n",
    "\n",
    "print(f\"Arithmetic Intensity for {M}x{N}x{K} matmul:\")\n",
    "print(f\"  Naive:  {ai_naive:.1f} FLOPS/byte\")\n",
    "print(f\"  Tiled:  {ai_tiled:.1f} FLOPS/byte (approximate)\")\n",
    "print()\n",
    "print(\"H100 specs:\")\n",
    "print(f\"  Peak FP16 Tensor Core: 990 TFLOPS\")\n",
    "print(f\"  HBM Bandwidth: 3.35 TB/s\")\n",
    "print(f\"  Balance point: {990 / 3.35:.1f} FLOPS/byte\")\n",
    "print()\n",
    "if ai_tiled > 990 / 3.35:\n",
    "    print(\"→ Kernel should be COMPUTE-bound (good for Tensor Cores)\")\n",
    "else:\n",
    "    print(\"→ Kernel is MEMORY-bound (need better tiling)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It (30 min)\n",
    "\n",
    "### Using Triton's Built-in Profiler\n",
    "\n",
    "Triton provides `triton.testing.do_bench` for accurate benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton.testing import do_bench\n",
    "\n",
    "def profile_matmul_detailed(M, N, K):\n",
    "    \"\"\"Profile matmul with detailed metrics.\"\"\"\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Triton's benchmark function handles warmup and statistics\n",
    "    ms = do_bench(lambda: matmul_triton(a, b))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    flops = 2 * M * N * K\n",
    "    tflops = flops / (ms * 1e-3) / 1e12\n",
    "    \n",
    "    # Compare to cuBLAS\n",
    "    cublas_ms = do_bench(lambda: torch.mm(a, b))\n",
    "    cublas_tflops = flops / (cublas_ms * 1e-3) / 1e12\n",
    "    \n",
    "    return {\n",
    "        'triton_ms': ms,\n",
    "        'triton_tflops': tflops,\n",
    "        'cublas_ms': cublas_ms,\n",
    "        'cublas_tflops': cublas_tflops,\n",
    "        'efficiency': tflops / cublas_tflops * 100,\n",
    "    }\n",
    "\n",
    "# Compare against cuBLAS\n",
    "print(\"Triton vs cuBLAS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Size':<15} {'Triton (ms)':<12} {'cuBLAS (ms)':<12} {'Triton TFLOPS':<14} {'Efficiency':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for size in [1024, 2048, 4096, 8192]:\n",
    "    metrics = profile_matmul_detailed(size, size, size)\n",
    "    print(f\"{f'{size}x{size}':<15} {metrics['triton_ms']:<12.3f} {metrics['cublas_ms']:<12.3f} \"\n",
    "          f\"{metrics['triton_tflops']:<14.2f} {metrics['efficiency']:<10.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nsight Compute Profiling (Command Line)\n",
    "\n",
    "For deep analysis, use NVIDIA's Nsight Compute profiler. Here's how to use it:\n",
    "\n",
    "```bash\n",
    "# Profile a Python script\n",
    "ncu --set full -o profile_output python your_script.py\n",
    "\n",
    "# Key metrics to look for:\n",
    "# - sm__throughput.avg_pct_of_peak_sustained_elapsed  (SM utilization)\n",
    "# - dram__throughput.avg_pct_of_peak_sustained_elapsed (Memory bandwidth)\n",
    "# - sm__warps_active.avg_pct_of_peak_sustained_elapsed (Occupancy)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a script we can profile with ncu\n",
    "profile_script = '''\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    a_ptr, b_ptr, c_ptr, M, N, K,\n",
    "    stride_am, stride_ak, stride_bk, stride_bn, stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        acc += tl.dot(a, b)\n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n",
    "\n",
    "M = N = K = 4096\n",
    "a = torch.randn(M, K, device=\"cuda\", dtype=torch.float16)\n",
    "b = torch.randn(K, N, device=\"cuda\", dtype=torch.float16)\n",
    "c = torch.empty(M, N, device=\"cuda\", dtype=torch.float32)\n",
    "\n",
    "grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n",
    "matmul_kernel[grid](\n",
    "    a, b, c, M, N, K,\n",
    "    a.stride(0), a.stride(1), b.stride(0), b.stride(1), c.stride(0), c.stride(1),\n",
    "    BLOCK_M=64, BLOCK_N=64, BLOCK_K=32,\n",
    ")\n",
    "torch.cuda.synchronize()\n",
    "print(\"Kernel executed successfully\")\n",
    "'''\n",
    "\n",
    "with open('profile_matmul.py', 'w') as f:\n",
    "    f.write(profile_script)\n",
    "\n",
    "print(\"Created profile_matmul.py\")\n",
    "print(\"\\nTo profile with Nsight Compute:\")\n",
    "print(\"  ncu --set full python profile_matmul.py\")\n",
    "print(\"\\nOr for a quick summary:\")\n",
    "print(\"  ncu --metrics sm__throughput.avg_pct_of_peak_sustained_elapsed,dram__throughput.avg_pct_of_peak_sustained_elapsed python profile_matmul.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Occupancy\n",
    "\n",
    "**Occupancy** = fraction of maximum warps that are active on an SM.\n",
    "\n",
    "Low occupancy means the GPU is underutilized. Common causes:\n",
    "1. **Too many registers per thread** → fewer warps fit\n",
    "2. **Too much shared memory per block** → fewer blocks fit\n",
    "3. **Block size too large** → fewer blocks scheduled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_occupancy(block_size, registers_per_thread=32, smem_per_block=0):\n",
    "    \"\"\"Estimate occupancy for a given configuration.\n",
    "    \n",
    "    Based on H100 specs:\n",
    "    - 65536 registers per SM\n",
    "    - 228 KB shared memory per SM\n",
    "    - 2048 threads per SM (64 warps)\n",
    "    \"\"\"\n",
    "    # H100 specs\n",
    "    REGS_PER_SM = 65536\n",
    "    SMEM_PER_SM = 228 * 1024  # bytes\n",
    "    MAX_THREADS_PER_SM = 2048\n",
    "    MAX_WARPS_PER_SM = 64\n",
    "    MAX_BLOCKS_PER_SM = 32\n",
    "    \n",
    "    threads_per_block = block_size\n",
    "    warps_per_block = (threads_per_block + 31) // 32\n",
    "    \n",
    "    # Limit by registers\n",
    "    regs_per_block = threads_per_block * registers_per_thread\n",
    "    blocks_by_regs = REGS_PER_SM // regs_per_block if regs_per_block > 0 else MAX_BLOCKS_PER_SM\n",
    "    \n",
    "    # Limit by shared memory\n",
    "    blocks_by_smem = SMEM_PER_SM // smem_per_block if smem_per_block > 0 else MAX_BLOCKS_PER_SM\n",
    "    \n",
    "    # Limit by warps\n",
    "    blocks_by_warps = MAX_WARPS_PER_SM // warps_per_block\n",
    "    \n",
    "    # Limit by max blocks\n",
    "    blocks_per_sm = min(blocks_by_regs, blocks_by_smem, blocks_by_warps, MAX_BLOCKS_PER_SM)\n",
    "    \n",
    "    active_warps = blocks_per_sm * warps_per_block\n",
    "    occupancy = active_warps / MAX_WARPS_PER_SM * 100\n",
    "    \n",
    "    return {\n",
    "        'blocks_per_sm': blocks_per_sm,\n",
    "        'active_warps': active_warps,\n",
    "        'occupancy_pct': occupancy,\n",
    "        'limited_by': 'regs' if blocks_per_sm == blocks_by_regs else \n",
    "                      'smem' if blocks_per_sm == blocks_by_smem else\n",
    "                      'warps' if blocks_per_sm == blocks_by_warps else 'blocks',\n",
    "    }\n",
    "\n",
    "# Test different configurations\n",
    "print(\"Occupancy Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Block Size':<12} {'Regs/Thread':<12} {'SMEM (KB)':<12} {'Occupancy':<12} {'Limited By':<12}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "configs = [\n",
    "    (256, 32, 0),      # Small block, few regs\n",
    "    (256, 64, 0),      # Small block, more regs\n",
    "    (256, 32, 48*1024), # With shared memory\n",
    "    (512, 32, 0),      # Larger block\n",
    "    (1024, 32, 0),     # Max block size\n",
    "]\n",
    "\n",
    "for block_size, regs, smem in configs:\n",
    "    result = estimate_occupancy(block_size, regs, smem)\n",
    "    print(f\"{block_size:<12} {regs:<12} {smem/1024:<12.0f} {result['occupancy_pct']:<12.1f}% {result['limited_by']:<12}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify (10 min)\n",
    "\n",
    "### Exercise: Diagnose This Kernel\n",
    "\n",
    "Given these metrics from a matmul kernel:\n",
    "- Achieved: 150 TFLOPS\n",
    "- Memory bandwidth: 2.8 TB/s (out of 3.35 TB/s peak)\n",
    "- Occupancy: 45%\n",
    "\n",
    "**Questions:**\n",
    "1. Is this kernel compute-bound or memory-bound?\n",
    "2. What's the efficiency vs peak Tensor Core performance?\n",
    "3. What optimizations would you try first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your analysis here\n",
    "achieved_tflops = 150\n",
    "memory_bw_tbs = 2.8\n",
    "occupancy_pct = 45\n",
    "\n",
    "# H100 peaks\n",
    "peak_tflops = 990  # FP16 Tensor Core\n",
    "peak_bw_tbs = 3.35\n",
    "\n",
    "# Calculate utilizations\n",
    "compute_util = achieved_tflops / peak_tflops * 100\n",
    "memory_util = memory_bw_tbs / peak_bw_tbs * 100\n",
    "\n",
    "print(\"Kernel Analysis\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Compute utilization: {compute_util:.1f}%\")\n",
    "print(f\"Memory utilization:  {memory_util:.1f}%\")\n",
    "print(f\"Occupancy:           {occupancy_pct:.1f}%\")\n",
    "print()\n",
    "\n",
    "if memory_util > compute_util:\n",
    "    print(\"Diagnosis: MEMORY-BOUND\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"1. Increase tile sizes for better data reuse\")\n",
    "    print(\"2. Use Tensor Cores (they have higher compute/memory ratio)\")\n",
    "    print(\"3. Check for coalescing issues\")\n",
    "else:\n",
    "    print(\"Diagnosis: COMPUTE-BOUND\")\n",
    "    print(\"\\nRecommendations:\")\n",
    "    print(\"1. Ensure Tensor Cores are being used\")\n",
    "    print(\"2. Check for warp divergence\")\n",
    "    print(\"3. Improve occupancy if possible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz\n",
    "\n",
    "**Q1:** A kernel achieves 90% memory bandwidth utilization but only 20% compute utilization. What does this indicate?\n",
    "\n",
    "A) The kernel is compute-bound  \n",
    "B) The kernel is memory-bound  \n",
    "C) The kernel is perfectly balanced  \n",
    "D) The kernel has a bug\n",
    "\n",
    "**Q2:** What's the arithmetic intensity of a vector addition kernel (C[i] = A[i] + B[i])?\n",
    "\n",
    "A) 0.5 FLOPS/byte (reads 8 bytes, writes 4 bytes, does 1 FLOP)  \n",
    "B) 0.083 FLOPS/byte (1 FLOP / 12 bytes for FP32)  \n",
    "C) 1 FLOP/byte  \n",
    "D) Depends on vector length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers\n",
    "print(\"Q1: B) The kernel is memory-bound\")\n",
    "print(\"   High memory utilization + low compute utilization = memory-bound\")\n",
    "print()\n",
    "print(\"Q2: B) 0.083 FLOPS/byte\")\n",
    "print(\"   For FP32: 1 FLOP / (4+4+4 bytes) = 1/12 = 0.083\")\n",
    "print(\"   Vector addition has very low arithmetic intensity - always memory-bound!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Three metrics matter:** TFLOPS, Memory Bandwidth, Occupancy\n",
    "2. **Roofline model:** Compare arithmetic intensity to the balance point\n",
    "3. **Memory-bound kernels:** Need better access patterns, more tiling, less data movement\n",
    "4. **Compute-bound kernels:** Need Tensor Cores, less warp divergence\n",
    "5. **Profile first, optimize second:** Never guess where the bottleneck is\n",
    "\n",
    "### Tools Learned\n",
    "\n",
    "| Tool | Use Case |\n",
    "|------|----------|\n",
    "| `triton.testing.do_bench` | Quick benchmarking |\n",
    "| `ncu` (Nsight Compute) | Deep profiling |\n",
    "| `nsys` (Nsight Systems) | Timeline analysis |\n",
    "\n",
    "### Tomorrow: Coalescing Experiments\n",
    "\n",
    "Now that we can measure performance, we'll experiment with memory access patterns and see exactly how coalescing affects bandwidth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import os\n",
    "if os.path.exists('profile_matmul.py'):\n",
    "    os.remove('profile_matmul.py')\n",
    "    print(\"Cleaned up profile_matmul.py\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
