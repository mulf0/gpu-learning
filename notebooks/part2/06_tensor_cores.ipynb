{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2, Day 6: Tensor Cores\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Understand and use Tensor Cores for matrix operations.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "CUDA cores do ~15 TFLOPS. Tensor Cores do ~990 TFLOPS (H100 FP16). That's **66x faster** for matrix math!\n",
    "\n",
    "But Tensor Cores have rules: specific shapes, specific types, specific layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from triton.testing import do_bench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "### What Are Tensor Cores?\n",
    "\n",
    "Specialized hardware units that compute **D = A × B + C** in one operation.\n",
    "\n",
    "```\n",
    "Single Tensor Core Operation:\n",
    "\n",
    "    A              B              C              D\n",
    "  [m×k]    ×    [k×n]    +    [m×n]    =    [m×n]\n",
    "\n",
    "Example (FP16, V100):\n",
    "  [16×16]  ×   [16×16]  +   [16×16]  =   [16×16]\n",
    "  = 8,192 FLOPs in ONE cycle!\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Tensor Core support\n",
    "def check_tensor_core_support():\n",
    "    \"\"\"Check GPU's Tensor Core capabilities.\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return None\n",
    "    \n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    sm = props.major * 10 + props.minor\n",
    "    \n",
    "    capabilities = {\n",
    "        'name': props.name,\n",
    "        'sm': f\"SM{props.major}{props.minor}\",\n",
    "        'has_tensor_cores': sm >= 70,  # Volta+\n",
    "        'fp16': sm >= 70,\n",
    "        'bf16': sm >= 80,\n",
    "        'tf32': sm >= 80,\n",
    "        'fp8': sm >= 89,\n",
    "        'fp4': sm >= 100,  # Blackwell\n",
    "    }\n",
    "    \n",
    "    return capabilities\n",
    "\n",
    "caps = check_tensor_core_support()\n",
    "if caps:\n",
    "    print(f\"GPU: {caps['name']} ({caps['sm']})\")\n",
    "    print(f\"\\nTensor Core Support:\")\n",
    "    print(f\"  FP16: {'Yes' if caps['fp16'] else 'No'}\")\n",
    "    print(f\"  BF16: {'Yes' if caps['bf16'] else 'No'}\")\n",
    "    print(f\"  TF32: {'Yes' if caps['tf32'] else 'No'}\")\n",
    "    print(f\"  FP8:  {'Yes' if caps['fp8'] else 'No'}\")\n",
    "else:\n",
    "    print(\"No CUDA GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore (15 min)\n",
    "\n",
    "### Tensor Core Evolution\n",
    "\n",
    "| Generation | Architecture | MMA Shape | Types | Peak TFLOPS (FP16) |\n",
    "|------------|-------------|-----------|-------|--------------------|\n",
    "| 1st | Volta (V100) | 16×16×16 | FP16 | 125 |\n",
    "| 2nd | Turing (T4) | 16×16×16 | FP16, INT8 | 65 |\n",
    "| 3rd | Ampere (A100) | 16×8×16 | +BF16, TF32 | 312 |\n",
    "| 4th | Hopper (H100) | Warpgroup | +FP8 | 990 |\n",
    "| 5th | Blackwell (B100) | — | +FP4, FP6 | 2500+ |\n",
    "\n",
    "### How Tensor Cores Work\n",
    "\n",
    "```\n",
    "32 threads (1 warp) cooperate to compute a small matrix multiply:\n",
    "\n",
    "Thread 0-3:   Load A[0:4, 0:4]     Thread 0-3:   Store D[0:4, 0:4]\n",
    "Thread 4-7:   Load A[4:8, 0:4]  →  Thread 4-7:   Store D[4:8, 0:4]\n",
    "...                                 ...\n",
    "Thread 28-31: Load B[0:4, 12:16]   Thread 28-31: Store D[12:16, 12:16]\n",
    "\n",
    "The actual MMA is done by dedicated hardware, not CUDA cores.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triton uses tl.dot which maps to Tensor Cores automatically\n",
    "@triton.jit\n",
    "def matmul_tensor_core(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Matmul using Tensor Cores via tl.dot.\n",
    "    \n",
    "    Triton automatically uses Tensor Cores when:\n",
    "    - Input types are FP16, BF16, or FP8\n",
    "    - Tile sizes are compatible (multiples of 16)\n",
    "    - Using tl.dot operation\n",
    "    \"\"\"\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    # Accumulator in FP32 for precision\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        \n",
    "        # tl.dot uses Tensor Cores when inputs are FP16\n",
    "        acc += tl.dot(a, b)\n",
    "        \n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    tl.store(c_ptrs, acc, mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_tensor_cores(M, N, K):\n",
    "    \"\"\"Benchmark Tensor Core matmul.\"\"\"\n",
    "    # FP16 inputs (required for Tensor Cores)\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    c = torch.empty(M, N, device='cuda', dtype=torch.float32)\n",
    "    \n",
    "    # Tile sizes should be multiples of 16 for Tensor Cores\n",
    "    BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "    \n",
    "    # Triton kernel (uses Tensor Cores)\n",
    "    ms_triton = do_bench(lambda: matmul_tensor_core[grid](\n",
    "        a, b, c, M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "    ))\n",
    "    \n",
    "    # PyTorch (also uses Tensor Cores via cuBLAS)\n",
    "    ms_torch = do_bench(lambda: torch.mm(a, b))\n",
    "    \n",
    "    # Calculate TFLOPS\n",
    "    flops = 2 * M * N * K\n",
    "    tflops_triton = flops / (ms_triton * 1e-3) / 1e12\n",
    "    tflops_torch = flops / (ms_torch * 1e-3) / 1e12\n",
    "    \n",
    "    return {\n",
    "        'triton_ms': ms_triton,\n",
    "        'torch_ms': ms_torch,\n",
    "        'triton_tflops': tflops_triton,\n",
    "        'torch_tflops': tflops_torch,\n",
    "        'efficiency': tflops_triton / tflops_torch * 100,\n",
    "    }\n",
    "\n",
    "print(\"Tensor Core Matmul Benchmark\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Size':<15} {'Triton (TFLOPS)':<18} {'PyTorch (TFLOPS)':<18} {'Efficiency':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for size in [1024, 2048, 4096, 8192]:\n",
    "    result = benchmark_tensor_cores(size, size, size)\n",
    "    print(f\"{f'{size}x{size}':<15} {result['triton_tflops']:<18.1f} {result['torch_tflops']:<18.1f} {result['efficiency']:<12.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept (10 min)\n",
    "\n",
    "### Tensor Core Requirements\n",
    "\n",
    "To use Tensor Cores, you must satisfy:\n",
    "\n",
    "1. **Data Types:**\n",
    "   - A, B: FP16, BF16, TF32, FP8, INT8\n",
    "   - C, D: FP32 or same as A, B\n",
    "\n",
    "2. **Tile Dimensions:**\n",
    "   - Must be multiples of the native MMA shape\n",
    "   - Typically 16×16 or 8×16\n",
    "\n",
    "3. **Memory Layout:**\n",
    "   - Specific thread-to-data mapping\n",
    "   - Triton handles this automatically\n",
    "\n",
    "### Data Type Comparison\n",
    "\n",
    "| Type | Bits | Range | Precision | TFLOPS (H100) |\n",
    "|------|------|-------|-----------|---------------|\n",
    "| FP32 | 32 | ±3.4e38 | High | 67 (CUDA cores) |\n",
    "| TF32 | 19 | ±3.4e38 | Medium | 495 |\n",
    "| FP16 | 16 | ±65504 | Medium | 990 |\n",
    "| BF16 | 16 | ±3.4e38 | Lower | 990 |\n",
    "| FP8 | 8 | ±448 (E4M3) | Low | 1979 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dtypes(M, N, K):\n",
    "    \"\"\"Compare performance across data types.\"\"\"\n",
    "    results = {}\n",
    "    flops = 2 * M * N * K\n",
    "    \n",
    "    dtypes = [\n",
    "        ('float32', torch.float32),\n",
    "        ('float16', torch.float16),\n",
    "        ('bfloat16', torch.bfloat16),\n",
    "    ]\n",
    "    \n",
    "    for name, dtype in dtypes:\n",
    "        try:\n",
    "            a = torch.randn(M, K, device='cuda', dtype=dtype)\n",
    "            b = torch.randn(K, N, device='cuda', dtype=dtype)\n",
    "            \n",
    "            ms = do_bench(lambda: torch.mm(a, b))\n",
    "            tflops = flops / (ms * 1e-3) / 1e12\n",
    "            results[name] = {'ms': ms, 'tflops': tflops}\n",
    "        except Exception as e:\n",
    "            results[name] = {'error': str(e)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Data Type Performance Comparison (4096x4096)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = compare_dtypes(4096, 4096, 4096)\n",
    "for dtype, data in results.items():\n",
    "    if 'error' in data:\n",
    "        print(f\"{dtype:<10}: Not supported\")\n",
    "    else:\n",
    "        print(f\"{dtype:<10}: {data['tflops']:.1f} TFLOPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It (30 min)\n",
    "\n",
    "### Mixed Precision Matmul\n",
    "\n",
    "Common pattern: FP16 inputs, FP32 accumulator, FP16 output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_mixed_precision(\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    M, N, K,\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, BLOCK_K: tl.constexpr,\n",
    "    OUTPUT_FP16: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Mixed precision matmul: FP16 inputs, FP32 accumulator.\"\"\"\n",
    "    pid_m = tl.program_id(0)\n",
    "    pid_n = tl.program_id(1)\n",
    "    \n",
    "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
    "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
    "    offs_k = tl.arange(0, BLOCK_K)\n",
    "    \n",
    "    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn\n",
    "    \n",
    "    # FP32 accumulator for precision\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    \n",
    "    for k in range(0, K, BLOCK_K):\n",
    "        # Load FP16 inputs\n",
    "        a = tl.load(a_ptrs, mask=(offs_m[:, None] < M) & (offs_k[None, :] + k < K), other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=(offs_k[:, None] + k < K) & (offs_n[None, :] < N), other=0.0)\n",
    "        \n",
    "        # Tensor Core MMA with FP32 accumulator\n",
    "        acc += tl.dot(a, b)\n",
    "        \n",
    "        a_ptrs += BLOCK_K * stride_ak\n",
    "        b_ptrs += BLOCK_K * stride_bk\n",
    "    \n",
    "    # Store output\n",
    "    c_ptrs = c_ptr + offs_m[:, None] * stride_cm + offs_n[None, :] * stride_cn\n",
    "    mask = (offs_m[:, None] < M) & (offs_n[None, :] < N)\n",
    "    \n",
    "    if OUTPUT_FP16:\n",
    "        # Convert to FP16 for output\n",
    "        tl.store(c_ptrs, acc.to(tl.float16), mask=mask)\n",
    "    else:\n",
    "        tl.store(c_ptrs, acc, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_mixed_precision(M, N, K):\n",
    "    \"\"\"Compare FP32 output vs FP16 output.\"\"\"\n",
    "    a = torch.randn(M, K, device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn(K, N, device='cuda', dtype=torch.float16)\n",
    "    c_fp32 = torch.empty(M, N, device='cuda', dtype=torch.float32)\n",
    "    c_fp16 = torch.empty(M, N, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    BLOCK_M, BLOCK_N, BLOCK_K = 64, 64, 32\n",
    "    grid = (triton.cdiv(M, BLOCK_M), triton.cdiv(N, BLOCK_N))\n",
    "    \n",
    "    # FP32 output\n",
    "    ms_fp32 = do_bench(lambda: matmul_mixed_precision[grid](\n",
    "        a, b, c_fp32, M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c_fp32.stride(0), c_fp32.stride(1),\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "        OUTPUT_FP16=False,\n",
    "    ))\n",
    "    \n",
    "    # FP16 output\n",
    "    ms_fp16 = do_bench(lambda: matmul_mixed_precision[grid](\n",
    "        a, b, c_fp16, M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c_fp16.stride(0), c_fp16.stride(1),\n",
    "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
    "        OUTPUT_FP16=True,\n",
    "    ))\n",
    "    \n",
    "    flops = 2 * M * N * K\n",
    "    \n",
    "    return {\n",
    "        'fp32_out_ms': ms_fp32,\n",
    "        'fp16_out_ms': ms_fp16,\n",
    "        'fp32_tflops': flops / (ms_fp32 * 1e-3) / 1e12,\n",
    "        'fp16_tflops': flops / (ms_fp16 * 1e-3) / 1e12,\n",
    "    }\n",
    "\n",
    "print(\"Mixed Precision Output Comparison\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for size in [2048, 4096]:\n",
    "    result = benchmark_mixed_precision(size, size, size)\n",
    "    print(f\"\\nSize {size}x{size}:\")\n",
    "    print(f\"  FP32 output: {result['fp32_tflops']:.1f} TFLOPS\")\n",
    "    print(f\"  FP16 output: {result['fp16_tflops']:.1f} TFLOPS\")\n",
    "    print(f\"  Speedup: {result['fp32_out_ms'] / result['fp16_out_ms']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warpgroup MMA (Hopper)\n",
    "\n",
    "Hopper introduces **Warpgroup MMA**: 128 threads (4 warps) cooperate on larger tiles.\n",
    "\n",
    "Benefits:\n",
    "- Larger tiles (64×64 or bigger)\n",
    "- Better data reuse\n",
    "- Native async execution\n",
    "\n",
    "Triton handles this automatically on Hopper when tile sizes are appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify (10 min)\n",
    "\n",
    "### Quiz\n",
    "\n",
    "**Q1:** What data types enable Tensor Cores?\n",
    "\n",
    "A) FP64 only  \n",
    "B) FP32 only  \n",
    "C) FP16, BF16, TF32, FP8  \n",
    "D) Any floating point\n",
    "\n",
    "**Q2:** Why use FP32 accumulator with FP16 inputs?\n",
    "\n",
    "A) Faster computation  \n",
    "B) Prevents precision loss during accumulation  \n",
    "C) Required by hardware  \n",
    "D) Saves memory\n",
    "\n",
    "**Q3:** What tile size constraint exists for Tensor Cores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Quiz Answers\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"Q1: C) FP16, BF16, TF32, FP8\")\n",
    "print(\"    Tensor Cores support reduced precision types.\")\n",
    "print(\"    FP64 uses different (slower) Tensor Core units on some GPUs.\")\n",
    "print()\n",
    "print(\"Q2: B) Prevents precision loss during accumulation\")\n",
    "print(\"    When adding many FP16 values, small values can be lost.\")\n",
    "print(\"    FP32 accumulator maintains precision until final output.\")\n",
    "print()\n",
    "print(\"Q3: Multiples of 16 (or 8 for some shapes)\")\n",
    "print(\"    Native MMA shapes are 16x16x16 or 16x8x16.\")\n",
    "print(\"    Tile dimensions must be multiples of these.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Tensor Cores = 66x faster** than CUDA cores for matrix math\n",
    "2. **Data type matters**: FP16/BF16/TF32/FP8 enable Tensor Cores\n",
    "3. **Tile sizes**: multiples of 16 for best performance\n",
    "4. **FP32 accumulator**: use with FP16 inputs for precision\n",
    "5. **Triton's tl.dot**: automatically uses Tensor Cores\n",
    "\n",
    "### Tensor Core Performance (H100)\n",
    "\n",
    "| Type | TFLOPS | vs FP32 CUDA |\n",
    "|------|--------|-------------|\n",
    "| FP32 (CUDA) | 67 | 1x |\n",
    "| TF32 (TC) | 495 | 7.4x |\n",
    "| FP16 (TC) | 990 | 14.8x |\n",
    "| FP8 (TC) | 1979 | 29.5x |\n",
    "\n",
    "### Tomorrow: Optimized GEMM\n",
    "\n",
    "We have all the pieces: coalescing, bank conflicts, pipelining, TMA, Tensor Cores. Tomorrow we combine everything into a production-quality GEMM kernel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
