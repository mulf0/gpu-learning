{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 5: KV Cache Optimization\n",
    "\n",
    "**Time:** ~45 minutes\n",
    "\n",
    "The KV cache is often the memory bottleneck in LLM inference. This lab explores KV cache sizing, quantization, and management strategies.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Calculate KV cache memory requirements\n",
    "2. Implement KV cache quantization\n",
    "3. Understand PagedAttention concepts\n",
    "4. Optimize for long-context inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. KV Cache Memory Calculation\n",
    "\n",
    "KV cache grows linearly with sequence length and batch size:\n",
    "\n",
    "```\n",
    "Memory = 2 × layers × heads × head_dim × seq_len × batch × bytes_per_element\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kv_cache_size(model_config, seq_len, batch_size, dtype_bytes=2):\n",
    "    \"\"\"\n",
    "    Calculate KV cache memory in bytes.\n",
    "    \n",
    "    Args:\n",
    "        model_config: dict with 'layers', 'heads', 'head_dim'\n",
    "        seq_len: sequence length\n",
    "        batch_size: batch size\n",
    "        dtype_bytes: bytes per element (2 for FP16, 1 for INT8/FP8)\n",
    "    \"\"\"\n",
    "    return (2 *  # K and V\n",
    "            model_config['layers'] *\n",
    "            model_config['heads'] *\n",
    "            model_config['head_dim'] *\n",
    "            seq_len *\n",
    "            batch_size *\n",
    "            dtype_bytes)\n",
    "\n",
    "# Common model configurations\n",
    "models = {\n",
    "    'Llama-2-7B': {'layers': 32, 'heads': 32, 'head_dim': 128},\n",
    "    'Llama-2-13B': {'layers': 40, 'heads': 40, 'head_dim': 128},\n",
    "    'Llama-2-70B': {'layers': 80, 'heads': 64, 'head_dim': 128},\n",
    "    'Llama-3-8B': {'layers': 32, 'heads': 32, 'head_dim': 128},\n",
    "    'Llama-3-70B': {'layers': 80, 'heads': 64, 'head_dim': 128},\n",
    "}\n",
    "\n",
    "print(\"KV Cache Memory Requirements (FP16, batch=1):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Model':<15} {'4K ctx':>10} {'16K ctx':>10} {'32K ctx':>10} {'128K ctx':>10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, config in models.items():\n",
    "    sizes = []\n",
    "    for ctx in [4096, 16384, 32768, 131072]:\n",
    "        size_gb = calculate_kv_cache_size(config, ctx, 1, 2) / (1024**3)\n",
    "        sizes.append(f\"{size_gb:.1f} GB\")\n",
    "    print(f\"{name:<15} {sizes[0]:>10} {sizes[1]:>10} {sizes[2]:>10} {sizes[3]:>10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. KV Cache Quantization\n",
    "\n",
    "Quantizing KV cache to INT8 or FP8 halves memory with minimal quality loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedKVCache:\n",
    "    \"\"\"KV cache with INT8 quantization.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_layers, num_heads, head_dim, max_seq_len, device='cuda'):\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.device = device\n",
    "        \n",
    "        # Quantized storage\n",
    "        self.k_cache = torch.zeros(\n",
    "            num_layers, max_seq_len, num_heads, head_dim,\n",
    "            dtype=torch.int8, device=device\n",
    "        )\n",
    "        self.v_cache = torch.zeros(\n",
    "            num_layers, max_seq_len, num_heads, head_dim,\n",
    "            dtype=torch.int8, device=device\n",
    "        )\n",
    "        \n",
    "        # Per-head scale factors\n",
    "        self.k_scales = torch.ones(\n",
    "            num_layers, max_seq_len, num_heads,\n",
    "            dtype=torch.float16, device=device\n",
    "        )\n",
    "        self.v_scales = torch.ones(\n",
    "            num_layers, max_seq_len, num_heads,\n",
    "            dtype=torch.float16, device=device\n",
    "        )\n",
    "        \n",
    "        self.seq_len = 0\n",
    "    \n",
    "    def update(self, layer_idx, k, v):\n",
    "        \"\"\"\n",
    "        Update cache with new K, V tensors.\n",
    "        k, v: [batch=1, num_heads, seq_len, head_dim]\n",
    "        \"\"\"\n",
    "        new_tokens = k.shape[2]\n",
    "        start_pos = self.seq_len\n",
    "        \n",
    "        # Quantize K\n",
    "        k_reshaped = k.squeeze(0).transpose(0, 1)  # [seq, heads, dim]\n",
    "        k_abs_max = k_reshaped.abs().max(dim=-1, keepdim=True)[0]\n",
    "        k_scale = k_abs_max / 127.0\n",
    "        k_scale = k_scale.clamp(min=1e-8)\n",
    "        k_int8 = (k_reshaped / k_scale).round().clamp(-128, 127).to(torch.int8)\n",
    "        \n",
    "        # Quantize V\n",
    "        v_reshaped = v.squeeze(0).transpose(0, 1)  # [seq, heads, dim]\n",
    "        v_abs_max = v_reshaped.abs().max(dim=-1, keepdim=True)[0]\n",
    "        v_scale = v_abs_max / 127.0\n",
    "        v_scale = v_scale.clamp(min=1e-8)\n",
    "        v_int8 = (v_reshaped / v_scale).round().clamp(-128, 127).to(torch.int8)\n",
    "        \n",
    "        # Store\n",
    "        self.k_cache[layer_idx, start_pos:start_pos+new_tokens] = k_int8\n",
    "        self.v_cache[layer_idx, start_pos:start_pos+new_tokens] = v_int8\n",
    "        self.k_scales[layer_idx, start_pos:start_pos+new_tokens] = k_scale.squeeze(-1).half()\n",
    "        self.v_scales[layer_idx, start_pos:start_pos+new_tokens] = v_scale.squeeze(-1).half()\n",
    "        \n",
    "        if layer_idx == self.num_layers - 1:\n",
    "            self.seq_len += new_tokens\n",
    "    \n",
    "    def get(self, layer_idx):\n",
    "        \"\"\"Get dequantized K, V for attention.\"\"\"\n",
    "        k_int8 = self.k_cache[layer_idx, :self.seq_len]  # [seq, heads, dim]\n",
    "        v_int8 = self.v_cache[layer_idx, :self.seq_len]\n",
    "        k_scale = self.k_scales[layer_idx, :self.seq_len].unsqueeze(-1)\n",
    "        v_scale = self.v_scales[layer_idx, :self.seq_len].unsqueeze(-1)\n",
    "        \n",
    "        k = k_int8.float() * k_scale\n",
    "        v = v_int8.float() * v_scale\n",
    "        \n",
    "        # Reshape back: [seq, heads, dim] -> [1, heads, seq, dim]\n",
    "        k = k.transpose(0, 1).unsqueeze(0)\n",
    "        v = v.transpose(0, 1).unsqueeze(0)\n",
    "        \n",
    "        return k, v\n",
    "\n",
    "# Test quantized KV cache\n",
    "cache = QuantizedKVCache(\n",
    "    num_layers=4, num_heads=8, head_dim=64,\n",
    "    max_seq_len=1024, device=device\n",
    ")\n",
    "\n",
    "# Simulate adding tokens\n",
    "for step in range(10):\n",
    "    k = torch.randn(1, 8, 1, 64, device=device)  # 1 new token\n",
    "    v = torch.randn(1, 8, 1, 64, device=device)\n",
    "    for layer in range(4):\n",
    "        cache.update(layer, k, v)\n",
    "\n",
    "# Retrieve\n",
    "k_retrieved, v_retrieved = cache.get(0)\n",
    "print(f\"Cache sequence length: {cache.seq_len}\")\n",
    "print(f\"Retrieved K shape: {k_retrieved.shape}\")\n",
    "print(f\"Memory: {cache.k_cache.numel() + cache.v_cache.numel()} bytes (INT8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. PagedAttention Concepts\n",
    "\n",
    "PagedAttention (vLLM) manages KV cache like virtual memory with fixed-size blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PagedKVCache:\n",
    "    \"\"\"Simplified PagedAttention-style KV cache.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_blocks, block_size, num_layers, num_heads, head_dim, device='cuda'):\n",
    "        self.num_blocks = num_blocks\n",
    "        self.block_size = block_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.device = device\n",
    "        \n",
    "        # Block pool\n",
    "        self.k_blocks = torch.zeros(\n",
    "            num_blocks, num_layers, block_size, num_heads, head_dim,\n",
    "            dtype=torch.float16, device=device\n",
    "        )\n",
    "        self.v_blocks = torch.zeros(\n",
    "            num_blocks, num_layers, block_size, num_heads, head_dim,\n",
    "            dtype=torch.float16, device=device\n",
    "        )\n",
    "        \n",
    "        # Block allocation tracking\n",
    "        self.free_blocks = list(range(num_blocks))\n",
    "        self.sequence_blocks = {}  # seq_id -> [block_ids]\n",
    "        self.sequence_lengths = {}  # seq_id -> length\n",
    "    \n",
    "    def allocate_sequence(self, seq_id):\n",
    "        \"\"\"Start a new sequence.\"\"\"\n",
    "        self.sequence_blocks[seq_id] = []\n",
    "        self.sequence_lengths[seq_id] = 0\n",
    "    \n",
    "    def append_token(self, seq_id, layer_idx, k, v):\n",
    "        \"\"\"Append a token to a sequence.\"\"\"\n",
    "        seq_len = self.sequence_lengths[seq_id]\n",
    "        block_idx_in_seq = seq_len // self.block_size\n",
    "        pos_in_block = seq_len % self.block_size\n",
    "        \n",
    "        # Allocate new block if needed\n",
    "        if pos_in_block == 0:\n",
    "            if not self.free_blocks:\n",
    "                raise RuntimeError(\"Out of KV cache blocks!\")\n",
    "            new_block = self.free_blocks.pop()\n",
    "            self.sequence_blocks[seq_id].append(new_block)\n",
    "        \n",
    "        # Get physical block\n",
    "        physical_block = self.sequence_blocks[seq_id][block_idx_in_seq]\n",
    "        \n",
    "        # Store KV\n",
    "        self.k_blocks[physical_block, layer_idx, pos_in_block] = k.squeeze().half()\n",
    "        self.v_blocks[physical_block, layer_idx, pos_in_block] = v.squeeze().half()\n",
    "        \n",
    "        if layer_idx == self.num_layers - 1:\n",
    "            self.sequence_lengths[seq_id] += 1\n",
    "    \n",
    "    def free_sequence(self, seq_id):\n",
    "        \"\"\"Free all blocks for a sequence.\"\"\"\n",
    "        blocks = self.sequence_blocks.pop(seq_id)\n",
    "        self.free_blocks.extend(blocks)\n",
    "        del self.sequence_lengths[seq_id]\n",
    "    \n",
    "    def memory_utilization(self):\n",
    "        \"\"\"Calculate memory utilization percentage.\"\"\"\n",
    "        used_blocks = self.num_blocks - len(self.free_blocks)\n",
    "        return used_blocks / self.num_blocks * 100\n",
    "\n",
    "# Demo PagedAttention\n",
    "paged_cache = PagedKVCache(\n",
    "    num_blocks=100, block_size=16,\n",
    "    num_layers=4, num_heads=8, head_dim=64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Simulate multiple sequences\n",
    "for seq_id in range(5):\n",
    "    paged_cache.allocate_sequence(seq_id)\n",
    "    # Each sequence has different length\n",
    "    for _ in range(np.random.randint(10, 50)):\n",
    "        k = torch.randn(1, 8, 64, device=device)\n",
    "        v = torch.randn(1, 8, 64, device=device)\n",
    "        for layer in range(4):\n",
    "            paged_cache.append_token(seq_id, layer, k, v)\n",
    "\n",
    "print(\"PagedAttention Demo:\")\n",
    "for seq_id in paged_cache.sequence_lengths:\n",
    "    blocks_used = len(paged_cache.sequence_blocks[seq_id])\n",
    "    print(f\"  Seq {seq_id}: {paged_cache.sequence_lengths[seq_id]} tokens, {blocks_used} blocks\")\n",
    "print(f\"Memory utilization: {paged_cache.memory_utilization():.1f}%\")\n",
    "print(f\"Free blocks: {len(paged_cache.free_blocks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Memory Comparison\n",
    "\n",
    "Compare different KV cache strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {'layers': 32, 'heads': 32, 'head_dim': 128}  # Llama-2-7B\n",
    "batch_size = 32\n",
    "context_length = 4096\n",
    "\n",
    "print(f\"KV Cache Memory Comparison (Llama-2-7B, batch={batch_size}, ctx={context_length}):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "strategies = [\n",
    "    (\"FP32\", 4),\n",
    "    (\"FP16\", 2),\n",
    "    (\"FP8\", 1),\n",
    "    (\"INT8\", 1),\n",
    "    (\"INT4 (experimental)\", 0.5),\n",
    "]\n",
    "\n",
    "baseline = calculate_kv_cache_size(model_config, context_length, batch_size, 2)\n",
    "\n",
    "for name, bytes_per_elem in strategies:\n",
    "    size = calculate_kv_cache_size(model_config, context_length, batch_size, bytes_per_elem)\n",
    "    reduction = baseline / size if size < baseline else 1\n",
    "    print(f\"  {name:<20}: {size / (1024**3):.2f} GB ({reduction:.1f}x vs FP16)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Prefix Caching**: Implement copy-on-write for shared prefixes between sequences\n",
    "2. **Sliding Window**: Implement sliding window attention with fixed KV cache size\n",
    "3. **Speculative Decoding**: Implement KV cache management for speculative decoding\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- KV cache dominates memory for long-context LLM inference\n",
    "- INT8/FP8 quantization halves KV cache memory with minimal quality loss\n",
    "- PagedAttention enables ~95% memory utilization vs ~50-60% traditional\n",
    "- Block-based management enables efficient multi-sequence batching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
