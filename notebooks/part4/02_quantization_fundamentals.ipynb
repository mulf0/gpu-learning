{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 2: Quantization Fundamentals\n",
    "\n",
    "**Time:** ~45 minutes\n",
    "\n",
    "Quantization maps floating-point values to lower-precision representations. This lab covers the core math: scale factors, zero points, and error bounds.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand symmetric vs asymmetric quantization\n",
    "2. Implement scale factor computation\n",
    "3. Analyze quantization error\n",
    "4. Understand per-tensor vs per-channel vs per-group scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Symmetric Quantization\n",
    "\n",
    "The simplest form: map [-max, +max] to [-127, +127] for INT8.\n",
    "\n",
    "```\n",
    "scale = max(|x|) / 127\n",
    "quantized = round(x / scale)\n",
    "dequantized = quantized * scale\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symmetric_quantize_int8(x):\n",
    "    \"\"\"Symmetric INT8 quantization.\"\"\"\n",
    "    # Compute scale factor\n",
    "    abs_max = np.abs(x).max()\n",
    "    scale = abs_max / 127.0\n",
    "    \n",
    "    # Quantize\n",
    "    x_quant = np.round(x / scale).astype(np.int8)\n",
    "    \n",
    "    return x_quant, scale\n",
    "\n",
    "def symmetric_dequantize(x_quant, scale):\n",
    "    \"\"\"Dequantize back to float.\"\"\"\n",
    "    return x_quant.astype(np.float32) * scale\n",
    "\n",
    "# Test on simulated weights\n",
    "weights = np.random.randn(1024, 1024).astype(np.float32)\n",
    "\n",
    "# Quantize and dequantize\n",
    "w_quant, scale = symmetric_quantize_int8(weights)\n",
    "w_dequant = symmetric_dequantize(w_quant, scale)\n",
    "\n",
    "# Measure error\n",
    "mse = np.mean((weights - w_dequant) ** 2)\n",
    "rel_error = np.abs(weights - w_dequant) / (np.abs(weights) + 1e-8)\n",
    "\n",
    "print(f\"Scale factor: {scale:.6f}\")\n",
    "print(f\"MSE: {mse:.8f}\")\n",
    "print(f\"Mean relative error: {rel_error.mean() * 100:.2f}%\")\n",
    "print(f\"Max relative error: {rel_error.max() * 100:.2f}%\")\n",
    "print(f\"Memory: {weights.nbytes / 1024:.1f} KB â†’ {w_quant.nbytes / 1024:.1f} KB ({weights.nbytes / w_quant.nbytes:.0f}x reduction)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Asymmetric Quantization\n",
    "\n",
    "For values that aren't centered at zero (like ReLU activations), asymmetric quantization is more efficient.\n",
    "\n",
    "```\n",
    "scale = (max - min) / 255\n",
    "zero_point = round(-min / scale)\n",
    "quantized = round(x / scale) + zero_point\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asymmetric_quantize_uint8(x):\n",
    "    \"\"\"Asymmetric UINT8 quantization with zero point.\"\"\"\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    \n",
    "    # Compute scale and zero point\n",
    "    scale = (x_max - x_min) / 255.0\n",
    "    zero_point = int(round(-x_min / scale))\n",
    "    zero_point = max(0, min(255, zero_point))  # Clamp to uint8 range\n",
    "    \n",
    "    # Quantize\n",
    "    x_quant = np.round(x / scale + zero_point).astype(np.uint8)\n",
    "    \n",
    "    return x_quant, scale, zero_point\n",
    "\n",
    "def asymmetric_dequantize(x_quant, scale, zero_point):\n",
    "    \"\"\"Dequantize back to float.\"\"\"\n",
    "    return (x_quant.astype(np.float32) - zero_point) * scale\n",
    "\n",
    "# Test on ReLU-like activations (non-negative)\n",
    "activations = np.maximum(0, np.random.randn(1024, 1024)).astype(np.float32)\n",
    "\n",
    "# Quantize both ways\n",
    "a_sym, scale_sym = symmetric_quantize_int8(activations)\n",
    "a_asym, scale_asym, zp = asymmetric_quantize_uint8(activations)\n",
    "\n",
    "# Compare errors\n",
    "a_dequant_sym = symmetric_dequantize(a_sym, scale_sym)\n",
    "a_dequant_asym = asymmetric_dequantize(a_asym, scale_asym, zp)\n",
    "\n",
    "mse_sym = np.mean((activations - a_dequant_sym) ** 2)\n",
    "mse_asym = np.mean((activations - a_dequant_asym) ** 2)\n",
    "\n",
    "print(f\"Symmetric MSE:   {mse_sym:.8f}\")\n",
    "print(f\"Asymmetric MSE:  {mse_asym:.8f}\")\n",
    "print(f\"Asymmetric is {mse_sym / mse_asym:.1f}x better for non-negative data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Per-Channel vs Per-Tensor Scaling\n",
    "\n",
    "Per-tensor uses one scale for the entire tensor (simple, fast).\n",
    "Per-channel uses one scale per output channel (better accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_channel_quantize_int8(x, axis=0):\n",
    "    \"\"\"Per-channel symmetric INT8 quantization.\"\"\"\n",
    "    # Compute scale per channel\n",
    "    abs_max = np.abs(x).max(axis=axis, keepdims=True)\n",
    "    scales = abs_max / 127.0\n",
    "    scales = np.maximum(scales, 1e-8)  # Avoid division by zero\n",
    "    \n",
    "    # Quantize\n",
    "    x_quant = np.round(x / scales).astype(np.int8)\n",
    "    \n",
    "    return x_quant, scales.squeeze()\n",
    "\n",
    "# Simulate weights with varying magnitudes per channel\n",
    "weights = np.random.randn(256, 1024).astype(np.float32)\n",
    "weights *= np.random.uniform(0.1, 10, size=(256, 1))  # Different scale per row\n",
    "\n",
    "# Compare per-tensor vs per-channel\n",
    "w_pt, scale_pt = symmetric_quantize_int8(weights)\n",
    "w_pc, scales_pc = per_channel_quantize_int8(weights, axis=1)\n",
    "\n",
    "# Dequantize\n",
    "w_dequant_pt = symmetric_dequantize(w_pt, scale_pt)\n",
    "w_dequant_pc = w_pc.astype(np.float32) * scales_pc[:, np.newaxis]\n",
    "\n",
    "mse_pt = np.mean((weights - w_dequant_pt) ** 2)\n",
    "mse_pc = np.mean((weights - w_dequant_pc) ** 2)\n",
    "\n",
    "print(f\"Per-tensor MSE:  {mse_pt:.8f}\")\n",
    "print(f\"Per-channel MSE: {mse_pc:.8f}\")\n",
    "print(f\"Per-channel is {mse_pt / mse_pc:.1f}x better for varying scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Group Quantization (Block Scaling)\n",
    "\n",
    "Modern quantization (GPTQ, AWQ, NVFP4) uses small groups (16-128 elements) for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_quantize_int4(x, group_size=128):\n",
    "    \"\"\"Group-wise INT4 quantization.\"\"\"\n",
    "    original_shape = x.shape\n",
    "    x_flat = x.reshape(-1)\n",
    "    \n",
    "    # Pad to multiple of group_size\n",
    "    pad_size = (group_size - len(x_flat) % group_size) % group_size\n",
    "    x_padded = np.pad(x_flat, (0, pad_size), mode='constant')\n",
    "    \n",
    "    # Reshape into groups\n",
    "    x_groups = x_padded.reshape(-1, group_size)\n",
    "    \n",
    "    # Compute scale per group (INT4 range: -8 to 7)\n",
    "    abs_max = np.abs(x_groups).max(axis=1, keepdims=True)\n",
    "    scales = abs_max / 7.0\n",
    "    scales = np.maximum(scales, 1e-8)\n",
    "    \n",
    "    # Quantize to INT4 range\n",
    "    x_quant = np.round(x_groups / scales).astype(np.int8)\n",
    "    x_quant = np.clip(x_quant, -8, 7)\n",
    "    \n",
    "    return x_quant, scales.squeeze(), original_shape, pad_size\n",
    "\n",
    "def group_dequantize_int4(x_quant, scales, original_shape, pad_size, group_size=128):\n",
    "    \"\"\"Dequantize group-wise INT4.\"\"\"\n",
    "    x_dequant = x_quant.astype(np.float32) * scales[:, np.newaxis]\n",
    "    x_flat = x_dequant.reshape(-1)\n",
    "    if pad_size > 0:\n",
    "        x_flat = x_flat[:-pad_size]\n",
    "    return x_flat.reshape(original_shape)\n",
    "\n",
    "# Test group quantization\n",
    "weights = np.random.randn(1024, 1024).astype(np.float32)\n",
    "\n",
    "# Different group sizes\n",
    "for group_size in [32, 64, 128]:\n",
    "    w_quant, scales, shape, pad = group_quantize_int4(weights, group_size)\n",
    "    w_dequant = group_dequantize_int4(w_quant, scales, shape, pad, group_size)\n",
    "    mse = np.mean((weights - w_dequant) ** 2)\n",
    "    num_scales = len(scales)\n",
    "    overhead = num_scales * 2 / weights.size * 100  # Assuming FP16 scales\n",
    "    print(f\"Group size {group_size:3d}: MSE = {mse:.6f}, scale overhead = {overhead:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Calibration**: Implement calibration that finds optimal scale factors using a calibration dataset\n",
    "2. **Mixed Precision**: Implement a scheme where sensitive layers use INT8 and others use INT4\n",
    "3. **Outlier Handling**: Implement SmoothQuant-style outlier migration\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Symmetric quantization is simpler; asymmetric is better for non-symmetric distributions\n",
    "- Per-channel/per-group scaling dramatically improves accuracy\n",
    "- Smaller groups = better accuracy but more scale factor overhead\n",
    "- INT4 with group_size=128 is a good balance (used by GPTQ, AWQ)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
