{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 1: FP8 Conversion\n",
    "\n",
    "**Time:** ~30 minutes\n",
    "\n",
    "FP8 (8-bit floating point) is the sweet spot for inference—half the memory of FP16 with minimal quality loss. In this lab, you'll understand the FP8 formats and implement conversion.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand FP8 E4M3 vs E5M2 formats\n",
    "2. Implement FP8 conversion manually\n",
    "3. Compare precision vs range trade-offs\n",
    "4. Use PyTorch's native FP8 support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. FP8 Format Overview\n",
    "\n",
    "FP8 comes in two variants:\n",
    "- **E4M3**: 4 exponent bits, 3 mantissa bits → more precision, less range (±448)\n",
    "- **E5M2**: 5 exponent bits, 2 mantissa bits → more range (±57344), less precision\n",
    "\n",
    "E4M3 is typically used for forward pass (activations), E5M2 for gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Check for FP8 support (requires H100/Ada or newer)\n",
    "if torch.cuda.is_available():\n",
    "    capability = torch.cuda.get_device_capability()\n",
    "    print(f\"GPU Compute Capability: {capability}\")\n",
    "    fp8_supported = capability[0] >= 9 or (capability[0] == 8 and capability[1] >= 9)\n",
    "    print(f\"Native FP8 support: {fp8_supported}\")\n",
    "else:\n",
    "    print(\"No GPU available - will use CPU emulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Manual FP8 Conversion\n",
    "\n",
    "Let's understand FP8 by implementing conversion ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_fp8_e4m3(value):\n",
    "    \"\"\"Convert float to FP8 E4M3 format (emulated).\n",
    "    \n",
    "    E4M3: 1 sign + 4 exponent + 3 mantissa bits\n",
    "    Bias: 7, Range: ±448, No infinity/NaN\n",
    "    \"\"\"\n",
    "    if value == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Extract sign\n",
    "    sign = 0 if value >= 0 else 1\n",
    "    value = abs(value)\n",
    "    \n",
    "    # Clamp to representable range\n",
    "    max_val = 448.0  # Max for E4M3\n",
    "    min_val = 2**-9  # Smallest normal\n",
    "    value = min(max(value, min_val), max_val)\n",
    "    \n",
    "    # Calculate exponent and mantissa\n",
    "    import math\n",
    "    exp = math.floor(math.log2(value))\n",
    "    exp_biased = exp + 7  # Bias is 7 for E4M3\n",
    "    exp_biased = max(0, min(15, exp_biased))  # Clamp to 4 bits\n",
    "    \n",
    "    # Calculate mantissa (3 bits = 8 values)\n",
    "    mantissa = value / (2 ** exp) - 1.0  # Normalized: 1.xxx\n",
    "    mantissa_int = int(round(mantissa * 8))  # 3 mantissa bits\n",
    "    mantissa_int = max(0, min(7, mantissa_int))\n",
    "    \n",
    "    # Pack into byte\n",
    "    fp8_byte = (sign << 7) | (exp_biased << 3) | mantissa_int\n",
    "    return fp8_byte\n",
    "\n",
    "def fp8_e4m3_to_float(fp8_byte):\n",
    "    \"\"\"Convert FP8 E4M3 back to float.\"\"\"\n",
    "    sign = (fp8_byte >> 7) & 1\n",
    "    exp = (fp8_byte >> 3) & 0xF  # 4 bits\n",
    "    mantissa = fp8_byte & 0x7     # 3 bits\n",
    "    \n",
    "    if exp == 0 and mantissa == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    # Reconstruct value\n",
    "    value = (1.0 + mantissa / 8.0) * (2 ** (exp - 7))\n",
    "    return -value if sign else value\n",
    "\n",
    "# Test conversion\n",
    "test_values = [1.0, 0.5, 2.0, 100.0, 0.125, -3.14]\n",
    "print(\"FP8 E4M3 Conversion Test:\")\n",
    "print(\"-\" * 50)\n",
    "for val in test_values:\n",
    "    fp8 = float_to_fp8_e4m3(val)\n",
    "    recovered = fp8_e4m3_to_float(fp8)\n",
    "    error = abs(val - recovered) / abs(val) * 100 if val != 0 else 0\n",
    "    print(f\"{val:>8.3f} → 0x{fp8:02X} → {recovered:>8.3f}  (error: {error:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Quantization Error Analysis\n",
    "\n",
    "Let's visualize the quantization error across the representable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test values across the FP8 range\n",
    "test_range = np.logspace(-3, 2.5, 1000)  # 0.001 to ~300\n",
    "\n",
    "errors = []\n",
    "for val in test_range:\n",
    "    fp8 = float_to_fp8_e4m3(val)\n",
    "    recovered = fp8_e4m3_to_float(fp8)\n",
    "    rel_error = abs(val - recovered) / val * 100\n",
    "    errors.append(rel_error)\n",
    "\n",
    "print(f\"Average relative error: {np.mean(errors):.2f}%\")\n",
    "print(f\"Max relative error: {np.max(errors):.2f}%\")\n",
    "print(f\"Values within 5% error: {sum(e < 5 for e in errors) / len(errors) * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. PyTorch FP8 (if available)\n",
    "\n",
    "PyTorch 2.1+ has native FP8 support on compatible hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Requires PyTorch 2.1+ and H100/Ada GPU\n",
    "try:\n",
    "    # Create test tensor\n",
    "    x = torch.randn(1024, 1024, device='cuda', dtype=torch.float16)\n",
    "    \n",
    "    # Convert to FP8 (if supported)\n",
    "    if hasattr(torch, 'float8_e4m3fn'):\n",
    "        x_fp8 = x.to(torch.float8_e4m3fn)\n",
    "        x_back = x_fp8.to(torch.float16)\n",
    "        \n",
    "        error = (x - x_back).abs().mean().item()\n",
    "        print(f\"Native FP8 conversion error: {error:.6f}\")\n",
    "    else:\n",
    "        print(\"FP8 dtype not available in this PyTorch version\")\n",
    "except Exception as e:\n",
    "    print(f\"FP8 test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **E5M2 Implementation**: Implement `float_to_fp8_e5m2()` with 5 exponent bits and 2 mantissa bits\n",
    "2. **Error Distribution**: Plot a histogram of quantization errors for random neural network weights\n",
    "3. **Scaling**: Implement per-tensor scaling to improve FP8 utilization for values outside the natural range\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- FP8 E4M3 has good precision (±6.25% max error) for values in a reasonable range\n",
    "- E4M3 is preferred for activations (better precision), E5M2 for gradients (better range)\n",
    "- Scaling factors are critical for mapping your actual value distribution to FP8's representable range"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
