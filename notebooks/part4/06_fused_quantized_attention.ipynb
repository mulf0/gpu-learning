{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 6: Fused Quantized Attention\n",
    "\n",
    "**Time:** ~45 minutes\n",
    "\n",
    "Combine quantization with fused attention kernels for maximum inference efficiency. This lab brings together concepts from attention and quantization.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand fused attention kernels\n",
    "2. Implement attention with quantized KV cache\n",
    "3. Measure memory bandwidth savings\n",
    "4. Profile and optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Standard vs Fused Attention\n",
    "\n",
    "Standard attention computes QK^T, softmax, and V multiplication separately.\n",
    "Fused attention (FlashAttention) combines these operations, reducing memory traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_attention(Q, K, V, scale):\n",
    "    \"\"\"Standard attention: high memory usage, clear implementation.\"\"\"\n",
    "    # QK^T: [batch, heads, seq_q, seq_kv]\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "    \n",
    "    # Softmax\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum with V\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def memory_efficient_attention(Q, K, V, scale, block_size=64):\n",
    "    \"\"\"\n",
    "    Memory-efficient attention using online softmax.\n",
    "    Processes K, V in blocks to reduce memory.\n",
    "    \"\"\"\n",
    "    batch, heads, seq_q, head_dim = Q.shape\n",
    "    seq_kv = K.shape[2]\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    output = torch.zeros_like(Q)\n",
    "    m = torch.full((batch, heads, seq_q, 1), float('-inf'), device=Q.device)\n",
    "    l = torch.zeros((batch, heads, seq_q, 1), device=Q.device)\n",
    "    \n",
    "    # Process K, V in blocks\n",
    "    for block_start in range(0, seq_kv, block_size):\n",
    "        block_end = min(block_start + block_size, seq_kv)\n",
    "        K_block = K[:, :, block_start:block_end, :]\n",
    "        V_block = V[:, :, block_start:block_end, :]\n",
    "        \n",
    "        # Compute scores for this block\n",
    "        scores = torch.matmul(Q, K_block.transpose(-2, -1)) * scale\n",
    "        \n",
    "        # Online softmax update\n",
    "        m_block = scores.max(dim=-1, keepdim=True)[0]\n",
    "        m_new = torch.maximum(m, m_block)\n",
    "        \n",
    "        # Rescale old accumulator\n",
    "        exp_old = torch.exp(m - m_new)\n",
    "        l = l * exp_old\n",
    "        output = output * exp_old\n",
    "        \n",
    "        # Add new block contribution\n",
    "        exp_scores = torch.exp(scores - m_new)\n",
    "        l = l + exp_scores.sum(dim=-1, keepdim=True)\n",
    "        output = output + torch.matmul(exp_scores, V_block)\n",
    "        \n",
    "        m = m_new\n",
    "    \n",
    "    # Normalize\n",
    "    output = output / l\n",
    "    return output\n",
    "\n",
    "# Compare implementations\n",
    "batch, heads, seq_len, head_dim = 1, 8, 512, 64\n",
    "Q = torch.randn(batch, heads, 1, head_dim, device=device)  # Decode: 1 query token\n",
    "K = torch.randn(batch, heads, seq_len, head_dim, device=device)\n",
    "V = torch.randn(batch, heads, seq_len, head_dim, device=device)\n",
    "scale = 1.0 / (head_dim ** 0.5)\n",
    "\n",
    "out_standard = standard_attention(Q, K, V, scale)\n",
    "out_efficient = memory_efficient_attention(Q, K, V, scale)\n",
    "\n",
    "print(f\"Output difference: {(out_standard - out_efficient).abs().max().item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Attention with Quantized KV Cache\n",
    "\n",
    "Dequantize K, V on-the-fly during attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_kv_attention(Q, K_int8, K_scales, V_int8, V_scales, scale):\n",
    "    \"\"\"\n",
    "    Attention with INT8 quantized KV cache.\n",
    "    \n",
    "    K_int8, V_int8: [batch, heads, seq_kv, head_dim] as INT8\n",
    "    K_scales, V_scales: [batch, heads, seq_kv, 1] per-token scales\n",
    "    \"\"\"\n",
    "    # Dequantize K\n",
    "    K = K_int8.float() * K_scales\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Dequantize V and compute output\n",
    "    V = V_int8.float() * V_scales\n",
    "    output = torch.matmul(attn_weights, V)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def quantize_kv(K, V):\n",
    "    \"\"\"Quantize K, V to INT8 with per-token scales.\"\"\"\n",
    "    # Quantize K\n",
    "    K_abs_max = K.abs().max(dim=-1, keepdim=True)[0]\n",
    "    K_scales = K_abs_max / 127.0\n",
    "    K_scales = K_scales.clamp(min=1e-8)\n",
    "    K_int8 = (K / K_scales).round().clamp(-128, 127).to(torch.int8)\n",
    "    \n",
    "    # Quantize V\n",
    "    V_abs_max = V.abs().max(dim=-1, keepdim=True)[0]\n",
    "    V_scales = V_abs_max / 127.0\n",
    "    V_scales = V_scales.clamp(min=1e-8)\n",
    "    V_int8 = (V / V_scales).round().clamp(-128, 127).to(torch.int8)\n",
    "    \n",
    "    return K_int8, K_scales, V_int8, V_scales\n",
    "\n",
    "# Test quantized attention\n",
    "K_int8, K_scales, V_int8, V_scales = quantize_kv(K, V)\n",
    "out_quantized = quantized_kv_attention(Q, K_int8, K_scales, V_int8, V_scales, scale)\n",
    "\n",
    "print(f\"Quantized vs Standard difference: {(out_standard - out_quantized).abs().max().item():.6f}\")\n",
    "\n",
    "# Memory comparison\n",
    "fp16_memory = K.numel() * 2 + V.numel() * 2  # FP16\n",
    "int8_memory = K_int8.numel() + V_int8.numel() + K_scales.numel() * 2 + V_scales.numel() * 2\n",
    "print(f\"Memory: {fp16_memory / 1024:.1f} KB (FP16) vs {int8_memory / 1024:.1f} KB (INT8)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Bandwidth Analysis\n",
    "\n",
    "For decode (batch=1, seq_q=1), attention is entirely memory-bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_bandwidth(seq_kv, num_heads, head_dim, dtype_bytes):\n",
    "    \"\"\"\n",
    "    Analyze memory bandwidth requirements for decode attention.\n",
    "    \n",
    "    For each generated token, we must load:\n",
    "    - All K: seq_kv × num_heads × head_dim × dtype_bytes\n",
    "    - All V: seq_kv × num_heads × head_dim × dtype_bytes\n",
    "    \"\"\"\n",
    "    kv_bytes = 2 * seq_kv * num_heads * head_dim * dtype_bytes\n",
    "    return kv_bytes\n",
    "\n",
    "# Llama-2-7B-like configuration\n",
    "num_layers = 32\n",
    "num_heads = 32\n",
    "head_dim = 128\n",
    "\n",
    "print(\"Decode Attention Memory Bandwidth Analysis (Llama-2-7B):\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Context':<10} {'FP16':>12} {'FP8/INT8':>12} {'Savings':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for ctx in [1024, 4096, 16384, 65536]:\n",
    "    bw_fp16 = analyze_bandwidth(ctx, num_heads, head_dim, 2) * num_layers\n",
    "    bw_int8 = analyze_bandwidth(ctx, num_heads, head_dim, 1) * num_layers\n",
    "    \n",
    "    print(f\"{ctx:<10} {bw_fp16 / 1024**2:>10.1f} MB {bw_int8 / 1024**2:>10.1f} MB {bw_fp16 / bw_int8:>10.1f}x\")\n",
    "\n",
    "# Estimate tokens per second\n",
    "h100_bandwidth = 3.35e12  # bytes/second\n",
    "ctx = 4096\n",
    "bw_per_token_fp16 = analyze_bandwidth(ctx, num_heads, head_dim, 2) * num_layers\n",
    "bw_per_token_int8 = analyze_bandwidth(ctx, num_heads, head_dim, 1) * num_layers\n",
    "\n",
    "print(f\"\\nEstimated decode throughput at ctx={ctx} (H100, attention only):\")\n",
    "print(f\"  FP16: {h100_bandwidth / bw_per_token_fp16:.0f} tokens/sec\")\n",
    "print(f\"  INT8: {h100_bandwidth / bw_per_token_int8:.0f} tokens/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Triton Kernel Structure (Conceptual)\n",
    "\n",
    "Here's the structure of a fused quantized attention kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudo-code for fused quantized attention kernel\n",
    "quantized_attention_kernel = '''\n",
    "@triton.jit\n",
    "def fused_quantized_attention(\n",
    "    Q_ptr, K_int8_ptr, K_scale_ptr, V_int8_ptr, V_scale_ptr, O_ptr,\n",
    "    seq_len, num_heads, head_dim,\n",
    "    BLOCK_KV: tl.constexpr, BLOCK_HEAD: tl.constexpr\n",
    "):\n",
    "    # Each program handles one query head\n",
    "    head_idx = tl.program_id(0)\n",
    "    \n",
    "    # Load query vector (fits in registers)\n",
    "    q = tl.load(Q_ptr + head_idx * head_dim + tl.arange(0, BLOCK_HEAD))\n",
    "    \n",
    "    # Initialize online softmax accumulators\n",
    "    m = float('-inf')  # Running max\n",
    "    l = 0.0           # Running sum of exp\n",
    "    acc = tl.zeros([BLOCK_HEAD], dtype=tl.float32)  # Weighted V accumulator\n",
    "    \n",
    "    # Stream through KV cache in blocks\n",
    "    for kv_start in range(0, seq_len, BLOCK_KV):\n",
    "        # Load and dequantize K block\n",
    "        k_int8 = tl.load(K_int8_ptr + ...)  # INT8 values\n",
    "        k_scale = tl.load(K_scale_ptr + ...) # Scale factors\n",
    "        k = k_int8.to(tl.float32) * k_scale\n",
    "        \n",
    "        # Compute dot products: q @ k.T\n",
    "        scores = tl.dot(q, k.T) * (1.0 / sqrt(head_dim))\n",
    "        \n",
    "        # Online softmax update\n",
    "        m_block = tl.max(scores)\n",
    "        m_new = tl.maximum(m, m_block)\n",
    "        \n",
    "        # Rescale old accumulator\n",
    "        exp_diff = tl.exp(m - m_new)\n",
    "        l = l * exp_diff\n",
    "        acc = acc * exp_diff\n",
    "        \n",
    "        # Add new block\n",
    "        exp_scores = tl.exp(scores - m_new)\n",
    "        l += tl.sum(exp_scores)\n",
    "        \n",
    "        # Load and dequantize V block, accumulate weighted sum\n",
    "        v_int8 = tl.load(V_int8_ptr + ...)\n",
    "        v_scale = tl.load(V_scale_ptr + ...)\n",
    "        v = v_int8.to(tl.float32) * v_scale\n",
    "        acc += tl.dot(exp_scores, v)\n",
    "        \n",
    "        m = m_new\n",
    "    \n",
    "    # Normalize and store\n",
    "    output = acc / l\n",
    "    tl.store(O_ptr + head_idx * head_dim + tl.arange(0, BLOCK_HEAD), output)\n",
    "'''\n",
    "\n",
    "print(\"Fused Quantized Attention Kernel Structure:\")\n",
    "print(quantized_attention_kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Implement in Triton**: Turn the pseudo-code into a working Triton kernel\n",
    "2. **FP8 Variant**: Implement FP8 E4M3 KV cache instead of INT8\n",
    "3. **Profile**: Use Nsight Compute to measure actual memory bandwidth utilization\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Decode attention is entirely memory-bound (bandwidth limited)\n",
    "- INT8/FP8 KV cache doubles effective bandwidth\n",
    "- Fused kernels minimize memory traffic by computing online softmax\n",
    "- Per-token scaling is a good balance between accuracy and overhead"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
