{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 4: NVFP4 and Microscaling Formats\n",
    "\n",
    "**Time:** ~30 minutes\n",
    "\n",
    "NVIDIA's Blackwell architecture introduces NVFP4—a 4-bit floating point format with per-block FP8 scaling. This lab explores microscaling formats and their implementation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand NVFP4 (E2M1) format\n",
    "2. Implement block scaling with FP8 scale factors\n",
    "3. Compare NVFP4 vs INT4\n",
    "4. Understand OCP Microscaling (MX) formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. FP4 E2M1 Format\n",
    "\n",
    "NVFP4 uses E2M1: 1 sign bit, 2 exponent bits, 1 mantissa bit.\n",
    "\n",
    "This gives only 16 distinct values (including ±0), but with block scaling, it works well for neural network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2M1 representable values (without scaling)\n",
    "# Exponent bias = 1, so exp range is [-1, 2] for values [0, 1, 2, 3]\n",
    "# Value = (1 + m/2) * 2^(e-1) for normalized, or (m/2) * 2^(-1) for subnormal\n",
    "\n",
    "def get_fp4_e2m1_values():\n",
    "    \"\"\"Generate all representable FP4 E2M1 values.\"\"\"\n",
    "    values = []\n",
    "    \n",
    "    for sign in [0, 1]:\n",
    "        for exp in range(4):  # 2 bits: 0-3\n",
    "            for mantissa in range(2):  # 1 bit: 0-1\n",
    "                if exp == 0:  # Subnormal\n",
    "                    value = (mantissa / 2) * (2 ** -1)\n",
    "                else:  # Normalized\n",
    "                    value = (1 + mantissa / 2) * (2 ** (exp - 1))\n",
    "                \n",
    "                if sign:\n",
    "                    value = -value\n",
    "                values.append(value)\n",
    "    \n",
    "    return sorted(set(values))\n",
    "\n",
    "fp4_values = get_fp4_e2m1_values()\n",
    "print(\"FP4 E2M1 representable values:\")\n",
    "print(fp4_values)\n",
    "print(f\"\\nRange: [{min(fp4_values)}, {max(fp4_values)}]\")\n",
    "print(f\"Unique values: {len(fp4_values)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Block Scaling for NVFP4\n",
    "\n",
    "NVFP4 uses 16-element blocks with an FP8 scale factor per block. This extends the effective range dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_nvfp4(x, block_size=16):\n",
    "    \"\"\"\n",
    "    Quantize to NVFP4 with block scaling.\n",
    "    \n",
    "    Each block of 16 values shares one FP8 scale factor.\n",
    "    Values are quantized to the nearest FP4 E2M1 representable value.\n",
    "    \"\"\"\n",
    "    original_shape = x.shape\n",
    "    x_flat = x.flatten()\n",
    "    \n",
    "    # Pad to multiple of block_size\n",
    "    pad_size = (block_size - len(x_flat) % block_size) % block_size\n",
    "    if pad_size > 0:\n",
    "        x_flat = np.pad(x_flat, (0, pad_size), mode='constant')\n",
    "    \n",
    "    # Reshape into blocks\n",
    "    x_blocks = x_flat.reshape(-1, block_size)\n",
    "    \n",
    "    # Compute scale per block (map to FP4 range of ±6)\n",
    "    abs_max = np.abs(x_blocks).max(axis=1, keepdims=True)\n",
    "    scales = abs_max / 6.0  # FP4 E2M1 max positive value\n",
    "    scales = np.maximum(scales, 1e-8)\n",
    "    \n",
    "    # Normalize values\n",
    "    x_normalized = x_blocks / scales\n",
    "    \n",
    "    # Quantize to nearest FP4 value\n",
    "    fp4_values = np.array(get_fp4_e2m1_values())\n",
    "    x_quantized = np.zeros_like(x_normalized)\n",
    "    \n",
    "    for i in range(x_normalized.shape[0]):\n",
    "        for j in range(x_normalized.shape[1]):\n",
    "            val = x_normalized[i, j]\n",
    "            # Find nearest FP4 value\n",
    "            idx = np.abs(fp4_values - val).argmin()\n",
    "            x_quantized[i, j] = fp4_values[idx]\n",
    "    \n",
    "    return x_quantized, scales.squeeze(), original_shape, pad_size\n",
    "\n",
    "def dequantize_nvfp4(x_quantized, scales, original_shape, pad_size, block_size=16):\n",
    "    \"\"\"Dequantize NVFP4.\"\"\"\n",
    "    x_dequant = x_quantized * scales[:, np.newaxis]\n",
    "    x_flat = x_dequant.flatten()\n",
    "    if pad_size > 0:\n",
    "        x_flat = x_flat[:-pad_size]\n",
    "    return x_flat.reshape(original_shape)\n",
    "\n",
    "# Test NVFP4 quantization\n",
    "weights = np.random.randn(1024, 1024).astype(np.float32)\n",
    "\n",
    "x_quant, scales, shape, pad = quantize_nvfp4(weights)\n",
    "x_dequant = dequantize_nvfp4(x_quant, scales, shape, pad)\n",
    "\n",
    "mse = np.mean((weights - x_dequant) ** 2)\n",
    "rel_error = np.abs(weights - x_dequant) / (np.abs(weights) + 1e-8)\n",
    "\n",
    "print(f\"NVFP4 Quantization Results:\")\n",
    "print(f\"  MSE: {mse:.6f}\")\n",
    "print(f\"  Mean relative error: {rel_error.mean() * 100:.2f}%\")\n",
    "print(f\"  Bits per value: 4 + {len(scales) * 8 / weights.size:.2f} (scale overhead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. NVFP4 vs INT4 Comparison\n",
    "\n",
    "FP4 has non-uniform spacing (denser near zero), which can be advantageous for weight distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_int4_grouped(x, block_size=16):\n",
    "    \"\"\"INT4 with same block size for fair comparison.\"\"\"\n",
    "    original_shape = x.shape\n",
    "    x_flat = x.flatten()\n",
    "    \n",
    "    pad_size = (block_size - len(x_flat) % block_size) % block_size\n",
    "    if pad_size > 0:\n",
    "        x_flat = np.pad(x_flat, (0, pad_size), mode='constant')\n",
    "    \n",
    "    x_blocks = x_flat.reshape(-1, block_size)\n",
    "    \n",
    "    # Scale to INT4 range [-8, 7]\n",
    "    abs_max = np.abs(x_blocks).max(axis=1, keepdims=True)\n",
    "    scales = abs_max / 7.0\n",
    "    scales = np.maximum(scales, 1e-8)\n",
    "    \n",
    "    x_quantized = np.round(x_blocks / scales).clip(-8, 7)\n",
    "    \n",
    "    return x_quantized, scales.squeeze(), original_shape, pad_size\n",
    "\n",
    "def dequantize_int4_grouped(x_quantized, scales, original_shape, pad_size):\n",
    "    x_dequant = x_quantized * scales[:, np.newaxis]\n",
    "    x_flat = x_dequant.flatten()\n",
    "    if pad_size > 0:\n",
    "        x_flat = x_flat[:-pad_size]\n",
    "    return x_flat.reshape(original_shape)\n",
    "\n",
    "# Compare on different distributions\n",
    "print(\"Comparison: NVFP4 vs INT4 (block_size=16)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "distributions = [\n",
    "    (\"Normal(0, 1)\", np.random.randn(10000)),\n",
    "    (\"Normal(0, 0.1)\", np.random.randn(10000) * 0.1),\n",
    "    (\"Uniform[-1, 1]\", np.random.uniform(-1, 1, 10000)),\n",
    "    (\"Laplace(0, 0.5)\", np.random.laplace(0, 0.5, 10000)),\n",
    "]\n",
    "\n",
    "for name, data in distributions:\n",
    "    data = data.astype(np.float32)\n",
    "    \n",
    "    # NVFP4\n",
    "    q_fp4, s_fp4, sh, p = quantize_nvfp4(data)\n",
    "    d_fp4 = dequantize_nvfp4(q_fp4, s_fp4, sh, p)\n",
    "    mse_fp4 = np.mean((data - d_fp4) ** 2)\n",
    "    \n",
    "    # INT4\n",
    "    q_int4, s_int4, sh, p = quantize_int4_grouped(data)\n",
    "    d_int4 = dequantize_int4_grouped(q_int4, s_int4, sh, p)\n",
    "    mse_int4 = np.mean((data - d_int4) ** 2)\n",
    "    \n",
    "    winner = \"NVFP4\" if mse_fp4 < mse_int4 else \"INT4\"\n",
    "    print(f\"{name:20s}: NVFP4={mse_fp4:.6f}, INT4={mse_int4:.6f} → {winner}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. OCP Microscaling (MX) Formats\n",
    "\n",
    "The Open Compute Project defines standardized microscaling formats. MXFP4 is similar to NVFP4 but uses a shared exponent within blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MX format overview\n",
    "mx_formats = {\n",
    "    \"MXFP8 E5M2\": {\"elem_bits\": 8, \"block_size\": 32, \"scale_bits\": 8, \"desc\": \"FP8 elements, shared E8M0 scale\"},\n",
    "    \"MXFP8 E4M3\": {\"elem_bits\": 8, \"block_size\": 32, \"scale_bits\": 8, \"desc\": \"FP8 elements, shared E8M0 scale\"},\n",
    "    \"MXFP6 E3M2\": {\"elem_bits\": 6, \"block_size\": 32, \"scale_bits\": 8, \"desc\": \"FP6 elements, shared E8M0 scale\"},\n",
    "    \"MXFP6 E2M3\": {\"elem_bits\": 6, \"block_size\": 32, \"scale_bits\": 8, \"desc\": \"FP6 elements, shared E8M0 scale\"},\n",
    "    \"MXFP4 E2M1\": {\"elem_bits\": 4, \"block_size\": 32, \"scale_bits\": 8, \"desc\": \"FP4 elements, shared E8M0 scale\"},\n",
    "    \"MXINT8\": {\"elem_bits\": 8, \"block_size\": 32, \"scale_bits\": 8, \"desc\": \"INT8 elements, shared E8M0 scale\"},\n",
    "}\n",
    "\n",
    "print(\"OCP Microscaling Formats:\")\n",
    "print(\"=\" * 70)\n",
    "for name, info in mx_formats.items():\n",
    "    effective_bits = info[\"elem_bits\"] + info[\"scale_bits\"] / info[\"block_size\"]\n",
    "    print(f\"{name:15s}: {info['elem_bits']}b elements, {info['block_size']}-elem blocks, \"\n",
    "          f\"~{effective_bits:.2f} effective bits/elem\")\n",
    "    print(f\"                {info['desc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **E8M0 Scale**: Implement FP8 E8M0 (exponent-only) format for scale factors\n",
    "2. **Hardware Sim**: Simulate NVFP4 Tensor Core MMA operation with proper accumulation\n",
    "3. **Quality Eval**: Compare NVFP4 vs AWQ INT4 on actual LLM weights\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- NVFP4 E2M1 has only 16 distinct values but works well with block scaling\n",
    "- Non-uniform FP4 spacing can match or beat INT4 for many distributions\n",
    "- Block size of 16-32 balances accuracy vs scale factor overhead\n",
    "- MX formats are standardized for interoperability across hardware vendors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
