{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 7: Production Integration\n",
    "\n",
    "**Time:** ~30 minutes\n",
    "\n",
    "Integrate your knowledge into production systems. This lab covers using vLLM and TensorRT-LLM for optimized inference.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Deploy models with vLLM\n",
    "2. Understand TensorRT-LLM optimization\n",
    "3. Configure quantization in production\n",
    "4. Benchmark and profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. vLLM Quick Start\n",
    "\n",
    "vLLM provides high-throughput serving with PagedAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation (uncomment to install)\n",
    "# !pip install vllm\n",
    "\n",
    "# Note: vLLM requires GPU. This cell demonstrates the API.\n",
    "vllm_example = '''\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load model with default settings\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Or with quantization\n",
    "llm_quantized = LLM(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    quantization=\"awq\",  # Options: awq, gptq, squeezellm, fp8\n",
    "    dtype=\"float16\",\n",
    "    gpu_memory_utilization=0.9,\n",
    ")\n",
    "\n",
    "# Configure sampling\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=256,\n",
    ")\n",
    "\n",
    "# Generate\n",
    "prompts = [\n",
    "    \"Explain GPU memory hierarchy in one paragraph:\",\n",
    "    \"What is FlashAttention?\",\n",
    "]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Prompt: {output.prompt}\")\n",
    "    print(f\"Output: {output.outputs[0].text}\")\n",
    "    print()\n",
    "'''\n",
    "\n",
    "print(\"vLLM Usage Example:\")\n",
    "print(vllm_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. vLLM Server Mode\n",
    "\n",
    "vLLM can run as an OpenAI-compatible API server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_commands = '''\n",
    "# Start vLLM server\n",
    "python -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\\\n",
    "    --tensor-parallel-size 1 \\\\\n",
    "    --dtype float16 \\\\\n",
    "    --port 8000\n",
    "\n",
    "# With quantization\n",
    "python -m vllm.entrypoints.openai.api_server \\\\\n",
    "    --model TheBloke/Llama-2-7B-AWQ \\\\\n",
    "    --quantization awq \\\\\n",
    "    --port 8000\n",
    "\n",
    "# Client usage (standard OpenAI API)\n",
    "curl http://localhost:8000/v1/completions \\\\\n",
    "    -H \"Content-Type: application/json\" \\\\\n",
    "    -d '{\n",
    "        \"model\": \"meta-llama/Llama-2-7b-hf\",\n",
    "        \"prompt\": \"Hello, world!\",\n",
    "        \"max_tokens\": 100,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n",
    "'''\n",
    "\n",
    "print(\"vLLM Server Commands:\")\n",
    "print(server_commands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. TensorRT-LLM Overview\n",
    "\n",
    "TensorRT-LLM provides maximum performance on NVIDIA GPUs through compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trtllm_workflow = '''\n",
    "# TensorRT-LLM Workflow:\n",
    "\n",
    "# 1. Install TensorRT-LLM\n",
    "pip install tensorrt-llm\n",
    "\n",
    "# 2. Convert model to TensorRT-LLM format\n",
    "python convert_checkpoint.py \\\\\n",
    "    --model_dir meta-llama/Llama-2-7b-hf \\\\\n",
    "    --output_dir ./llama2-7b-trtllm \\\\\n",
    "    --dtype float16\n",
    "\n",
    "# With INT8 quantization\n",
    "python convert_checkpoint.py \\\\\n",
    "    --model_dir meta-llama/Llama-2-7b-hf \\\\\n",
    "    --output_dir ./llama2-7b-int8 \\\\\n",
    "    --dtype float16 \\\\\n",
    "    --int8_kv_cache \\\\\n",
    "    --weight_only_precision int8\n",
    "\n",
    "# 3. Build TensorRT engine\n",
    "trtllm-build \\\\\n",
    "    --checkpoint_dir ./llama2-7b-trtllm \\\\\n",
    "    --output_dir ./llama2-7b-engine \\\\\n",
    "    --gemm_plugin float16 \\\\\n",
    "    --max_batch_size 64 \\\\\n",
    "    --max_input_len 2048 \\\\\n",
    "    --max_output_len 512\n",
    "\n",
    "# 4. Run inference\n",
    "python run.py \\\\\n",
    "    --engine_dir ./llama2-7b-engine \\\\\n",
    "    --input_text \"Hello, world!\"\n",
    "'''\n",
    "\n",
    "print(\"TensorRT-LLM Workflow:\")\n",
    "print(trtllm_workflow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorRT-LLM Python API example\n",
    "trtllm_python = '''\n",
    "from tensorrt_llm import LLM, SamplingParams\n",
    "\n",
    "# Load compiled engine\n",
    "llm = LLM(model=\"./llama2-7b-engine\")\n",
    "\n",
    "# Or directly from HuggingFace (auto-compile)\n",
    "llm = LLM(model=\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "# Generate with sampling\n",
    "outputs = llm.generate(\n",
    "    [\"What is the capital of France?\"],\n",
    "    sampling_params=SamplingParams(\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        max_tokens=100\n",
    "    )\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)\n",
    "'''\n",
    "\n",
    "print(\"TensorRT-LLM Python API:\")\n",
    "print(trtllm_python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Quantization Configuration\n",
    "\n",
    "Different quantization strategies for different use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_configs = {\n",
    "    \"Maximum Quality (FP16)\": {\n",
    "        \"vllm\": \"--dtype float16\",\n",
    "        \"trtllm\": \"--dtype float16\",\n",
    "        \"use_case\": \"When accuracy is critical\",\n",
    "        \"memory\": \"1x baseline\"\n",
    "    },\n",
    "    \"Balanced (FP8 KV Cache)\": {\n",
    "        \"vllm\": \"--dtype float16 --kv-cache-dtype fp8\",\n",
    "        \"trtllm\": \"--dtype float16 --fp8_kv_cache\",\n",
    "        \"use_case\": \"H100, best quality/throughput ratio\",\n",
    "        \"memory\": \"~0.75x (KV only)\"\n",
    "    },\n",
    "    \"Weight INT8 (W8A16)\": {\n",
    "        \"vllm\": \"--quantization squeezellm\",  # or load pre-quantized\n",
    "        \"trtllm\": \"--weight_only_precision int8\",\n",
    "        \"use_case\": \"Memory-constrained, good quality\",\n",
    "        \"memory\": \"~0.5x (weights only)\"\n",
    "    },\n",
    "    \"Weight INT4 (W4A16)\": {\n",
    "        \"vllm\": \"--quantization awq\",  # or gptq\n",
    "        \"trtllm\": \"--weight_only_precision int4_awq\",\n",
    "        \"use_case\": \"Maximum memory savings\",\n",
    "        \"memory\": \"~0.3x (weights only)\"\n",
    "    },\n",
    "    \"Full INT8 (W8A8)\": {\n",
    "        \"vllm\": \"N/A (not directly supported)\",\n",
    "        \"trtllm\": \"--dtype int8 --int8_kv_cache\",\n",
    "        \"use_case\": \"Maximum throughput, calibration needed\",\n",
    "        \"memory\": \"~0.5x\"\n",
    "    },\n",
    "    \"FP8 (W8A8 FP8)\": {\n",
    "        \"vllm\": \"--quantization fp8 --kv-cache-dtype fp8\",\n",
    "        \"trtllm\": \"--dtype fp8 --fp8_kv_cache\",\n",
    "        \"use_case\": \"H100/Ada, near-FP16 quality, 2x throughput\",\n",
    "        \"memory\": \"~0.5x\"\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Quantization Configuration Guide:\")\n",
    "print(\"=\" * 80)\n",
    "for name, config in quantization_configs.items():\n",
    "    print(f\"\\n{name}\")\n",
    "    print(f\"  Use case: {config['use_case']}\")\n",
    "    print(f\"  Memory:   {config['memory']}\")\n",
    "    print(f\"  vLLM:     {config['vllm']}\")\n",
    "    print(f\"  TRT-LLM:  {config['trtllm']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Benchmarking\n",
    "\n",
    "Key metrics for LLM inference performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_guide = '''\n",
    "Key Metrics for LLM Inference:\n",
    "\n",
    "1. Time to First Token (TTFT)\n",
    "   - Latency from request to first output token\n",
    "   - Critical for interactive applications\n",
    "   - Dominated by prompt processing (prefill)\n",
    "\n",
    "2. Time Per Output Token (TPOT)\n",
    "   - Latency per generated token\n",
    "   - Determines streaming speed\n",
    "   - Often memory-bandwidth bound\n",
    "\n",
    "3. Throughput (tokens/second)\n",
    "   - Total tokens generated per second\n",
    "   - Maximize with batching\n",
    "   - Trade-off with latency\n",
    "\n",
    "4. Memory Usage\n",
    "   - Model weights + KV cache + activations\n",
    "   - Limits batch size and context length\n",
    "   - Quantization reduces requirements\n",
    "\n",
    "Benchmarking Commands:\n",
    "\n",
    "# vLLM benchmark\n",
    "python benchmark_serving.py \\\\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\\\n",
    "    --num-prompts 1000 \\\\\n",
    "    --request-rate 10\n",
    "\n",
    "# TensorRT-LLM benchmark\n",
    "python benchmark.py \\\\\n",
    "    --engine_dir ./llama2-7b-engine \\\\\n",
    "    --batch_size 32 \\\\\n",
    "    --input_len 128 \\\\\n",
    "    --output_len 128\n",
    "'''\n",
    "\n",
    "print(benchmark_guide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Decision Framework\n",
    "\n",
    "Choosing the right framework and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = '''\n",
    "Framework Selection Decision Tree:\n",
    "\n",
    "1. What hardware?\n",
    "   ├── NVIDIA GPU → Continue\n",
    "   ├── AMD GPU → vLLM (ROCm support)\n",
    "   └── CPU/Edge → llama.cpp\n",
    "\n",
    "2. Priority?\n",
    "   ├── Maximum throughput → TensorRT-LLM\n",
    "   ├── Easy deployment → vLLM\n",
    "   └── Research/flexibility → vLLM or HF Transformers\n",
    "\n",
    "3. Model size vs GPU memory?\n",
    "   ├── Fits in FP16 → Start with FP16\n",
    "   ├── Tight fit → FP8 or INT8 KV cache\n",
    "   └── Doesn't fit → Weight quantization (INT4/INT8)\n",
    "\n",
    "4. Latency requirements?\n",
    "   ├── Interactive (<100ms TTFT) → Small batch, high priority\n",
    "   ├── Batch processing → Maximize batch size\n",
    "   └── Streaming → Optimize TPOT\n",
    "\n",
    "Recommended Starting Points:\n",
    "\n",
    "- Development: vLLM with default settings\n",
    "- Production (NVIDIA): TensorRT-LLM with FP8\n",
    "- Memory-constrained: vLLM with AWQ INT4\n",
    "- Multi-GPU: TensorRT-LLM with tensor parallelism\n",
    "'''\n",
    "\n",
    "print(decision_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Deploy vLLM**: Set up a vLLM server with an AWQ-quantized model\n",
    "2. **Benchmark**: Compare throughput and latency of FP16 vs INT8 vs INT4\n",
    "3. **Profile**: Use Nsight Systems to identify bottlenecks in your deployment\n",
    "\n",
    "## Course Completion\n",
    "\n",
    "Congratulations! You've completed the GPU Learning course. You now understand:\n",
    "\n",
    "- GPU architecture and execution model\n",
    "- Memory hierarchy and optimization\n",
    "- Writing kernels with Triton\n",
    "- Profiling and optimization techniques\n",
    "- Attention mechanisms and FlashAttention\n",
    "- Quantization formats and implementation\n",
    "- Production deployment with vLLM and TensorRT-LLM\n",
    "\n",
    "**Next steps**: Apply this knowledge to your own projects, contribute to open-source inference libraries, or dive deeper into specific areas like kernel development or model optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
