{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4, Lab 3: INT8 and INT4 Weight Quantization\n",
    "\n",
    "**Time:** ~45 minutes\n",
    "\n",
    "Weight-only quantization (W8A16, W4A16) is the most practical approach for memory-bound LLM inference. This lab covers implementing and evaluating INT8 and INT4 weight quantization.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Implement weight-only INT8 quantization\n",
    "2. Implement INT4 with group scaling\n",
    "3. Measure accuracy impact on a toy model\n",
    "4. Understand GPTQ and AWQ concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. INT8 Weight-Only Quantization\n",
    "\n",
    "Weights stored as INT8, dequantized to FP16 during matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinearINT8(nn.Module):\n",
    "    \"\"\"Linear layer with INT8 weight-only quantization.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        # Quantized weights (INT8) and scales (FP16 per output channel)\n",
    "        self.register_buffer('weight_int8', torch.zeros(out_features, in_features, dtype=torch.int8))\n",
    "        self.register_buffer('scales', torch.ones(out_features, dtype=torch.float16))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_float(linear_layer):\n",
    "        \"\"\"Convert a regular linear layer to quantized.\"\"\"\n",
    "        quant_layer = QuantizedLinearINT8(\n",
    "            linear_layer.in_features,\n",
    "            linear_layer.out_features,\n",
    "            bias=linear_layer.bias is not None\n",
    "        )\n",
    "        \n",
    "        # Get weights\n",
    "        weight = linear_layer.weight.data.float()\n",
    "        \n",
    "        # Per-channel quantization\n",
    "        abs_max = weight.abs().max(dim=1)[0]\n",
    "        scales = abs_max / 127.0\n",
    "        scales = scales.clamp(min=1e-8)\n",
    "        \n",
    "        # Quantize\n",
    "        weight_int8 = (weight / scales.unsqueeze(1)).round().clamp(-128, 127).to(torch.int8)\n",
    "        \n",
    "        quant_layer.weight_int8.copy_(weight_int8)\n",
    "        quant_layer.scales.copy_(scales.half())\n",
    "        \n",
    "        if linear_layer.bias is not None:\n",
    "            quant_layer.bias.data.copy_(linear_layer.bias.data)\n",
    "        \n",
    "        return quant_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Dequantize weights on-the-fly\n",
    "        weight_fp = self.weight_int8.float() * self.scales.float().unsqueeze(1)\n",
    "        return nn.functional.linear(x, weight_fp, self.bias)\n",
    "\n",
    "# Test INT8 quantization\n",
    "linear = nn.Linear(1024, 512).to(device)\n",
    "linear_int8 = QuantizedLinearINT8.from_float(linear).to(device)\n",
    "\n",
    "# Compare outputs\n",
    "x = torch.randn(32, 1024, device=device)\n",
    "y_fp32 = linear(x)\n",
    "y_int8 = linear_int8(x)\n",
    "\n",
    "error = (y_fp32 - y_int8).abs().mean().item()\n",
    "print(f\"INT8 vs FP32 output difference: {error:.6f}\")\n",
    "print(f\"Memory: {linear.weight.numel() * 4 / 1024:.1f} KB → {linear_int8.weight_int8.numel() / 1024:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. INT4 with Group Quantization\n",
    "\n",
    "INT4 requires group-wise scaling for acceptable accuracy. Typically groups of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizedLinearINT4(nn.Module):\n",
    "    \"\"\"Linear layer with INT4 weight quantization (group-wise scaling).\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, group_size=128, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.group_size = group_size\n",
    "        \n",
    "        # Calculate number of groups\n",
    "        assert in_features % group_size == 0, f\"in_features must be divisible by group_size\"\n",
    "        num_groups = in_features // group_size\n",
    "        \n",
    "        # Store as INT8 (2 INT4 values packed per byte in practice)\n",
    "        # For simplicity, we store as INT8 here\n",
    "        self.register_buffer('weight_int4', torch.zeros(out_features, in_features, dtype=torch.int8))\n",
    "        self.register_buffer('scales', torch.ones(out_features, num_groups, dtype=torch.float16))\n",
    "        \n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_float(linear_layer, group_size=128):\n",
    "        \"\"\"Convert a regular linear layer to INT4 quantized.\"\"\"\n",
    "        quant_layer = QuantizedLinearINT4(\n",
    "            linear_layer.in_features,\n",
    "            linear_layer.out_features,\n",
    "            group_size=group_size,\n",
    "            bias=linear_layer.bias is not None\n",
    "        )\n",
    "        \n",
    "        weight = linear_layer.weight.data.float()\n",
    "        out_features, in_features = weight.shape\n",
    "        \n",
    "        # Reshape for group quantization\n",
    "        weight_grouped = weight.view(out_features, -1, group_size)\n",
    "        \n",
    "        # Per-group scaling (INT4 range: -8 to 7)\n",
    "        abs_max = weight_grouped.abs().max(dim=2)[0]\n",
    "        scales = abs_max / 7.0\n",
    "        scales = scales.clamp(min=1e-8)\n",
    "        \n",
    "        # Quantize\n",
    "        weight_int4 = (weight_grouped / scales.unsqueeze(2)).round().clamp(-8, 7).to(torch.int8)\n",
    "        weight_int4 = weight_int4.view(out_features, in_features)\n",
    "        \n",
    "        quant_layer.weight_int4.copy_(weight_int4)\n",
    "        quant_layer.scales.copy_(scales.half())\n",
    "        \n",
    "        if linear_layer.bias is not None:\n",
    "            quant_layer.bias.data.copy_(linear_layer.bias.data)\n",
    "        \n",
    "        return quant_layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out_features = self.out_features\n",
    "        in_features = self.in_features\n",
    "        \n",
    "        # Dequantize: reshape, multiply by scales, reshape back\n",
    "        weight_grouped = self.weight_int4.view(out_features, -1, self.group_size).float()\n",
    "        weight_fp = weight_grouped * self.scales.float().unsqueeze(2)\n",
    "        weight_fp = weight_fp.view(out_features, in_features)\n",
    "        \n",
    "        return nn.functional.linear(x, weight_fp, self.bias)\n",
    "\n",
    "# Test INT4 quantization\n",
    "linear = nn.Linear(1024, 512).to(device)\n",
    "linear_int4 = QuantizedLinearINT4.from_float(linear, group_size=128).to(device)\n",
    "\n",
    "# Compare outputs\n",
    "x = torch.randn(32, 1024, device=device)\n",
    "y_fp32 = linear(x)\n",
    "y_int4 = linear_int4(x)\n",
    "\n",
    "error = (y_fp32 - y_int4).abs().mean().item()\n",
    "print(f\"INT4 vs FP32 output difference: {error:.6f}\")\n",
    "print(f\"Effective bits: 4 + {linear_int4.scales.numel() * 16 / linear_int4.weight_int4.numel():.2f} (scale overhead)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Accuracy Comparison on MLP\n",
    "\n",
    "Let's compare INT8 vs INT4 on a simple MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, hidden_size=1024):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size * 4)\n",
    "        self.fc2 = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.act = nn.GELU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "def quantize_mlp(mlp, quant_class, **kwargs):\n",
    "    \"\"\"Quantize all linear layers in MLP.\"\"\"\n",
    "    quant_mlp = SimpleMLP.__new__(SimpleMLP)\n",
    "    nn.Module.__init__(quant_mlp)\n",
    "    quant_mlp.fc1 = quant_class.from_float(mlp.fc1, **kwargs) if kwargs else quant_class.from_float(mlp.fc1)\n",
    "    quant_mlp.fc2 = quant_class.from_float(mlp.fc2, **kwargs) if kwargs else quant_class.from_float(mlp.fc2)\n",
    "    quant_mlp.act = mlp.act\n",
    "    return quant_mlp\n",
    "\n",
    "# Create and quantize\n",
    "mlp = SimpleMLP().to(device)\n",
    "mlp_int8 = quantize_mlp(mlp, QuantizedLinearINT8).to(device)\n",
    "mlp_int4 = quantize_mlp(mlp, QuantizedLinearINT4, group_size=128).to(device)\n",
    "\n",
    "# Test\n",
    "x = torch.randn(32, 1024, device=device)\n",
    "y_fp32 = mlp(x)\n",
    "y_int8 = mlp_int8(x)\n",
    "y_int4 = mlp_int4(x)\n",
    "\n",
    "print(\"MLP Output Comparison:\")\n",
    "print(f\"  INT8 error: {(y_fp32 - y_int8).abs().mean().item():.6f}\")\n",
    "print(f\"  INT4 error: {(y_fp32 - y_int4).abs().mean().item():.6f}\")\n",
    "print(f\"  INT4/INT8 error ratio: {(y_fp32 - y_int4).abs().mean().item() / (y_fp32 - y_int8).abs().mean().item():.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Understanding GPTQ and AWQ\n",
    "\n",
    "Production INT4 methods like GPTQ and AWQ improve on naive quantization:\n",
    "\n",
    "**GPTQ**: Uses calibration data to find optimal quantization order and adjust remaining weights to compensate for errors.\n",
    "\n",
    "**AWQ**: Identifies \"salient\" weights (important for accuracy) and protects them with higher effective precision through scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified AWQ-style importance detection\n",
    "def compute_weight_importance(weight, activations):\n",
    "    \"\"\"\n",
    "    AWQ insight: weight importance = weight magnitude × activation magnitude\n",
    "    Weights that multiply large activations are more important.\n",
    "    \"\"\"\n",
    "    # Activation statistics (mean abs value per input dimension)\n",
    "    act_scale = activations.abs().mean(dim=0)\n",
    "    \n",
    "    # Weight importance = weight × activation scale\n",
    "    importance = weight.abs() * act_scale.unsqueeze(0)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "# Demo: identify important weights\n",
    "weight = torch.randn(512, 1024)\n",
    "activations = torch.randn(1000, 1024)  # Calibration data\n",
    "\n",
    "importance = compute_weight_importance(weight, activations)\n",
    "\n",
    "# Top 1% most important weights\n",
    "threshold = importance.quantile(0.99)\n",
    "important_mask = importance > threshold\n",
    "\n",
    "print(f\"Important weights: {important_mask.sum().item()} / {importance.numel()} ({important_mask.float().mean() * 100:.1f}%)\")\n",
    "print(f\"These weights could be kept at higher precision or protected via scaling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exercises\n",
    "\n",
    "1. **Bit Packing**: Implement actual INT4 bit packing (2 values per byte)\n",
    "2. **Calibration**: Add calibration to find optimal scales using real activations\n",
    "3. **Mixed Precision**: Keep first/last layers in INT8, middle layers in INT4\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- INT8 weight-only quantization has minimal accuracy impact (~0.1% typically)\n",
    "- INT4 requires group quantization (group_size=128 is common)\n",
    "- Advanced methods (GPTQ, AWQ) use calibration to minimize quantization error\n",
    "- Weight-only quantization helps memory-bound workloads (small batch, long context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
