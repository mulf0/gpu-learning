{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 1: The Dot Product — Similarity in Code\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Understand the dot product as the foundation of attention, both mathematically and computationally.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "You have a query vector and 1000 key vectors. How do you find which keys are most \"relevant\" to the query?\n",
    "\n",
    "The answer: **dot products**. The dot product measures how much two vectors \"agree\" — it's the mathematical backbone of attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress scientific notation for readability\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "Imagine you're building an LLM. Your current token (the \"query\") needs to figure out which previous tokens (the \"keys\") are most relevant for predicting the next word.\n",
    "\n",
    "**Concrete example:** In the sentence \"The cat sat on the mat because it was tired\", when processing \"tired\", the model needs to figure out that \"it\" refers to \"cat\", not \"mat\".\n",
    "\n",
    "The dot product gives us a **similarity score** between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 4-dimensional vectors (like a tiny embedding)\n",
    "query = np.array([1.0, 0.0, 0.5, -0.5])  # Current token's query\n",
    "\n",
    "# Three candidate keys (previous tokens)\n",
    "key_cat = np.array([0.9, 0.1, 0.4, -0.6])    # \"cat\" embedding\n",
    "key_mat = np.array([-0.5, 0.8, 0.2, 0.3])   # \"mat\" embedding  \n",
    "key_the = np.array([0.1, -0.1, 0.1, 0.0])   # \"the\" embedding\n",
    "\n",
    "# Which key is most similar to our query?\n",
    "print(f\"Query · cat = {np.dot(query, key_cat):.4f}\")\n",
    "print(f\"Query · mat = {np.dot(query, key_mat):.4f}\")\n",
    "print(f\"Query · the = {np.dot(query, key_the):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** The higher the dot product, the more \"similar\" or \"relevant\" that key is to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore — The Geometry (15 min)\n",
    "\n",
    "### What does the dot product actually measure?\n",
    "\n",
    "The dot product has two equivalent definitions:\n",
    "\n",
    "**Algebraic:** Sum of element-wise products\n",
    "$$a \\cdot b = \\sum_{i=1}^{n} a_i b_i$$\n",
    "\n",
    "**Geometric:** Projection scaled by magnitudes\n",
    "$$a \\cdot b = |a| |b| \\cos(\\theta)$$\n",
    "\n",
    "where $\\theta$ is the angle between vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_dot_product_2d(a, b, title=\"Dot Product Visualization\"):\n",
    "    \"\"\"Visualize two 2D vectors and their dot product relationship.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Draw vectors from origin\n",
    "    ax.quiver(0, 0, a[0], a[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='blue', width=0.02, label=f'a = {a}')\n",
    "    ax.quiver(0, 0, b[0], b[1], angles='xy', scale_units='xy', scale=1, \n",
    "              color='red', width=0.02, label=f'b = {b}')\n",
    "    \n",
    "    # Calculate dot product and angle\n",
    "    dot = np.dot(a, b)\n",
    "    mag_a = np.linalg.norm(a)\n",
    "    mag_b = np.linalg.norm(b)\n",
    "    cos_theta = dot / (mag_a * mag_b) if mag_a * mag_b > 0 else 0\n",
    "    theta = np.arccos(np.clip(cos_theta, -1, 1))\n",
    "    \n",
    "    # Draw projection of b onto a\n",
    "    proj_scalar = dot / (mag_a ** 2) if mag_a > 0 else 0\n",
    "    proj = proj_scalar * a\n",
    "    ax.plot([b[0], proj[0]], [b[1], proj[1]], 'g--', linewidth=1.5, label='Projection')\n",
    "    ax.scatter([proj[0]], [proj[1]], color='green', s=50, zorder=5)\n",
    "    \n",
    "    # Set limits and labels\n",
    "    all_coords = np.array([a, b, [0, 0]])\n",
    "    margin = 0.5\n",
    "    ax.set_xlim(all_coords[:, 0].min() - margin, all_coords[:, 0].max() + margin)\n",
    "    ax.set_ylim(all_coords[:, 1].min() - margin, all_coords[:, 1].max() + margin)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0, color='k', linewidth=0.5)\n",
    "    ax.axvline(x=0, color='k', linewidth=0.5)\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_title(f\"{title}\\na·b = {dot:.2f}, θ = {np.degrees(theta):.1f}°, cos(θ) = {cos_theta:.2f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return dot, theta\n",
    "\n",
    "# Vectors pointing in similar directions → positive dot product\n",
    "a = np.array([2.0, 1.0])\n",
    "b = np.array([1.5, 1.5])\n",
    "visualize_dot_product_2d(a, b, \"Similar Directions (Positive Dot Product)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perpendicular vectors → zero dot product\n",
    "a = np.array([2.0, 0.0])\n",
    "b = np.array([0.0, 1.5])\n",
    "visualize_dot_product_2d(a, b, \"Perpendicular (Zero Dot Product)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opposite directions → negative dot product\n",
    "a = np.array([2.0, 0.5])\n",
    "b = np.array([-1.5, 0.0])\n",
    "visualize_dot_product_2d(a, b, \"Opposite Directions (Negative Dot Product)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Insight\n",
    "\n",
    "| Dot Product | Angle | Interpretation |\n",
    "|-------------|-------|----------------|\n",
    "| Positive (large) | 0° - 45° | Vectors point in similar directions |\n",
    "| Small positive | 45° - 90° | Weak similarity |\n",
    "| Zero | 90° | Perpendicular — no relationship |\n",
    "| Negative | 90° - 180° | Vectors point in opposite directions |\n",
    "\n",
    "In attention, **positive = relevant**, **zero = unrelated**, **negative = anti-relevant**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept — Dot Products in Attention (10 min)\n",
    "\n",
    "In transformer attention, we compute dot products between **every query** and **every key**:\n",
    "\n",
    "$$\\text{scores} = QK^T$$\n",
    "\n",
    "Where:\n",
    "- $Q$ is `[batch, seq_len, d_model]` — queries (what we're looking for)\n",
    "- $K$ is `[batch, seq_len, d_model]` — keys (what we have)\n",
    "- $QK^T$ is `[batch, seq_len, seq_len]` — attention scores\n",
    "\n",
    "### The Quadratic Problem\n",
    "\n",
    "For a sequence of length $N$ with embedding dimension $d$:\n",
    "- We compute $N^2$ dot products\n",
    "- Each dot product sums $d$ multiplications\n",
    "- Total: $O(N^2 \\cdot d)$ operations\n",
    "\n",
    "For GPT-4 with 128K context: $128000^2 = 16.4$ billion attention scores per layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute attention scores for a small example\n",
    "batch_size = 1\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "\n",
    "# Random Q and K matrices\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Compute attention scores: QK^T\n",
    "# Q: [batch, seq_len, d_model]\n",
    "# K^T: [batch, d_model, seq_len]  (transposed last two dims)\n",
    "# Result: [batch, seq_len, seq_len]\n",
    "scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"Scores shape: {scores.shape}\")\n",
    "print(f\"\\nScore matrix (first batch):\")\n",
    "print(scores[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the score matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(scores[0].numpy(), cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Attention Score')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('QK^T Attention Scores\\n(Red = positive/similar, Blue = negative/dissimilar)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Scale by √d?\n",
    "\n",
    "When we sum $d$ random products, the variance grows with $d$. If $Q$ and $K$ entries have variance 1:\n",
    "\n",
    "$$\\text{Var}(Q \\cdot K) = d \\cdot \\text{Var}(q_i) \\cdot \\text{Var}(k_i) = d$$\n",
    "\n",
    "Dividing by $\\sqrt{d}$ normalizes the variance back to 1, keeping scores in a reasonable range for softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate variance scaling\n",
    "d_values = [4, 16, 64, 256, 1024]\n",
    "\n",
    "print(\"Effect of dimension on dot product variance:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for d in d_values:\n",
    "    # Random unit-variance vectors\n",
    "    q = torch.randn(10000, d)\n",
    "    k = torch.randn(10000, d)\n",
    "    \n",
    "    # Raw dot products\n",
    "    raw_dots = (q * k).sum(dim=1)\n",
    "    \n",
    "    # Scaled dot products\n",
    "    scaled_dots = raw_dots / np.sqrt(d)\n",
    "    \n",
    "    print(f\"d={d:4d}: raw variance={raw_dots.var():.2f}, scaled variance={scaled_dots.var():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It — Efficient Dot Products (30 min)\n",
    "\n",
    "Let's implement attention score computation efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_attention_scores(Q, K):\n",
    "    \"\"\"\n",
    "    Compute attention scores using explicit loops.\n",
    "    Q: [batch, seq_len, d_model]\n",
    "    K: [batch, seq_len, d_model]\n",
    "    Returns: [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, d_model = Q.shape\n",
    "    scores = torch.zeros(batch_size, seq_len, seq_len)\n",
    "    \n",
    "    for b in range(batch_size):\n",
    "        for i in range(seq_len):  # query position\n",
    "            for j in range(seq_len):  # key position\n",
    "                # Dot product between query i and key j\n",
    "                dot = 0.0\n",
    "                for k in range(d_model):\n",
    "                    dot += Q[b, i, k] * K[b, j, k]\n",
    "                scores[b, i, j] = dot / np.sqrt(d_model)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def batched_attention_scores(Q, K):\n",
    "    \"\"\"\n",
    "    Compute attention scores using matrix multiplication.\n",
    "    This is what PyTorch/Triton actually do.\n",
    "    \"\"\"\n",
    "    d_model = Q.shape[-1]\n",
    "    # Q @ K^T computes all dot products at once\n",
    "    return torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify they produce the same result\n",
    "Q_test = torch.randn(1, 4, 8)\n",
    "K_test = torch.randn(1, 4, 8)\n",
    "\n",
    "scores_naive = naive_attention_scores(Q_test, K_test)\n",
    "scores_batched = batched_attention_scores(Q_test, K_test)\n",
    "\n",
    "print(\"Naive scores:\")\n",
    "print(scores_naive[0].numpy())\n",
    "print(\"\\nBatched scores:\")\n",
    "print(scores_batched[0].numpy())\n",
    "print(f\"\\nMax difference: {(scores_naive - scores_batched).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark: loops vs batched\n",
    "import time\n",
    "\n",
    "Q_bench = torch.randn(1, 32, 64)\n",
    "K_bench = torch.randn(1, 32, 64)\n",
    "\n",
    "# Warm up\n",
    "_ = batched_attention_scores(Q_bench, K_bench)\n",
    "\n",
    "# Time naive (small size because it's slow)\n",
    "Q_small = torch.randn(1, 16, 32)\n",
    "K_small = torch.randn(1, 16, 32)\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    _ = naive_attention_scores(Q_small, K_small)\n",
    "naive_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(10):\n",
    "    _ = batched_attention_scores(Q_small, K_small)\n",
    "batched_time = (time.perf_counter() - start) / 10\n",
    "\n",
    "print(f\"Naive: {naive_time*1000:.3f} ms\")\n",
    "print(f\"Batched: {batched_time*1000:.3f} ms\")\n",
    "print(f\"Speedup: {naive_time/batched_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement Scaled Dot-Product Attention Scores\n",
    "\n",
    "Complete the function below that:\n",
    "1. Computes Q @ K^T\n",
    "2. Scales by √d_k\n",
    "3. Applies a causal mask (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_scores(Q, K, causal=False):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention scores.\n",
    "    \n",
    "    Args:\n",
    "        Q: Query tensor [batch, seq_len, d_k]\n",
    "        K: Key tensor [batch, seq_len, d_k]\n",
    "        causal: If True, mask future positions with -inf\n",
    "    \n",
    "    Returns:\n",
    "        scores: Attention scores [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    seq_len = Q.shape[1]\n",
    "    \n",
    "    # TODO: Compute Q @ K^T\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "    \n",
    "    # TODO: Scale by sqrt(d_k)\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # TODO: Apply causal mask if requested\n",
    "    # Causal mask: position i can only attend to positions <= i\n",
    "    if causal:\n",
    "        # Create upper triangular mask (True for positions to mask)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test\n",
    "Q_test = torch.randn(1, 4, 8)\n",
    "K_test = torch.randn(1, 4, 8)\n",
    "\n",
    "print(\"Without causal mask:\")\n",
    "print(scaled_dot_product_scores(Q_test, K_test, causal=False)[0].numpy())\n",
    "\n",
    "print(\"\\nWith causal mask:\")\n",
    "print(scaled_dot_product_scores(Q_test, K_test, causal=True)[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Implementation with Triton\n",
    "\n",
    "For completeness, here's how you'd write a dot product kernel in Triton. We'll use this pattern extensively later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def dot_product_kernel(\n",
    "    a_ptr, b_ptr, output_ptr,\n",
    "    N,  # Vector length\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute dot product of two vectors a and b.\n",
    "    This kernel uses a single program to sum all elements.\n",
    "    \"\"\"\n",
    "    # Accumulator\n",
    "    acc = tl.zeros([BLOCK_SIZE], dtype=tl.float32)\n",
    "    \n",
    "    # Process in chunks of BLOCK_SIZE\n",
    "    for start in range(0, N, BLOCK_SIZE):\n",
    "        offsets = start + tl.arange(0, BLOCK_SIZE)\n",
    "        mask = offsets < N\n",
    "        \n",
    "        # Load elements\n",
    "        a_vals = tl.load(a_ptr + offsets, mask=mask, other=0.0)\n",
    "        b_vals = tl.load(b_ptr + offsets, mask=mask, other=0.0)\n",
    "        \n",
    "        # Accumulate products\n",
    "        acc += a_vals * b_vals\n",
    "    \n",
    "    # Sum across the block and store\n",
    "    result = tl.sum(acc)\n",
    "    tl.store(output_ptr, result)\n",
    "\n",
    "def triton_dot_product(a, b):\n",
    "    \"\"\"Wrapper to call the Triton dot product kernel.\"\"\"\n",
    "    assert a.shape == b.shape\n",
    "    assert a.is_cuda and b.is_cuda\n",
    "    \n",
    "    N = a.numel()\n",
    "    output = torch.zeros(1, device=a.device)\n",
    "    \n",
    "    BLOCK_SIZE = 1024\n",
    "    dot_product_kernel[(1,)](\n",
    "        a, b, output,\n",
    "        N,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    \n",
    "    return output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Triton implementation (requires GPU)\n",
    "if torch.cuda.is_available():\n",
    "    a_gpu = torch.randn(1024, device='cuda')\n",
    "    b_gpu = torch.randn(1024, device='cuda')\n",
    "    \n",
    "    triton_result = triton_dot_product(a_gpu, b_gpu)\n",
    "    torch_result = torch.dot(a_gpu, b_gpu)\n",
    "    \n",
    "    print(f\"Triton result: {triton_result:.4f}\")\n",
    "    print(f\"PyTorch result: {torch_result:.4f}\")\n",
    "    print(f\"Difference: {abs(triton_result - torch_result):.2e}\")\n",
    "else:\n",
    "    print(\"GPU not available. Triton kernel test skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Quiz & Reflection (10 min)\n",
    "\n",
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, your_answer, correct_answer):\n",
    "    if your_answer == correct_answer:\n",
    "        print(f\"✓ Correct! {question}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"✗ Incorrect. {question}\")\n",
    "        print(f\"  Your answer: {your_answer}\")\n",
    "        print(f\"  Correct answer: {correct_answer}\")\n",
    "        return False\n",
    "\n",
    "# Q1: What is the dot product of [1, 2, 3] and [4, 5, 6]?\n",
    "q1_answer = 1*4 + 2*5 + 3*6  # Replace with your calculation\n",
    "check_answer(\"[1,2,3] · [4,5,6]\", q1_answer, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: If Q has shape [2, 100, 64] and K has shape [2, 100, 64],\n",
    "# what is the shape of QK^T?\n",
    "q2_answer = (2, 100, 100)  # Replace with your answer as a tuple\n",
    "check_answer(\"Shape of QK^T\", q2_answer, (2, 100, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: For a sequence length of 4096 and d_model=512, \n",
    "# how many scalar multiplications are needed to compute QK^T?\n",
    "seq_len = 4096\n",
    "d_model = 512\n",
    "# Hint: We compute seq_len^2 dot products, each with d_model multiplications\n",
    "q3_answer = seq_len * seq_len * d_model  # Replace with your calculation\n",
    "check_answer(\"Scalar multiplications\", q3_answer, 4096 * 4096 * 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Why do we divide by sqrt(d_k)?\n",
    "# a) To make computation faster\n",
    "# b) To normalize variance of dot products\n",
    "# c) To make values positive\n",
    "# d) To reduce memory usage\n",
    "q4_answer = 'b'  # Replace with your answer\n",
    "check_answer(\"Why divide by sqrt(d_k)?\", q4_answer, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **Memory access pattern:** In the batched computation Q @ K^T, what's the memory access pattern? Is it efficient for GPU coalescing?\n",
    "\n",
    "2. **The scaling factor:** What would happen to softmax (next lesson) if we didn't scale by √d?\n",
    "\n",
    "3. **Causal masking:** Why do we mask with -∞ instead of 0?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Takeaway |\n",
    "|---------|-------------|\n",
    "| Dot product | Measures similarity: positive = similar, zero = orthogonal, negative = opposite |\n",
    "| QK^T | Computes all pairwise attention scores as a matrix multiplication |\n",
    "| √d scaling | Normalizes variance to keep softmax inputs in reasonable range |\n",
    "| Complexity | O(N² × d) — quadratic in sequence length |\n",
    "\n",
    "**Next:** The scores from QK^T can be large (positive or negative). Tomorrow we'll see how softmax converts them to probabilities — and why naive softmax breaks spectacularly.\n",
    "\n",
    "---\n",
    "\n",
    "**Interactive Reference:** [attention-math.html](../attention-math.html) Section 1 — Dot Product Calculator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
