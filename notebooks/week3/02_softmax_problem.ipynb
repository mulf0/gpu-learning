{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 2: The Softmax Problem\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Understand why naive softmax fails catastrophically and see the overflow in action.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Yesterday we computed attention scores with dot products. Now we need to convert them to **probabilities** that sum to 1.\n",
    "\n",
    "The tool: **softmax**. The problem: **your kernel will crash**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge — From Scores to Probabilities (5 min)\n",
    "\n",
    "Softmax converts arbitrary real numbers into a probability distribution:\n",
    "\n",
    "$$\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$$\n",
    "\n",
    "Properties:\n",
    "- All outputs are positive (because $e^x > 0$)\n",
    "- Outputs sum to 1\n",
    "- Larger inputs get exponentially larger shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    \"\"\"The obvious softmax implementation. What could go wrong?\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Works fine for small values\n",
    "scores_small = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "probs = naive_softmax(scores_small)\n",
    "print(f\"Scores: {scores_small}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"Sum: {probs.sum():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize softmax behavior\n",
    "def plot_softmax_effect(scores, title=\"Softmax Transformation\"):\n",
    "    probs = naive_softmax(scores)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Raw scores\n",
    "    axes[0].bar(range(len(scores)), scores, color='steelblue')\n",
    "    axes[0].set_title('Raw Scores')\n",
    "    axes[0].set_xlabel('Index')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].axhline(y=0, color='k', linewidth=0.5)\n",
    "    \n",
    "    # Probabilities\n",
    "    axes[1].bar(range(len(probs)), probs, color='coral')\n",
    "    axes[1].set_title('After Softmax')\n",
    "    axes[1].set_xlabel('Index')\n",
    "    axes[1].set_ylabel('Probability')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_softmax_effect(np.array([1.0, 2.0, 3.0, 0.5]), \"Normal Scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exponential Amplification\n",
    "\n",
    "Softmax doesn't just normalize — it **amplifies** differences exponentially.\n",
    "\n",
    "A score of 10 vs 5 becomes $e^{10}/e^5 \\approx 148×$ more weight, not $2×$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amplification effect\n",
    "scores = np.array([5.0, 10.0, 5.0, 5.0])\n",
    "probs = naive_softmax(scores)\n",
    "\n",
    "print(f\"Scores: {scores}\")\n",
    "print(f\"Score ratio (10 vs 5): {10/5:.1f}x\")\n",
    "print(f\"Probability ratio: {probs[1]/probs[0]:.1f}x\")\n",
    "\n",
    "plot_softmax_effect(scores, \"Exponential Amplification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore — When Softmax Breaks (15 min)\n",
    "\n",
    "Now let's see what happens with larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens as scores get larger?\n",
    "for max_val in [10, 50, 100, 500, 1000]:\n",
    "    scores = np.array([0.0, max_val, 0.0, 0.0])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        probs = naive_softmax(scores)\n",
    "    \n",
    "    print(f\"max_score={max_val:4d}: exp({max_val})={np.exp(max_val):.2e}, probs={probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:** At `max_score=1000`, we get `nan` (Not a Number) because:\n",
    "\n",
    "1. `exp(1000)` = $2.7 \\times 10^{434}$ — way bigger than `float64` can represent (~$10^{308}$)\n",
    "2. We get `inf` (infinity)\n",
    "3. `inf / inf = nan`\n",
    "\n",
    "### The FP16 Disaster\n",
    "\n",
    "In ML, we often use FP16 (16-bit floats) for speed. The problem is much worse there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 limits\n",
    "print(\"Float type limits:\")\n",
    "print(f\"FP16 max: {np.finfo(np.float16).max:.2e}\")\n",
    "print(f\"FP32 max: {np.finfo(np.float32).max:.2e}\")\n",
    "print(f\"FP64 max: {np.finfo(np.float64).max:.2e}\")\n",
    "\n",
    "print(f\"\\nexp() overflow threshold:\")\n",
    "print(f\"FP16: exp({np.log(65504):.1f}) = overflow\")\n",
    "print(f\"FP32: exp({np.log(3.4e38):.1f}) = overflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16 softmax failure\n",
    "def naive_softmax_fp16(x):\n",
    "    \"\"\"Softmax in FP16 — watch it fail.\"\"\"\n",
    "    x_fp16 = x.astype(np.float16)\n",
    "    exp_x = np.exp(x_fp16)\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# In FP16, we overflow much sooner\n",
    "for max_val in [5, 10, 11, 12, 15, 20]:\n",
    "    scores = np.array([0.0, max_val, 0.0, 0.0])\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        probs = naive_softmax_fp16(scores)\n",
    "    \n",
    "    exp_val = np.exp(np.float16(max_val))\n",
    "    print(f\"max_score={max_val:2d}: exp({max_val})={float(exp_val):.2e}, probs={probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FP16 breaks at just max_score=12!**\n",
    "\n",
    "In attention, dot product scores can easily exceed 12. This is a critical problem.\n",
    "\n",
    "### Realistic Attention Scores\n",
    "\n",
    "Let's see what actual attention scores look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate realistic attention scores\n",
    "seq_len = 128\n",
    "d_model = 64\n",
    "\n",
    "# Random Q and K (unit variance)\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# Attention scores (unscaled)\n",
    "scores_unscaled = Q @ K.T\n",
    "\n",
    "# Scaled by sqrt(d)\n",
    "scores_scaled = scores_unscaled / np.sqrt(d_model)\n",
    "\n",
    "print(\"Unscaled scores:\")\n",
    "print(f\"  Min: {scores_unscaled.min():.2f}\")\n",
    "print(f\"  Max: {scores_unscaled.max():.2f}\")\n",
    "print(f\"  Std: {scores_unscaled.std():.2f}\")\n",
    "\n",
    "print(\"\\nScaled scores (÷√d):\")\n",
    "print(f\"  Min: {scores_scaled.min():.2f}\")\n",
    "print(f\"  Max: {scores_scaled.max():.2f}\")\n",
    "print(f\"  Std: {scores_scaled.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of attention scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].hist(scores_unscaled.flatten(), bins=50, color='steelblue', edgecolor='black')\n",
    "axes[0].axvline(x=11, color='red', linestyle='--', label='FP16 exp() overflow')\n",
    "axes[0].set_title(f'Unscaled QK^T Scores\\n(d={d_model})')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(scores_scaled.flatten(), bins=50, color='coral', edgecolor='black')\n",
    "axes[1].axvline(x=11, color='red', linestyle='--', label='FP16 exp() overflow')\n",
    "axes[1].set_title('Scaled QK^T / √d Scores')\n",
    "axes[1].set_xlabel('Score')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Pattern of Failure\n",
    "\n",
    "1. **Long sequences** → more chances for a high dot product\n",
    "2. **Larger d_model** → scores have higher variance (before scaling)\n",
    "3. **Correlated embeddings** → systematic high scores\n",
    "4. **Layer norm effects** → can create outliers\n",
    "\n",
    "In real models, overflow is **guaranteed** without proper handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate failure in a realistic setting\n",
    "def attention_forward_naive(Q, K, V):\n",
    "    \"\"\"Naive attention implementation (will fail with large scores).\"\"\"\n",
    "    d = Q.shape[-1]\n",
    "    \n",
    "    # Compute scores\n",
    "    scores = Q @ K.T / np.sqrt(d)\n",
    "    \n",
    "    # Softmax (naive)\n",
    "    exp_scores = np.exp(scores)\n",
    "    attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Create inputs that will cause problems\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add some outliers (common in real embeddings)\n",
    "Q = np.random.randn(32, 64)\n",
    "K = np.random.randn(32, 64)\n",
    "V = np.random.randn(32, 64)\n",
    "\n",
    "# Make some Q-K pairs very similar (high attention)\n",
    "K[5] = Q[10] * 3  # This will create a score of ~3*64 = 192 (unscaled)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    output, weights = attention_forward_naive(Q, K, V)\n",
    "\n",
    "print(f\"Output contains NaN: {np.isnan(output).any()}\")\n",
    "print(f\"Weights contain NaN: {np.isnan(weights).any()}\")\n",
    "print(f\"\\nMax score: {(Q @ K.T / np.sqrt(64)).max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept — Why exp() Explodes (10 min)\n",
    "\n",
    "### Floating Point Representation\n",
    "\n",
    "A floating point number is stored as:\n",
    "\n",
    "$$\\text{value} = (-1)^s \\times 2^{e-\\text{bias}} \\times (1 + m)$$\n",
    "\n",
    "where:\n",
    "- $s$ = sign bit (0 or 1)\n",
    "- $e$ = exponent bits\n",
    "- $m$ = mantissa (fractional bits)\n",
    "- bias = offset to allow negative exponents\n",
    "\n",
    "| Format | Sign | Exponent | Mantissa | Max Value | exp() overflow |\n",
    "|--------|------|----------|----------|-----------|----------------|\n",
    "| FP16 | 1 | 5 | 10 | 65,504 | ~11 |\n",
    "| BF16 | 1 | 8 | 7 | 3.4×10³⁸ | ~88 |\n",
    "| FP32 | 1 | 8 | 23 | 3.4×10³⁸ | ~88 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the exponential function\n",
    "x = np.linspace(-5, 15, 1000)\n",
    "y = np.exp(x)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Linear scale\n",
    "axes[0].plot(x, y, 'b-', linewidth=2)\n",
    "axes[0].axhline(y=65504, color='red', linestyle='--', label='FP16 max')\n",
    "axes[0].axvline(x=11, color='red', linestyle=':', alpha=0.5)\n",
    "axes[0].set_xlabel('x')\n",
    "axes[0].set_ylabel('exp(x)')\n",
    "axes[0].set_title('exp(x) — Linear Scale')\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 100000)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log scale\n",
    "axes[1].semilogy(x, y, 'b-', linewidth=2)\n",
    "axes[1].axhline(y=65504, color='red', linestyle='--', label='FP16 max')\n",
    "axes[1].axhline(y=3.4e38, color='orange', linestyle='--', label='FP32 max')\n",
    "axes[1].set_xlabel('x')\n",
    "axes[1].set_ylabel('exp(x) [log scale]')\n",
    "axes[1].set_title('exp(x) — Log Scale')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Inf/NaN Cascade\n",
    "\n",
    "When softmax fails, here's the sequence:\n",
    "\n",
    "1. `exp(large_value)` → `inf`\n",
    "2. `sum([..., inf, ...])` → `inf`\n",
    "3. `inf / inf` → `nan`\n",
    "4. `nan` propagates through all subsequent operations\n",
    "5. Your model outputs garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace the failure step by step\n",
    "scores = np.array([1.0, 100.0, 2.0, 3.0])\n",
    "\n",
    "print(\"Step-by-step failure:\")\n",
    "print(f\"1. scores = {scores}\")\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    exp_scores = np.exp(scores)\n",
    "    print(f\"2. exp(scores) = {exp_scores}\")\n",
    "    \n",
    "    sum_exp = np.sum(exp_scores)\n",
    "    print(f\"3. sum(exp) = {sum_exp}\")\n",
    "    \n",
    "    probs = exp_scores / sum_exp\n",
    "    print(f\"4. probs = {probs}\")\n",
    "    print(f\"5. sum(probs) = {np.sum(probs)}  # Should be 1.0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What We Need\n",
    "\n",
    "A softmax implementation that:\n",
    "1. **Never overflows** — even with large inputs\n",
    "2. **Preserves accuracy** — produces correct probabilities\n",
    "3. **Works in FP16** — for efficient GPU computation\n",
    "\n",
    "Tomorrow's solution: the **max-subtraction trick**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It — Document the Problem (30 min)\n",
    "\n",
    "Let's write code that systematically identifies when softmax will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_softmax_safety(scores, dtype='float32'):\n",
    "    \"\"\"\n",
    "    Analyze whether softmax will overflow for given scores.\n",
    "    \n",
    "    Returns a dict with analysis results.\n",
    "    \"\"\"\n",
    "    if dtype == 'float16':\n",
    "        max_safe = 11.0  # exp(11) ≈ 59874, close to FP16 max of 65504\n",
    "        dtype_max = 65504.0\n",
    "    elif dtype == 'float32':\n",
    "        max_safe = 88.0  # exp(88) ≈ 1.6e38, close to FP32 max\n",
    "        dtype_max = 3.4e38\n",
    "    else:\n",
    "        max_safe = 709.0  # For float64\n",
    "        dtype_max = 1.8e308\n",
    "    \n",
    "    scores_flat = scores.flatten()\n",
    "    max_score = scores_flat.max()\n",
    "    min_score = scores_flat.min()\n",
    "    \n",
    "    will_overflow = max_score > max_safe\n",
    "    \n",
    "    # Count how many values would cause overflow\n",
    "    overflow_count = (scores_flat > max_safe).sum()\n",
    "    \n",
    "    return {\n",
    "        'dtype': dtype,\n",
    "        'max_safe_input': max_safe,\n",
    "        'dtype_max': dtype_max,\n",
    "        'scores_min': min_score,\n",
    "        'scores_max': max_score,\n",
    "        'will_overflow': will_overflow,\n",
    "        'overflow_count': overflow_count,\n",
    "        'total_values': len(scores_flat),\n",
    "    }\n",
    "\n",
    "# Test with various score distributions\n",
    "test_cases = [\n",
    "    ('Normal scores', np.random.randn(100, 100)),\n",
    "    ('Large d_model', np.random.randn(100, 100) * np.sqrt(512)),\n",
    "    ('With outliers', np.concatenate([np.random.randn(99, 100), np.ones((1, 100)) * 50])),\n",
    "]\n",
    "\n",
    "for name, scores in test_cases:\n",
    "    for dtype in ['float16', 'float32']:\n",
    "        result = analyze_softmax_safety(scores, dtype)\n",
    "        status = \"FAIL\" if result['will_overflow'] else \"OK\"\n",
    "        print(f\"{name} ({dtype}): max={result['scores_max']:.1f}, safe<{result['max_safe_input']:.0f} → {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_overflow_detection(x, dtype='float32'):\n",
    "    \"\"\"\n",
    "    Softmax that reports if overflow occurred.\n",
    "    \n",
    "    Returns: (probabilities, overflow_detected)\n",
    "    \"\"\"\n",
    "    # Check safety first\n",
    "    analysis = analyze_softmax_safety(x, dtype)\n",
    "    \n",
    "    # Compute softmax\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        exp_x = np.exp(x)\n",
    "        sum_exp = exp_x.sum(axis=-1, keepdims=True)\n",
    "        probs = exp_x / sum_exp\n",
    "    \n",
    "    # Check for actual overflow\n",
    "    has_inf = np.isinf(exp_x).any()\n",
    "    has_nan = np.isnan(probs).any()\n",
    "    \n",
    "    overflow_detected = has_inf or has_nan\n",
    "    \n",
    "    return probs, {\n",
    "        'predicted_overflow': analysis['will_overflow'],\n",
    "        'actual_overflow': overflow_detected,\n",
    "        'has_inf': has_inf,\n",
    "        'has_nan': has_nan,\n",
    "    }\n",
    "\n",
    "# Demo\n",
    "scores_safe = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "scores_dangerous = np.array([1.0, 100.0, 3.0, 4.0])\n",
    "\n",
    "probs, info = softmax_with_overflow_detection(scores_safe)\n",
    "print(f\"Safe scores: {info}\")\n",
    "\n",
    "probs, info = softmax_with_overflow_detection(scores_dangerous)\n",
    "print(f\"Dangerous scores: {info}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Find the Breaking Point\n",
    "\n",
    "Write code to find the exact threshold where softmax starts producing NaN for different dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_overflow_threshold(dtype_str):\n",
    "    \"\"\"\n",
    "    Binary search to find the exact input value where exp() overflows.\n",
    "    \"\"\"\n",
    "    if dtype_str == 'float16':\n",
    "        dtype = np.float16\n",
    "    elif dtype_str == 'float32':\n",
    "        dtype = np.float32\n",
    "    else:\n",
    "        dtype = np.float64\n",
    "    \n",
    "    low, high = 0.0, 1000.0\n",
    "    \n",
    "    while high - low > 0.01:\n",
    "        mid = (low + high) / 2\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            result = np.exp(dtype(mid))\n",
    "        \n",
    "        if np.isinf(result):\n",
    "            high = mid\n",
    "        else:\n",
    "            low = mid\n",
    "    \n",
    "    return low\n",
    "\n",
    "# Find thresholds\n",
    "for dtype in ['float16', 'float32', 'float64']:\n",
    "    threshold = find_overflow_threshold(dtype)\n",
    "    print(f\"{dtype}: exp(x) overflows at x ≈ {threshold:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create a \"stress test\" for softmax\n",
    "def softmax_stress_test(seq_lengths, d_models, num_trials=100):\n",
    "    \"\"\"\n",
    "    Test softmax failure rate across different configurations.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        for d_model in d_models:\n",
    "            failures = 0\n",
    "            max_scores = []\n",
    "            \n",
    "            for _ in range(num_trials):\n",
    "                # Generate random Q, K\n",
    "                Q = np.random.randn(seq_len, d_model)\n",
    "                K = np.random.randn(seq_len, d_model)\n",
    "                \n",
    "                # Compute scores (UNSCALED - to show the problem)\n",
    "                scores = Q @ K.T\n",
    "                max_scores.append(scores.max())\n",
    "                \n",
    "                # Check if softmax would fail in FP16\n",
    "                if scores.max() > 11:\n",
    "                    failures += 1\n",
    "            \n",
    "            results.append({\n",
    "                'seq_len': seq_len,\n",
    "                'd_model': d_model,\n",
    "                'failure_rate': failures / num_trials,\n",
    "                'avg_max_score': np.mean(max_scores),\n",
    "            })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run stress test\n",
    "results = softmax_stress_test(\n",
    "    seq_lengths=[32, 64, 128, 256],\n",
    "    d_models=[32, 64, 128],\n",
    "    num_trials=50\n",
    ")\n",
    "\n",
    "print(\"FP16 Softmax Failure Rate (UNSCALED scores):\")\n",
    "print(\"-\" * 60)\n",
    "for r in results:\n",
    "    print(f\"seq={r['seq_len']:4d}, d={r['d_model']:4d}: \"\n",
    "          f\"failure={r['failure_rate']*100:5.1f}%, avg_max={r['avg_max_score']:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Quiz & Reflection (10 min)\n",
    "\n",
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, your_answer, correct_answer):\n",
    "    if your_answer == correct_answer:\n",
    "        print(f\"✓ Correct! {question}\")\n",
    "    else:\n",
    "        print(f\"✗ Incorrect. {question}\")\n",
    "        print(f\"  Your answer: {your_answer}, Correct: {correct_answer}\")\n",
    "\n",
    "# Q1: What is exp(0)?\n",
    "q1_answer = 1  # Your answer\n",
    "check_answer(\"exp(0)\", q1_answer, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: At what approximate input value does exp() overflow in FP16?\n",
    "# a) 5\n",
    "# b) 11\n",
    "# c) 88\n",
    "# d) 709\n",
    "q2_answer = 'b'  # Your answer\n",
    "check_answer(\"FP16 exp() overflow threshold\", q2_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: If you have scores [1, 1000, 1, 1], what will naive_softmax return?\n",
    "# a) [0, 1, 0, 0]\n",
    "# b) [0.25, 0.25, 0.25, 0.25]\n",
    "# c) [nan, nan, nan, nan]\n",
    "# d) An error will be raised\n",
    "q3_answer = 'c'  # Your answer\n",
    "check_answer(\"Softmax of [1, 1000, 1, 1]\", q3_answer, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Why does scaling QK^T by 1/√d help prevent overflow?\n",
    "# a) It makes the scores smaller\n",
    "# b) It converts scores to probabilities\n",
    "# c) It normalizes the variance of scores\n",
    "# d) Both a and c\n",
    "q4_answer = 'd'  # Your answer\n",
    "check_answer(\"Why scale by 1/√d?\", q4_answer, 'd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **Why can't we just use FP64 everywhere?** (Think about memory bandwidth and Tensor Core support.)\n",
    "\n",
    "2. **The √d scaling helps but doesn't solve the problem.** Can you construct an example where scaled scores still overflow?\n",
    "\n",
    "3. **What if all scores are very negative (like -1000)?** Does that cause problems?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Problem | Cause | Impact |\n",
    "|---------|-------|--------|\n",
    "| exp() overflow | Large positive inputs | Returns inf |\n",
    "| inf/inf | Overflow in numerator and denominator | Returns nan |\n",
    "| NaN propagation | Any operation with nan produces nan | Model outputs garbage |\n",
    "\n",
    "**Key numbers to remember:**\n",
    "- FP16: exp(x) overflows at x ≈ 11\n",
    "- FP32: exp(x) overflows at x ≈ 88\n",
    "\n",
    "**Tomorrow:** The solution — stable softmax using the max-subtraction trick.\n",
    "\n",
    "---\n",
    "\n",
    "**Interactive Reference:** [attention-math.html](../attention-math.html) Section 2 — Softmax Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
