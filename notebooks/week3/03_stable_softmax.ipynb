{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 3: Stable Softmax\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Implement numerically stable softmax using the max-subtraction trick.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Yesterday we saw softmax explode with `nan`. Today we fix it with a simple mathematical trick that doesn't change the answer but prevents overflow.\n",
    "\n",
    "**The insight:** softmax(x) = softmax(x - c) for any constant c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "torch.set_printoptions(precision=6, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge (5 min)\n",
    "\n",
    "Recall our broken softmax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_softmax(x):\n",
    "    \"\"\"Broken softmax that overflows.\"\"\"\n",
    "    exp_x = np.exp(x)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# This fails\n",
    "scores = np.array([1.0, 100.0, 2.0, 3.0])\n",
    "print(f\"Naive softmax of {scores}:\")\n",
    "print(naive_softmax(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Make this work without changing the mathematical result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore — The Invariance Property (15 min)\n",
    "\n",
    "### The Key Mathematical Insight\n",
    "\n",
    "For any constant $c$:\n",
    "\n",
    "$$\\text{softmax}(x_i - c) = \\frac{e^{x_i - c}}{\\sum_j e^{x_j - c}} = \\frac{e^{x_i} \\cdot e^{-c}}{\\sum_j e^{x_j} \\cdot e^{-c}} = \\frac{e^{x_i}}{\\sum_j e^{x_j}} = \\text{softmax}(x_i)$$\n",
    "\n",
    "The $e^{-c}$ cancels out! This is called **shift invariance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate shift invariance\n",
    "x = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "# Original softmax\n",
    "probs_original = naive_softmax(x)\n",
    "\n",
    "# Shifted by various constants\n",
    "for c in [0, 5, -3, 100, -100]:\n",
    "    probs_shifted = naive_softmax(x - c)\n",
    "    max_diff = np.abs(probs_original - probs_shifted).max()\n",
    "    print(f\"c={c:4d}: probs={probs_shifted}, max_diff={max_diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Optimal Shift: max(x)\n",
    "\n",
    "If we subtract $\\max(x)$:\n",
    "- The largest value becomes 0\n",
    "- All other values become negative\n",
    "- $e^{\\text{negative}}$ is always in $(0, 1]$ — no overflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax(x):\n",
    "    \"\"\"Numerically stable softmax.\"\"\"\n",
    "    # Subtract max for numerical stability\n",
    "    x_shifted = x - np.max(x, axis=-1, keepdims=True)\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Now it works!\n",
    "scores = np.array([1.0, 100.0, 2.0, 3.0])\n",
    "print(f\"Stable softmax of {scores}:\")\n",
    "print(stable_softmax(scores))\n",
    "\n",
    "# Even extreme values work\n",
    "extreme_scores = np.array([1.0, 1000.0, 2.0, 3.0])\n",
    "print(f\"\\nStable softmax of {extreme_scores}:\")\n",
    "print(stable_softmax(extreme_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-by-step visualization\n",
    "def visualize_stable_softmax(x):\n",
    "    \"\"\"Show each step of stable softmax.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Input: {x}\")\n",
    "    \n",
    "    # Step 1: Find max\n",
    "    max_x = np.max(x)\n",
    "    print(f\"\\nStep 1: max(x) = {max_x}\")\n",
    "    \n",
    "    # Step 2: Subtract max\n",
    "    x_shifted = x - max_x\n",
    "    print(f\"Step 2: x - max = {x_shifted}\")\n",
    "    \n",
    "    # Step 3: Compute exp\n",
    "    exp_x = np.exp(x_shifted)\n",
    "    print(f\"Step 3: exp(x - max) = {exp_x}\")\n",
    "    \n",
    "    # Step 4: Sum\n",
    "    sum_exp = np.sum(exp_x)\n",
    "    print(f\"Step 4: sum = {sum_exp}\")\n",
    "    \n",
    "    # Step 5: Divide\n",
    "    probs = exp_x / sum_exp\n",
    "    print(f\"Step 5: probs = {probs}\")\n",
    "    print(f\"        sum(probs) = {np.sum(probs)}\")\n",
    "    \n",
    "    return probs\n",
    "\n",
    "visualize_stable_softmax(np.array([1.0, 100.0, 2.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What About Underflow?\n",
    "\n",
    "After subtracting max, small values become very negative. $e^{-1000} \\approx 0$ — this is **underflow**.\n",
    "\n",
    "But underflow is benign! A probability of 0 (or very close) is mathematically correct for tokens that are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underflow example\n",
    "scores = np.array([0.0, 100.0, -500.0, -1000.0])\n",
    "probs = stable_softmax(scores)\n",
    "\n",
    "print(f\"Scores: {scores}\")\n",
    "print(f\"After shifting: {scores - scores.max()}\")\n",
    "print(f\"Probabilities: {probs}\")\n",
    "print(f\"\\nThe -1000 score gets probability ≈ 0, which is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept — Mathematical Proof (10 min)\n",
    "\n",
    "Let's prove the shift invariance formally.\n",
    "\n",
    "### Theorem: softmax(x - c) = softmax(x)\n",
    "\n",
    "**Proof:**\n",
    "\n",
    "$$\\text{softmax}(x_i - c) = \\frac{e^{x_i - c}}{\\sum_{j=1}^{n} e^{x_j - c}}$$\n",
    "\n",
    "Using the property $e^{a-b} = e^a \\cdot e^{-b}$:\n",
    "\n",
    "$$= \\frac{e^{x_i} \\cdot e^{-c}}{\\sum_{j=1}^{n} e^{x_j} \\cdot e^{-c}}$$\n",
    "\n",
    "Factor out $e^{-c}$ from the sum:\n",
    "\n",
    "$$= \\frac{e^{x_i} \\cdot e^{-c}}{e^{-c} \\cdot \\sum_{j=1}^{n} e^{x_j}}$$\n",
    "\n",
    "Cancel $e^{-c}$:\n",
    "\n",
    "$$= \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}} = \\text{softmax}(x_i) \\quad \\square$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical verification of the proof\n",
    "np.random.seed(42)\n",
    "x = np.random.randn(1000)  # 1000 random values\n",
    "\n",
    "# Test with many different shift values\n",
    "probs_original = stable_softmax(x)\n",
    "\n",
    "max_errors = []\n",
    "for c in np.linspace(-1000, 1000, 100):\n",
    "    probs_shifted = stable_softmax(x - c)\n",
    "    max_error = np.abs(probs_original - probs_shifted).max()\n",
    "    max_errors.append(max_error)\n",
    "\n",
    "print(f\"Max error across 100 different shifts: {max(max_errors):.2e}\")\n",
    "print(\"(Should be essentially 0, limited by floating point precision)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Sum-Exp Trick\n",
    "\n",
    "A related technique is computing log(softmax) stably:\n",
    "\n",
    "$$\\log(\\text{softmax}(x_i)) = x_i - \\log\\sum_j e^{x_j}$$\n",
    "\n",
    "The log-sum-exp (LSE) can be computed stably:\n",
    "\n",
    "$$\\text{LSE}(x) = \\max(x) + \\log\\sum_j e^{x_j - \\max(x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_sum_exp(x):\n",
    "    \"\"\"Numerically stable log-sum-exp.\"\"\"\n",
    "    max_x = np.max(x, axis=-1, keepdims=True)\n",
    "    return max_x + np.log(np.sum(np.exp(x - max_x), axis=-1, keepdims=True))\n",
    "\n",
    "def log_softmax(x):\n",
    "    \"\"\"Numerically stable log-softmax.\"\"\"\n",
    "    return x - log_sum_exp(x)\n",
    "\n",
    "# Compare\n",
    "x = np.array([1.0, 100.0, 2.0, 3.0])\n",
    "\n",
    "# Method 1: log of stable softmax\n",
    "log_probs_v1 = np.log(stable_softmax(x))\n",
    "\n",
    "# Method 2: log-softmax directly\n",
    "log_probs_v2 = log_softmax(x)\n",
    "\n",
    "print(f\"log(stable_softmax): {log_probs_v1.flatten()}\")\n",
    "print(f\"log_softmax:         {log_probs_v2.flatten()}\")\n",
    "print(f\"Max difference: {np.abs(log_probs_v1 - log_probs_v2).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It — Efficient Implementation (30 min)\n",
    "\n",
    "### PyTorch Comparison\n",
    "\n",
    "Let's verify our implementation matches PyTorch's built-in softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_softmax_torch(x):\n",
    "    \"\"\"Our implementation in PyTorch.\"\"\"\n",
    "    max_x = x.max(dim=-1, keepdim=True).values\n",
    "    exp_x = torch.exp(x - max_x)\n",
    "    return exp_x / exp_x.sum(dim=-1, keepdim=True)\n",
    "\n",
    "# Test against PyTorch's implementation\n",
    "x = torch.randn(100, 50)\n",
    "\n",
    "our_result = stable_softmax_torch(x)\n",
    "pytorch_result = torch.softmax(x, dim=-1)\n",
    "\n",
    "max_diff = (our_result - pytorch_result).abs().max()\n",
    "print(f\"Max difference from torch.softmax: {max_diff:.2e}\")\n",
    "\n",
    "# Verify properties\n",
    "print(f\"All probabilities positive: {(our_result >= 0).all()}\")\n",
    "print(f\"Rows sum to 1: {our_result.sum(dim=-1).allclose(torch.ones(100))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triton Implementation\n",
    "\n",
    "For GPU, we implement softmax in Triton. The key is computing max and sum in a single pass through memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def softmax_kernel(\n",
    "    input_ptr, output_ptr,\n",
    "    n_cols,\n",
    "    input_row_stride, output_row_stride,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Stable softmax kernel.\n",
    "    Each program handles one row of the input.\n",
    "    \"\"\"\n",
    "    # Get row index\n",
    "    row_idx = tl.program_id(0)\n",
    "    \n",
    "    # Pointers to the start of this row\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    \n",
    "    # Load the row in chunks of BLOCK_SIZE\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # First pass: find max\n",
    "    row_max = float('-inf')\n",
    "    for start in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = start + col_offsets\n",
    "        mask = offsets < n_cols\n",
    "        row_vals = tl.load(row_start_ptr + offsets, mask=mask, other=float('-inf'))\n",
    "        row_max = tl.maximum(row_max, tl.max(row_vals, axis=0))\n",
    "    \n",
    "    # Second pass: compute exp(x - max) and sum\n",
    "    row_sum = 0.0\n",
    "    for start in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = start + col_offsets\n",
    "        mask = offsets < n_cols\n",
    "        row_vals = tl.load(row_start_ptr + offsets, mask=mask, other=float('-inf'))\n",
    "        row_vals = tl.exp(row_vals - row_max)\n",
    "        row_sum += tl.sum(row_vals, axis=0)\n",
    "    \n",
    "    # Third pass: normalize and store\n",
    "    out_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    for start in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = start + col_offsets\n",
    "        mask = offsets < n_cols\n",
    "        row_vals = tl.load(row_start_ptr + offsets, mask=mask, other=float('-inf'))\n",
    "        softmax_vals = tl.exp(row_vals - row_max) / row_sum\n",
    "        tl.store(out_row_start_ptr + offsets, softmax_vals, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triton_softmax(x):\n",
    "    \"\"\"Wrapper for Triton softmax kernel.\"\"\"\n",
    "    n_rows, n_cols = x.shape\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    # Choose BLOCK_SIZE based on n_cols\n",
    "    BLOCK_SIZE = triton.next_power_of_2(n_cols)\n",
    "    BLOCK_SIZE = min(BLOCK_SIZE, 1024)  # Cap at 1024\n",
    "    \n",
    "    # Launch kernel with one program per row\n",
    "    softmax_kernel[(n_rows,)](\n",
    "        x, output,\n",
    "        n_cols,\n",
    "        x.stride(0), output.stride(0),\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test on GPU\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = torch.randn(128, 256, device='cuda')\n",
    "    \n",
    "    triton_result = triton_softmax(x_gpu)\n",
    "    pytorch_result = torch.softmax(x_gpu, dim=-1)\n",
    "    \n",
    "    max_diff = (triton_result - pytorch_result).abs().max()\n",
    "    print(f\"Max diff from PyTorch: {max_diff:.2e}\")\n",
    "    print(f\"Rows sum to 1: {triton_result.sum(dim=-1).allclose(torch.ones(128, device='cuda'))}\")\n",
    "else:\n",
    "    print(\"GPU not available. Triton test skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Two-Pass vs Three-Pass\n",
    "\n",
    "Our kernel makes 3 passes over the data. Can we do better?\n",
    "\n",
    "**Challenge:** Implement a 2-pass softmax that computes max and sum together, then normalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_kernel_2pass(\n",
    "    input_ptr, output_ptr,\n",
    "    n_cols,\n",
    "    input_row_stride, output_row_stride,\n",
    "    BLOCK_SIZE: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    Two-pass softmax: \n",
    "    Pass 1: Compute max AND partial exp-sums\n",
    "    Pass 2: Normalize\n",
    "    \n",
    "    Uses online algorithm to update sum when max changes.\n",
    "    \"\"\"\n",
    "    row_idx = tl.program_id(0)\n",
    "    row_start_ptr = input_ptr + row_idx * input_row_stride\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    # Pass 1: Online max and sum computation\n",
    "    running_max = float('-inf')\n",
    "    running_sum = 0.0\n",
    "    \n",
    "    for start in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = start + col_offsets\n",
    "        mask = offsets < n_cols\n",
    "        row_vals = tl.load(row_start_ptr + offsets, mask=mask, other=float('-inf'))\n",
    "        \n",
    "        # Find block max\n",
    "        block_max = tl.max(row_vals, axis=0)\n",
    "        \n",
    "        # Update running max and rescale running sum\n",
    "        new_max = tl.maximum(running_max, block_max)\n",
    "        \n",
    "        # Rescale old sum to new max\n",
    "        running_sum = running_sum * tl.exp(running_max - new_max)\n",
    "        \n",
    "        # Add new block contribution\n",
    "        running_sum += tl.sum(tl.where(mask, tl.exp(row_vals - new_max), 0.0), axis=0)\n",
    "        \n",
    "        running_max = new_max\n",
    "    \n",
    "    # Pass 2: Normalize and store\n",
    "    out_row_start_ptr = output_ptr + row_idx * output_row_stride\n",
    "    for start in range(0, n_cols, BLOCK_SIZE):\n",
    "        offsets = start + col_offsets\n",
    "        mask = offsets < n_cols\n",
    "        row_vals = tl.load(row_start_ptr + offsets, mask=mask, other=float('-inf'))\n",
    "        softmax_vals = tl.exp(row_vals - running_max) / running_sum\n",
    "        tl.store(out_row_start_ptr + offsets, softmax_vals, mask=mask)\n",
    "\n",
    "def triton_softmax_2pass(x):\n",
    "    \"\"\"Wrapper for 2-pass Triton softmax.\"\"\"\n",
    "    n_rows, n_cols = x.shape\n",
    "    output = torch.empty_like(x)\n",
    "    \n",
    "    BLOCK_SIZE = min(triton.next_power_of_2(n_cols), 1024)\n",
    "    \n",
    "    softmax_kernel_2pass[(n_rows,)](\n",
    "        x, output,\n",
    "        n_cols,\n",
    "        x.stride(0), output.stride(0),\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Test\n",
    "if torch.cuda.is_available():\n",
    "    x_gpu = torch.randn(128, 256, device='cuda')\n",
    "    \n",
    "    result_2pass = triton_softmax_2pass(x_gpu)\n",
    "    result_pytorch = torch.softmax(x_gpu, dim=-1)\n",
    "    \n",
    "    print(f\"2-pass max diff: {(result_2pass - result_pytorch).abs().max():.2e}\")\n",
    "else:\n",
    "    print(\"GPU not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Benchmark different implementations\n",
    "    sizes = [(128, 128), (256, 512), (512, 1024), (1024, 2048)]\n",
    "    \n",
    "    print(f\"{'Size':>15} {'PyTorch':>12} {'Triton 3-pass':>15} {'Triton 2-pass':>15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for n_rows, n_cols in sizes:\n",
    "        x = torch.randn(n_rows, n_cols, device='cuda')\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = torch.softmax(x, dim=-1)\n",
    "            _ = triton_softmax(x)\n",
    "            _ = triton_softmax_2pass(x)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # PyTorch\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            _ = torch.softmax(x, dim=-1)\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_time = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        # Triton 3-pass\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            _ = triton_softmax(x)\n",
    "        torch.cuda.synchronize()\n",
    "        triton3_time = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        # Triton 2-pass\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(100):\n",
    "            _ = triton_softmax_2pass(x)\n",
    "        torch.cuda.synchronize()\n",
    "        triton2_time = (time.perf_counter() - start) / 100 * 1000\n",
    "        \n",
    "        print(f\"{n_rows}x{n_cols:>4}: {pytorch_time:>10.3f}ms {triton3_time:>13.3f}ms {triton2_time:>13.3f}ms\")\n",
    "else:\n",
    "    print(\"GPU not available for benchmarking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Quiz & Reflection (10 min)\n",
    "\n",
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, your_answer, correct_answer):\n",
    "    if your_answer == correct_answer:\n",
    "        print(f\"✓ Correct! {question}\")\n",
    "    else:\n",
    "        print(f\"✗ Incorrect. {question}\")\n",
    "        print(f\"  Your answer: {your_answer}, Correct: {correct_answer}\")\n",
    "\n",
    "# Q1: What is the key property that makes stable softmax work?\n",
    "# a) exp(x) is always positive\n",
    "# b) softmax(x - c) = softmax(x) for any constant c\n",
    "# c) max(x) is always finite\n",
    "# d) Division distributes over addition\n",
    "q1_answer = 'b'\n",
    "check_answer(\"Key property\", q1_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: After subtracting max(x), the largest value in x becomes:\n",
    "# a) 1\n",
    "# b) max(x)\n",
    "# c) 0\n",
    "# d) -max(x)\n",
    "q2_answer = 'c'\n",
    "check_answer(\"Largest value after shift\", q2_answer, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: stable_softmax([0, -1000, -2000]) produces approximately:\n",
    "# a) [1/3, 1/3, 1/3]\n",
    "# b) [1, 0, 0]\n",
    "# c) [nan, nan, nan]\n",
    "# d) [0.33, 0.33, 0.33]\n",
    "\n",
    "# Let's verify:\n",
    "result = stable_softmax(np.array([0.0, -1000.0, -2000.0]))\n",
    "print(f\"stable_softmax([0, -1000, -2000]) = {result}\")\n",
    "\n",
    "q3_answer = 'b'\n",
    "check_answer(\"Softmax of [0, -1000, -2000]\", q3_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: How many passes over the data does the 2-pass algorithm need?\n",
    "# a) 1 (compute everything in one go)\n",
    "# b) 2 (online max/sum, then normalize)\n",
    "# c) 3 (max, exp-sum, normalize)\n",
    "# d) It depends on the input size\n",
    "q4_answer = 'b'\n",
    "check_answer(\"Number of passes in 2-pass algorithm\", q4_answer, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **Memory bandwidth:** Why does reducing passes matter for GPU performance?\n",
    "\n",
    "2. **The online algorithm:** How does it update the sum when a new max is found? (Hint: multiply by exp(old_max - new_max))\n",
    "\n",
    "3. **Can we do 1-pass?** What information would we need to store to normalize in a single pass?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Technique | Key Insight |\n",
    "|-----------|------------|\n",
    "| Max subtraction | softmax(x - max) = softmax(x), prevents overflow |\n",
    "| Log-sum-exp | LSE(x) = max(x) + log(Σexp(x - max)), stable log computation |\n",
    "| Online algorithm | Update running sum when max changes: sum *= exp(old_max - new_max) |\n",
    "\n",
    "**Tomorrow:** We'll apply stable softmax to full attention computation and discover the quadratic memory problem.\n",
    "\n",
    "---\n",
    "\n",
    "**Interactive Reference:** [attention-math.html](../attention-math.html) Section 3 — Online Softmax Simulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
