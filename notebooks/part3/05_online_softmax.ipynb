{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 5: Online Softmax — The Streaming Algorithm\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Understand and implement online softmax, the key algorithm behind FlashAttention.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "Standard softmax needs to see **all values** before computing any probability:\n",
    "1. Find max(x) — requires seeing all x\n",
    "2. Compute Σexp(x - max) — requires seeing all x again\n",
    "3. Divide each exp(x - max) by the sum\n",
    "\n",
    "**The question:** Can we compute softmax **one block at a time** without storing the entire sequence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "torch.set_printoptions(precision=6, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge — Streaming Data (5 min)\n",
    "\n",
    "Imagine processing attention in **blocks**:\n",
    "- You load a block of K values into fast SRAM\n",
    "- Compute partial attention\n",
    "- Load the next block\n",
    "- **Problem:** When you see a larger value in block 2, you need to update block 1's results!\n",
    "\n",
    "The **online softmax** algorithm solves this with clever bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard softmax - needs all data at once\n",
    "def standard_softmax(x):\n",
    "    \"\"\"Must see entire array to compute.\"\"\"\n",
    "    max_x = x.max()         # Pass 1: find max\n",
    "    exp_x = np.exp(x - max_x)  # Pass 2: compute exp\n",
    "    sum_exp = exp_x.sum()   # Pass 3: sum\n",
    "    return exp_x / sum_exp  # Pass 4: normalize\n",
    "\n",
    "# What if data comes in blocks?\n",
    "x = np.array([1.0, 3.0, 2.0, 5.0, 4.0, 6.0, 2.0, 1.0])\n",
    "print(f\"Full data: {x}\")\n",
    "print(f\"Standard softmax: {standard_softmax(x)}\")\n",
    "print(f\"Sum: {standard_softmax(x).sum():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore — The Rescaling Trick (15 min)\n",
    "\n",
    "### Key Insight\n",
    "\n",
    "When we find a new maximum, we can **rescale** our previous results!\n",
    "\n",
    "If we computed exp(x - old_max), and then find new_max > old_max:\n",
    "\n",
    "$$\\exp(x - \\text{new\\_max}) = \\exp(x - \\text{old\\_max}) \\times \\exp(\\text{old\\_max} - \\text{new\\_max})$$\n",
    "\n",
    "The correction factor $\\exp(\\text{old\\_max} - \\text{new\\_max})$ is just a scalar multiplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the rescaling\n",
    "x = np.array([1.0, 3.0, 2.0])  # First block\n",
    "old_max = 3.0\n",
    "\n",
    "# Compute with old max\n",
    "exp_old = np.exp(x - old_max)\n",
    "print(f\"Block 1: {x}\")\n",
    "print(f\"exp(x - {old_max}) = {exp_old}\")\n",
    "\n",
    "# New block arrives with larger value\n",
    "new_block = np.array([5.0, 4.0, 6.0])\n",
    "new_max = 6.0\n",
    "\n",
    "# We need to rescale block 1's exp values\n",
    "correction = np.exp(old_max - new_max)  # = exp(3 - 6) = exp(-3)\n",
    "exp_rescaled = exp_old * correction\n",
    "\n",
    "print(f\"\\nNew max found: {new_max}\")\n",
    "print(f\"Correction factor: exp({old_max} - {new_max}) = {correction:.6f}\")\n",
    "print(f\"Rescaled block 1: {exp_rescaled}\")\n",
    "\n",
    "# Verify: this equals computing directly with new max\n",
    "exp_direct = np.exp(x - new_max)\n",
    "print(f\"Direct computation: {exp_direct}\")\n",
    "print(f\"Match: {np.allclose(exp_rescaled, exp_direct)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Online Softmax Algorithm\n",
    "\n",
    "We maintain two running statistics:\n",
    "- **m**: Running maximum\n",
    "- **l**: Running sum of exp(x - m)\n",
    "\n",
    "When processing a new block:\n",
    "1. Find block's max (m_block)\n",
    "2. Update global max: m_new = max(m_old, m_block)\n",
    "3. Rescale old sum: l_old *= exp(m_old - m_new)\n",
    "4. Add new block: l_new = l_old + sum(exp(block - m_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax_demo(x, block_size=2):\n",
    "    \"\"\"\n",
    "    Online softmax with detailed logging.\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Initialize\n",
    "    m = float('-inf')  # Running max\n",
    "    l = 0.0            # Running sum of exp\n",
    "    \n",
    "    print(\"Online Softmax Computation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Process blocks\n",
    "    for block_start in range(0, n, block_size):\n",
    "        block = x[block_start:block_start + block_size]\n",
    "        print(f\"\\nBlock [{block_start}:{block_start + len(block)}]: {block}\")\n",
    "        \n",
    "        # Find block max\n",
    "        m_block = block.max()\n",
    "        print(f\"  Block max: {m_block}\")\n",
    "        \n",
    "        # Update global max\n",
    "        m_old = m\n",
    "        m_new = max(m, m_block)\n",
    "        print(f\"  Global max: {m_old} → {m_new}\")\n",
    "        \n",
    "        # Rescale old sum\n",
    "        if m_old != float('-inf'):\n",
    "            correction = np.exp(m_old - m_new)\n",
    "            l_old = l\n",
    "            l = l * correction\n",
    "            print(f\"  Rescale sum: {l_old:.6f} × exp({m_old}-{m_new}) = {l:.6f}\")\n",
    "        \n",
    "        # Add new block contribution\n",
    "        block_exp = np.exp(block - m_new)\n",
    "        block_sum = block_exp.sum()\n",
    "        l += block_sum\n",
    "        print(f\"  Block exp(x - {m_new}): {block_exp}\")\n",
    "        print(f\"  Block sum: {block_sum:.6f}\")\n",
    "        print(f\"  Running sum: {l:.6f}\")\n",
    "        \n",
    "        m = m_new\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final: max = {m}, sum = {l:.6f}\")\n",
    "    \n",
    "    # Compute final softmax\n",
    "    probs = np.exp(x - m) / l\n",
    "    print(f\"\\nSoftmax result: {probs}\")\n",
    "    print(f\"Sum: {probs.sum():.6f}\")\n",
    "    \n",
    "    return probs, m, l\n",
    "\n",
    "x = np.array([1.0, 3.0, 2.0, 5.0, 4.0, 6.0, 2.0, 1.0])\n",
    "probs_online, _, _ = online_softmax_demo(x, block_size=2)\n",
    "\n",
    "print(f\"\\nVerification against standard softmax:\")\n",
    "probs_standard = standard_softmax(x)\n",
    "print(f\"Standard: {probs_standard}\")\n",
    "print(f\"Match: {np.allclose(probs_online, probs_standard)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept — Online Softmax for Attention (10 min)\n",
    "\n",
    "### Applying to Attention\n",
    "\n",
    "In attention, we need:\n",
    "$$\\text{output} = \\text{softmax}(\\text{scores}) \\times V$$\n",
    "\n",
    "The online algorithm maintains:\n",
    "- **m**: Running max of scores\n",
    "- **l**: Running sum of exp(scores - m)\n",
    "- **O**: Running **weighted sum** of values\n",
    "\n",
    "The update rule for O is:\n",
    "$$O_{new} = O_{old} \\times \\frac{l_{old}}{l_{new}} \\times \\exp(m_{old} - m_{new}) + \\frac{\\text{exp\\_block}}{l_{new}} \\times V_{block}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_attention_demo(Q, K, V, block_size=2):\n",
    "    \"\"\"\n",
    "    Online attention computation - processes K/V in blocks.\n",
    "    \n",
    "    Q: [d_k] - single query\n",
    "    K: [seq_len, d_k] - all keys\n",
    "    V: [seq_len, d_v] - all values\n",
    "    \"\"\"\n",
    "    seq_len = K.shape[0]\n",
    "    d_v = V.shape[1]\n",
    "    d_k = K.shape[1]\n",
    "    scale = np.sqrt(d_k)\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    m = float('-inf')  # Running max\n",
    "    l = 0.0            # Running sum of exp\n",
    "    O = np.zeros(d_v)  # Running output (unnormalized at first)\n",
    "    \n",
    "    print(\"Online Attention Computation\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for block_start in range(0, seq_len, block_size):\n",
    "        block_end = min(block_start + block_size, seq_len)\n",
    "        \n",
    "        # Get block of K, V\n",
    "        K_block = K[block_start:block_end]\n",
    "        V_block = V[block_start:block_end]\n",
    "        \n",
    "        # Compute scores for this block\n",
    "        scores_block = (Q @ K_block.T) / scale\n",
    "        \n",
    "        print(f\"\\nBlock [{block_start}:{block_end}]\")\n",
    "        print(f\"  Scores: {scores_block}\")\n",
    "        \n",
    "        # Block statistics\n",
    "        m_block = scores_block.max()\n",
    "        m_old = m\n",
    "        m_new = max(m, m_block)\n",
    "        \n",
    "        print(f\"  Block max: {m_block:.4f}, Global max: {m_old:.4f} → {m_new:.4f}\")\n",
    "        \n",
    "        # Rescale old accumulator\n",
    "        if m_old != float('-inf'):\n",
    "            correction = np.exp(m_old - m_new)\n",
    "            l_old = l\n",
    "            l = l * correction\n",
    "            O = O * correction  # Also rescale the output accumulator!\n",
    "            print(f\"  Rescaling: correction = {correction:.6f}\")\n",
    "        \n",
    "        # Process new block\n",
    "        exp_block = np.exp(scores_block - m_new)\n",
    "        l += exp_block.sum()\n",
    "        \n",
    "        # Accumulate weighted values (using unnormalized weights for now)\n",
    "        O += exp_block @ V_block\n",
    "        \n",
    "        m = m_new\n",
    "        \n",
    "        print(f\"  exp(scores - m): {exp_block}\")\n",
    "        print(f\"  Running l: {l:.6f}\")\n",
    "    \n",
    "    # Final normalization\n",
    "    O = O / l\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Final output: {O}\")\n",
    "    \n",
    "    return O, m, l\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "d_k, d_v = 4, 4\n",
    "seq_len = 6\n",
    "\n",
    "Q = np.random.randn(d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "output_online, _, _ = online_attention_demo(Q, K, V, block_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify against standard attention\n",
    "def standard_attention(Q, K, V):\n",
    "    \"\"\"Standard attention for single query.\"\"\"\n",
    "    d_k = K.shape[1]\n",
    "    scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "    weights = standard_softmax(scores)\n",
    "    return weights @ V\n",
    "\n",
    "output_standard = standard_attention(Q, K, V)\n",
    "\n",
    "print(f\"\\nVerification:\")\n",
    "print(f\"Online:   {output_online}\")\n",
    "print(f\"Standard: {output_standard}\")\n",
    "print(f\"Max diff: {np.abs(output_online - output_standard).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It — Efficient Online Softmax (30 min)\n",
    "\n",
    "### Clean Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax(x, block_size):\n",
    "    \"\"\"\n",
    "    Compute softmax using online algorithm.\n",
    "    \n",
    "    Args:\n",
    "        x: Input array\n",
    "        block_size: Process this many elements at a time\n",
    "    \n",
    "    Returns:\n",
    "        Softmax probabilities (same shape as x)\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # First pass: compute m (max) and l (sum of exp)\n",
    "    m = float('-inf')\n",
    "    l = 0.0\n",
    "    \n",
    "    for start in range(0, n, block_size):\n",
    "        block = x[start:start + block_size]\n",
    "        m_block = block.max()\n",
    "        \n",
    "        # Update max and rescale sum\n",
    "        m_new = max(m, m_block)\n",
    "        l = l * np.exp(m - m_new) + np.sum(np.exp(block - m_new))\n",
    "        m = m_new\n",
    "    \n",
    "    # Second pass: compute final probabilities\n",
    "    return np.exp(x - m) / l\n",
    "\n",
    "# Test various block sizes\n",
    "x = np.random.randn(100)\n",
    "probs_standard = standard_softmax(x)\n",
    "\n",
    "print(\"Block size | Max error\")\n",
    "print(\"-\" * 25)\n",
    "for bs in [1, 2, 5, 10, 25, 50, 100]:\n",
    "    probs_online = online_softmax(x, bs)\n",
    "    error = np.abs(probs_standard - probs_online).max()\n",
    "    print(f\"{bs:10d} | {error:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Attention (Full Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_attention(Q, K, V, block_size):\n",
    "    \"\"\"\n",
    "    Online attention for a single query.\n",
    "    Processes K/V in blocks, never materializing full attention matrix.\n",
    "    \n",
    "    Args:\n",
    "        Q: [d_k] single query vector\n",
    "        K: [seq_len, d_k] key matrix\n",
    "        V: [seq_len, d_v] value matrix\n",
    "        block_size: Number of K/V pairs to process at once\n",
    "    \n",
    "    Returns:\n",
    "        Output vector [d_v]\n",
    "    \"\"\"\n",
    "    seq_len, d_k = K.shape\n",
    "    d_v = V.shape[1]\n",
    "    scale = np.sqrt(d_k)\n",
    "    \n",
    "    # Initialize\n",
    "    m = float('-inf')\n",
    "    l = 0.0\n",
    "    O = np.zeros(d_v)\n",
    "    \n",
    "    for start in range(0, seq_len, block_size):\n",
    "        end = min(start + block_size, seq_len)\n",
    "        \n",
    "        # Compute scores for this block\n",
    "        scores = (Q @ K[start:end].T) / scale\n",
    "        \n",
    "        # Update statistics\n",
    "        m_block = scores.max()\n",
    "        m_new = max(m, m_block)\n",
    "        \n",
    "        # Rescale old accumulators\n",
    "        correction = np.exp(m - m_new) if m != float('-inf') else 1.0\n",
    "        l = l * correction\n",
    "        O = O * correction\n",
    "        \n",
    "        # Add new block\n",
    "        exp_scores = np.exp(scores - m_new)\n",
    "        l += exp_scores.sum()\n",
    "        O += exp_scores @ V[start:end]\n",
    "        \n",
    "        m = m_new\n",
    "    \n",
    "    # Final normalization\n",
    "    return O / l\n",
    "\n",
    "def online_attention_batched(Q, K, V, block_size):\n",
    "    \"\"\"\n",
    "    Online attention for multiple queries.\n",
    "    \n",
    "    Args:\n",
    "        Q: [num_queries, d_k]\n",
    "        K: [seq_len, d_k]\n",
    "        V: [seq_len, d_v]\n",
    "        block_size: K/V block size\n",
    "    \n",
    "    Returns:\n",
    "        Output [num_queries, d_v]\n",
    "    \"\"\"\n",
    "    num_queries = Q.shape[0]\n",
    "    seq_len, d_k = K.shape\n",
    "    d_v = V.shape[1]\n",
    "    scale = np.sqrt(d_k)\n",
    "    \n",
    "    # Initialize per-query accumulators\n",
    "    m = np.full(num_queries, float('-inf'))\n",
    "    l = np.zeros(num_queries)\n",
    "    O = np.zeros((num_queries, d_v))\n",
    "    \n",
    "    for start in range(0, seq_len, block_size):\n",
    "        end = min(start + block_size, seq_len)\n",
    "        \n",
    "        K_block = K[start:end]  # [block_size, d_k]\n",
    "        V_block = V[start:end]  # [block_size, d_v]\n",
    "        \n",
    "        # Scores: [num_queries, block_size]\n",
    "        scores = (Q @ K_block.T) / scale\n",
    "        \n",
    "        # Per-query max for this block\n",
    "        m_block = scores.max(axis=1)  # [num_queries]\n",
    "        m_new = np.maximum(m, m_block)\n",
    "        \n",
    "        # Rescale (handle -inf case)\n",
    "        correction = np.exp(np.where(m == float('-inf'), 0, m - m_new))\n",
    "        l = l * correction\n",
    "        O = O * correction[:, None]\n",
    "        \n",
    "        # Add block contribution\n",
    "        exp_scores = np.exp(scores - m_new[:, None])  # [num_queries, block_size]\n",
    "        l += exp_scores.sum(axis=1)\n",
    "        O += exp_scores @ V_block  # [num_queries, d_v]\n",
    "        \n",
    "        m = m_new\n",
    "    \n",
    "    return O / l[:, None]\n",
    "\n",
    "# Test batched version\n",
    "np.random.seed(42)\n",
    "num_queries = 4\n",
    "seq_len = 16\n",
    "d_k = d_v = 8\n",
    "\n",
    "Q = np.random.randn(num_queries, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "# Online attention\n",
    "output_online = online_attention_batched(Q, K, V, block_size=4)\n",
    "\n",
    "# Standard attention\n",
    "scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "weights = np.exp(scores - scores.max(axis=1, keepdims=True))\n",
    "weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "output_standard = weights @ V\n",
    "\n",
    "print(f\"Max difference: {np.abs(output_online - output_standard).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_attention_torch(Q, K, V, block_size):\n",
    "    \"\"\"\n",
    "    Online attention in PyTorch.\n",
    "    \n",
    "    Q: [batch, num_queries, d_k]\n",
    "    K: [batch, seq_len, d_k]\n",
    "    V: [batch, seq_len, d_v]\n",
    "    \"\"\"\n",
    "    batch, num_queries, d_k = Q.shape\n",
    "    seq_len = K.shape[1]\n",
    "    d_v = V.shape[2]\n",
    "    scale = d_k ** 0.5\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    m = torch.full((batch, num_queries), float('-inf'), device=Q.device)\n",
    "    l = torch.zeros((batch, num_queries), device=Q.device)\n",
    "    O = torch.zeros((batch, num_queries, d_v), device=Q.device)\n",
    "    \n",
    "    for start in range(0, seq_len, block_size):\n",
    "        end = min(start + block_size, seq_len)\n",
    "        \n",
    "        K_block = K[:, start:end, :]  # [batch, block_size, d_k]\n",
    "        V_block = V[:, start:end, :]  # [batch, block_size, d_v]\n",
    "        \n",
    "        # Scores: [batch, num_queries, block_size]\n",
    "        scores = torch.bmm(Q, K_block.transpose(-2, -1)) / scale\n",
    "        \n",
    "        # Block max: [batch, num_queries]\n",
    "        m_block = scores.max(dim=-1).values\n",
    "        m_new = torch.maximum(m, m_block)\n",
    "        \n",
    "        # Rescale\n",
    "        correction = torch.exp(torch.where(\n",
    "            m == float('-inf'),\n",
    "            torch.zeros_like(m),\n",
    "            m - m_new\n",
    "        ))\n",
    "        l = l * correction\n",
    "        O = O * correction.unsqueeze(-1)\n",
    "        \n",
    "        # Add block\n",
    "        exp_scores = torch.exp(scores - m_new.unsqueeze(-1))\n",
    "        l = l + exp_scores.sum(dim=-1)\n",
    "        O = O + torch.bmm(exp_scores, V_block)\n",
    "        \n",
    "        m = m_new\n",
    "    \n",
    "    return O / l.unsqueeze(-1)\n",
    "\n",
    "# Test\n",
    "Q_t = torch.randn(2, 8, 16)  # [batch, queries, d_k]\n",
    "K_t = torch.randn(2, 32, 16) # [batch, seq_len, d_k]\n",
    "V_t = torch.randn(2, 32, 16) # [batch, seq_len, d_v]\n",
    "\n",
    "output_online = online_attention_torch(Q_t, K_t, V_t, block_size=8)\n",
    "\n",
    "# Compare with standard attention\n",
    "scores = torch.bmm(Q_t, K_t.transpose(-2, -1)) / (16 ** 0.5)\n",
    "weights = torch.softmax(scores, dim=-1)\n",
    "output_standard = torch.bmm(weights, V_t)\n",
    "\n",
    "print(f\"Max difference: {(output_online - output_standard).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Online vs Standard Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_comparison(seq_lengths, block_size=64, d_model=64):\n",
    "    \"\"\"\n",
    "    Compare memory usage: standard vs online attention.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for n in seq_lengths:\n",
    "        # Standard: stores full N×N attention matrix\n",
    "        standard_memory = n * n * 4  # bytes (FP32)\n",
    "        \n",
    "        # Online: stores only block_size elements at a time\n",
    "        online_memory = n * block_size * 4 + n * d_model * 4  # scores + output\n",
    "        \n",
    "        results.append({\n",
    "            'seq_len': n,\n",
    "            'standard_mb': standard_memory / 1e6,\n",
    "            'online_mb': online_memory / 1e6,\n",
    "            'savings': standard_memory / online_memory\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "seq_lengths = [256, 512, 1024, 2048, 4096, 8192, 16384]\n",
    "results = memory_comparison(seq_lengths, block_size=64)\n",
    "\n",
    "print(f\"{'Seq Len':>10} {'Standard':>12} {'Online':>12} {'Savings':>10}\")\n",
    "print(\"-\" * 50)\n",
    "for r in results:\n",
    "    print(f\"{r['seq_len']:>10} {r['standard_mb']:>10.1f}MB {r['online_mb']:>10.1f}MB {r['savings']:>9.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Memory usage\n",
    "standard_mem = [r['standard_mb'] for r in results]\n",
    "online_mem = [r['online_mb'] for r in results]\n",
    "\n",
    "axes[0].semilogy(seq_lengths, standard_mem, 'r-o', label='Standard (O(N²))', linewidth=2)\n",
    "axes[0].semilogy(seq_lengths, online_mem, 'b-o', label='Online (O(N))', linewidth=2)\n",
    "axes[0].set_xlabel('Sequence Length')\n",
    "axes[0].set_ylabel('Memory (MB, log scale)')\n",
    "axes[0].set_title('Memory Usage Comparison')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Savings factor\n",
    "savings = [r['savings'] for r in results]\n",
    "axes[1].plot(seq_lengths, savings, 'g-o', linewidth=2)\n",
    "axes[1].set_xlabel('Sequence Length')\n",
    "axes[1].set_ylabel('Memory Savings (×)')\n",
    "axes[1].set_title('Online Memory Savings (grows with N)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Quiz & Reflection (10 min)\n",
    "\n",
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, your_answer, correct_answer):\n",
    "    if your_answer == correct_answer:\n",
    "        print(f\"✓ Correct! {question}\")\n",
    "    else:\n",
    "        print(f\"✗ Incorrect. Your answer: {your_answer}, Correct: {correct_answer}\")\n",
    "\n",
    "# Q1: What running statistics does online softmax maintain?\n",
    "# a) Only max\n",
    "# b) Max and sum\n",
    "# c) Max, sum, and count\n",
    "# d) Mean and variance\n",
    "q1_answer = 'b'\n",
    "check_answer(\"Running statistics\", q1_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: When a new max is found, how do we update the running sum?\n",
    "# a) Reset it to 0\n",
    "# b) Add the new max\n",
    "# c) Multiply by exp(old_max - new_max)\n",
    "# d) Divide by the new max\n",
    "q2_answer = 'c'\n",
    "check_answer(\"Updating running sum\", q2_answer, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: Online softmax reduces memory from O(N²) to:\n",
    "# a) O(N)\n",
    "# b) O(N log N)\n",
    "# c) O(√N)\n",
    "# d) O(1)\n",
    "q3_answer = 'a'\n",
    "check_answer(\"Memory reduction\", q3_answer, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Why is the correction factor exp(old_max - new_max) always ≤ 1?\n",
    "# a) Because old_max ≤ new_max (max can only increase)\n",
    "# b) Because exp is always positive\n",
    "# c) Because we normalize at the end\n",
    "# d) It's not always ≤ 1\n",
    "q4_answer = 'a'\n",
    "check_answer(\"Correction factor\", q4_answer, 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **Two passes:** Online softmax still needs two passes over the data (one for m/l, one to compute final probs). Can FlashAttention do better?\n",
    "\n",
    "2. **Numerical precision:** Why might online softmax have slightly different numerical errors than standard softmax?\n",
    "\n",
    "3. **Block size choice:** How does block size affect (a) memory usage, (b) numerical accuracy, (c) performance?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|------------|\n",
    "| Shift invariance | softmax(x - c) = softmax(x), so we can update when max changes |\n",
    "| Rescaling | When max changes, multiply old sum by exp(old_max - new_max) |\n",
    "| Memory reduction | O(N²) → O(N) by processing in blocks |\n",
    "| Output accumulation | Track weighted sum and rescale along with the sum |\n",
    "\n",
    "**Tomorrow:** Tiled attention — applying online softmax to compute attention block-by-block on GPU.\n",
    "\n",
    "---\n",
    "\n",
    "**Interactive Reference:** [attention-math.html](../attention-math.html) Section 3 — Online Softmax Simulation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
