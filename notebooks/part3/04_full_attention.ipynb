{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 4: Full Attention — The Complete Formula\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Implement the complete attention mechanism and understand its quadratic memory problem.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "We have all the pieces:\n",
    "- **Dot products** (Day 1): Compute similarity scores\n",
    "- **Softmax** (Days 2-3): Convert scores to probabilities\n",
    "\n",
    "Today we combine them into **scaled dot-product attention**:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge — What Attention Does (5 min)\n",
    "\n",
    "Attention answers: **For each position, what information from other positions should I use?**\n",
    "\n",
    "The formula:\n",
    "\n",
    "1. **QK^T**: Compute all pairwise similarities (which keys match which queries)\n",
    "2. **÷√d_k**: Scale to control variance\n",
    "3. **softmax**: Convert to probabilities (weights that sum to 1)\n",
    "4. **× V**: Weighted average of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual example with a tiny sequence\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "\n",
    "# Simple example: each position has distinct Q, K, V\n",
    "torch.manual_seed(42)\n",
    "Q = torch.randn(seq_len, d_model)\n",
    "K = torch.randn(seq_len, d_model)\n",
    "V = torch.randn(seq_len, d_model)\n",
    "\n",
    "print(f\"Q shape: {Q.shape} — Queries (what each position is looking for)\")\n",
    "print(f\"K shape: {K.shape} — Keys (what each position offers to be found)\")\n",
    "print(f\"V shape: {V.shape} — Values (what each position contributes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore — Step-by-Step Computation (15 min)\n",
    "\n",
    "Let's trace through the full attention computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_step_by_step(Q, K, V, verbose=True):\n",
    "    \"\"\"\n",
    "    Compute attention with detailed steps.\n",
    "    \"\"\"\n",
    "    seq_len, d_k = Q.shape\n",
    "    \n",
    "    # Step 1: QK^T\n",
    "    scores = Q @ K.T\n",
    "    if verbose:\n",
    "        print(f\"Step 1: QK^T\")\n",
    "        print(f\"  Shape: {Q.shape} @ {K.T.shape} = {scores.shape}\")\n",
    "        print(f\"  Result (raw scores):\")\n",
    "        print(scores.numpy())\n",
    "        print()\n",
    "    \n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scale = np.sqrt(d_k)\n",
    "    scores_scaled = scores / scale\n",
    "    if verbose:\n",
    "        print(f\"Step 2: Divide by √d_k = √{d_k} = {scale:.2f}\")\n",
    "        print(f\"  Result (scaled scores):\")\n",
    "        print(scores_scaled.numpy())\n",
    "        print()\n",
    "    \n",
    "    # Step 3: Softmax (row-wise)\n",
    "    attention_weights = F.softmax(scores_scaled, dim=-1)\n",
    "    if verbose:\n",
    "        print(f\"Step 3: Softmax (each row sums to 1)\")\n",
    "        print(f\"  Result (attention weights):\")\n",
    "        print(attention_weights.numpy())\n",
    "        print(f\"  Row sums: {attention_weights.sum(dim=-1).numpy()}\")\n",
    "        print()\n",
    "    \n",
    "    # Step 4: Weighted sum of V\n",
    "    output = attention_weights @ V\n",
    "    if verbose:\n",
    "        print(f\"Step 4: Attention @ V\")\n",
    "        print(f\"  Shape: {attention_weights.shape} @ {V.shape} = {output.shape}\")\n",
    "        print(f\"  Result (output):\")\n",
    "        print(output.numpy())\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "output, weights = attention_step_by_step(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention pattern\n",
    "def visualize_attention(weights, title=\"Attention Weights\"):\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.imshow(weights.numpy(), cmap='Blues', aspect='auto')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.xlabel('Key Position (attending to)')\n",
    "    plt.ylabel('Query Position (from)')\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(weights.shape[0]):\n",
    "        for j in range(weights.shape[1]):\n",
    "            plt.text(j, i, f'{weights[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention(weights, \"Full Attention (All positions attend to all)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Attention\n",
    "\n",
    "In decoder-only models (like GPT), each position can only attend to **itself and previous positions**. This is called **causal** or **autoregressive** attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Attention with causal mask: position i can only attend to positions <= i.\n",
    "    \"\"\"\n",
    "    seq_len, d_k = Q.shape\n",
    "    \n",
    "    # Compute scaled scores\n",
    "    scores = Q @ K.T / np.sqrt(d_k)\n",
    "    \n",
    "    # Create causal mask (upper triangular = True, meaning \"mask this out\")\n",
    "    causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "    \n",
    "    # Apply mask: set future positions to -infinity\n",
    "    scores_masked = scores.masked_fill(causal_mask, float('-inf'))\n",
    "    \n",
    "    # Softmax (rows with -inf become 0 probability)\n",
    "    attention_weights = F.softmax(scores_masked, dim=-1)\n",
    "    \n",
    "    # Weighted sum\n",
    "    output = attention_weights @ V\n",
    "    \n",
    "    return output, attention_weights, scores_masked\n",
    "\n",
    "output_causal, weights_causal, scores_causal = causal_attention(Q, K, V)\n",
    "\n",
    "print(\"Scores after causal mask:\")\n",
    "print(scores_causal.numpy())\n",
    "print(\"\\nAttention weights (causal):\")\n",
    "print(weights_causal.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_attention(weights_causal, \"Causal Attention (Lower triangular)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept — The Quadratic Memory Problem (10 min)\n",
    "\n",
    "### Memory Analysis\n",
    "\n",
    "For a sequence of length $N$ with embedding dimension $d$:\n",
    "\n",
    "| Tensor | Shape | Size (FP16) |\n",
    "|--------|-------|-------------|\n",
    "| Q, K, V | [N, d] | N × d × 2 bytes each |\n",
    "| QK^T (scores) | [N, N] | N² × 2 bytes |\n",
    "| softmax(scores) | [N, N] | N² × 2 bytes |\n",
    "| Output | [N, d] | N × d × 2 bytes |\n",
    "\n",
    "**The problem:** The attention matrix is **N² in size**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_analysis(seq_len, d_model, batch_size=1, n_heads=1, dtype_bytes=2):\n",
    "    \"\"\"\n",
    "    Calculate memory usage for standard attention.\n",
    "    \"\"\"\n",
    "    # Input/output tensors\n",
    "    qkv_memory = 3 * batch_size * seq_len * d_model * dtype_bytes\n",
    "    output_memory = batch_size * seq_len * d_model * dtype_bytes\n",
    "    \n",
    "    # Attention matrix (the quadratic part)\n",
    "    attention_matrix_memory = batch_size * n_heads * seq_len * seq_len * dtype_bytes\n",
    "    \n",
    "    total = qkv_memory + output_memory + attention_matrix_memory\n",
    "    \n",
    "    return {\n",
    "        'seq_len': seq_len,\n",
    "        'qkv_memory_mb': qkv_memory / 1e6,\n",
    "        'output_memory_mb': output_memory / 1e6,\n",
    "        'attention_matrix_mb': attention_matrix_memory / 1e6,\n",
    "        'total_mb': total / 1e6,\n",
    "        'attention_pct': attention_matrix_memory / total * 100,\n",
    "    }\n",
    "\n",
    "# Memory usage for different sequence lengths\n",
    "print(\"Memory usage (batch=1, d_model=4096, n_heads=32, FP16):\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Seq Len':>10} {'QKV':>10} {'Attn Matrix':>15} {'Total':>10} {'Attn %':>10}\")\n",
    "\n",
    "for seq_len in [512, 2048, 8192, 32768, 131072]:\n",
    "    stats = memory_analysis(seq_len, d_model=4096, batch_size=1, n_heads=32)\n",
    "    print(f\"{seq_len:>10} {stats['qkv_memory_mb']:>9.1f}MB {stats['attention_matrix_mb']:>14.1f}MB \"\n",
    "          f\"{stats['total_mb']:>9.1f}MB {stats['attention_pct']:>9.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the quadratic scaling\n",
    "seq_lengths = np.array([512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072])\n",
    "attention_memory_gb = [memory_analysis(n, 4096, 1, 32)['attention_matrix_mb'] / 1000 \n",
    "                       for n in seq_lengths]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(seq_lengths, attention_memory_gb, 'b-o', linewidth=2, markersize=6)\n",
    "plt.axhline(y=80, color='red', linestyle='--', label='A100 80GB VRAM')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Attention Matrix Memory (GB)')\n",
    "plt.title('Linear Scale')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.loglog(seq_lengths, attention_memory_gb, 'b-o', linewidth=2, markersize=6)\n",
    "plt.axhline(y=80, color='red', linestyle='--', label='A100 80GB VRAM')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Attention Matrix Memory (GB)')\n",
    "plt.title('Log-Log Scale (slope = 2 → quadratic)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"At 128K tokens: {attention_memory_gb[-1]:.1f} GB just for attention matrix!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem in Practice\n",
    "\n",
    "For GPT-4's 128K context window:\n",
    "- Attention matrix: 128K × 128K = 16.4 billion elements\n",
    "- In FP16: 32 GB per layer per head\n",
    "- With 32 heads: **1 TB per layer!**\n",
    "\n",
    "This is why we need **FlashAttention** — it computes attention without materializing the full N×N matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It — Complete Attention Implementation (30 min)\n",
    "\n",
    "### Standard Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Standard scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: [batch, seq_len, d_k] or [seq_len, d_k]\n",
    "        K: [batch, seq_len, d_k] or [seq_len, d_k]\n",
    "        V: [batch, seq_len, d_v] or [seq_len, d_v]\n",
    "        mask: Optional boolean mask [seq_len, seq_len], True = mask out\n",
    "    \n",
    "    Returns:\n",
    "        output: [batch, seq_len, d_v]\n",
    "        attention_weights: [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test\n",
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "\n",
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Weights shape: {weights.shape}\")\n",
    "print(f\"Weights sum per row: {weights.sum(dim=-1)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Real transformers use **multiple attention heads** — each head can learn different patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention as used in transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch, seq_len, d_model]\n",
    "            mask: Optional [seq_len, seq_len] causal mask\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)  # [batch, seq_len, d_model]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape for multi-head: [batch, n_heads, seq_len, d_k]\n",
    "        Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention for all heads in parallel\n",
    "        # scores: [batch, n_heads, seq_len, seq_len]\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        context = torch.matmul(attention_weights, V)  # [batch, n_heads, seq_len, d_k]\n",
    "        \n",
    "        # Concatenate heads\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(context)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(d_model=64, n_heads=8)\n",
    "x = torch.randn(2, 16, 64)  # [batch, seq_len, d_model]\n",
    "\n",
    "# Create causal mask\n",
    "causal_mask = torch.triu(torch.ones(16, 16), diagonal=1).bool()\n",
    "\n",
    "output, weights = mha(x, mask=causal_mask)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {weights.shape} (batch, heads, seq, seq)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns across heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(14, 7))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(weights[0, i].detach().numpy(), cmap='Blues', aspect='auto')\n",
    "    ax.set_title(f'Head {i}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "\n",
    "plt.suptitle('Multi-Head Attention Patterns (with causal mask)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Compare with PyTorch's Built-in\n",
    "\n",
    "PyTorch provides `torch.nn.functional.scaled_dot_product_attention`. Let's compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare our implementation with PyTorch's\n",
    "Q = torch.randn(2, 8, 32, 64)  # [batch, heads, seq_len, d_k]\n",
    "K = torch.randn(2, 8, 32, 64)\n",
    "V = torch.randn(2, 8, 32, 64)\n",
    "\n",
    "# Our implementation (adapted for batched multi-head input)\n",
    "def our_attention(Q, K, V, is_causal=False):\n",
    "    d_k = Q.shape[-1]\n",
    "    scores = Q @ K.transpose(-2, -1) / np.sqrt(d_k)\n",
    "    \n",
    "    if is_causal:\n",
    "        seq_len = Q.shape[-2]\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "        scores = scores.masked_fill(mask, float('-inf'))\n",
    "    \n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights @ V\n",
    "\n",
    "our_result = our_attention(Q, K, V, is_causal=True)\n",
    "pytorch_result = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "\n",
    "max_diff = (our_result - pytorch_result).abs().max()\n",
    "print(f\"Max difference: {max_diff:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark: See the Memory Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Benchmarking standard attention (watch memory grow):\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    d_model = 64\n",
    "    n_heads = 8\n",
    "    \n",
    "    for seq_len in [256, 512, 1024, 2048, 4096]:\n",
    "        try:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            \n",
    "            Q = torch.randn(1, n_heads, seq_len, d_model, device='cuda')\n",
    "            K = torch.randn(1, n_heads, seq_len, d_model, device='cuda')\n",
    "            V = torch.randn(1, n_heads, seq_len, d_model, device='cuda')\n",
    "            \n",
    "            # Force computation\n",
    "            output = our_attention(Q, K, V, is_causal=True)\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            peak_memory = torch.cuda.max_memory_allocated() / 1e6\n",
    "            \n",
    "            # Theoretical attention matrix size\n",
    "            attn_matrix_mb = n_heads * seq_len * seq_len * 4 / 1e6  # FP32\n",
    "            \n",
    "            print(f\"seq_len={seq_len:5d}: peak memory={peak_memory:8.1f}MB, \"\n",
    "                  f\"attn matrix={attn_matrix_mb:8.1f}MB\")\n",
    "            \n",
    "            del Q, K, V, output\n",
    "        except RuntimeError as e:\n",
    "            print(f\"seq_len={seq_len:5d}: OUT OF MEMORY!\")\n",
    "            break\n",
    "else:\n",
    "    print(\"GPU not available for memory benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Quiz & Reflection (10 min)\n",
    "\n",
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, your_answer, correct_answer):\n",
    "    if your_answer == correct_answer:\n",
    "        print(f\"✓ Correct! {question}\")\n",
    "    else:\n",
    "        print(f\"✗ Incorrect. {question}\")\n",
    "        print(f\"  Your answer: {your_answer}, Correct: {correct_answer}\")\n",
    "\n",
    "# Q1: What is the shape of the attention matrix for seq_len=1024?\n",
    "# a) [1024]\n",
    "# b) [1024, 64]\n",
    "# c) [1024, 1024]\n",
    "# d) [64, 1024]\n",
    "q1_answer = 'c'\n",
    "check_answer(\"Attention matrix shape\", q1_answer, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: If we double the sequence length, memory for attention matrix increases by:\n",
    "# a) 2x\n",
    "# b) 4x\n",
    "# c) 8x\n",
    "# d) log(2)x\n",
    "q2_answer = 'b'  # N² → (2N)² = 4N²\n",
    "check_answer(\"Memory scaling when doubling seq_len\", q2_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: In causal attention, which positions can query position 5 attend to?\n",
    "# a) Only position 5\n",
    "# b) Positions 0-4\n",
    "# c) Positions 0-5\n",
    "# d) All positions\n",
    "q3_answer = 'c'  # Can attend to self and previous\n",
    "check_answer(\"Causal attention for position 5\", q3_answer, 'c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: Why do we mask with -inf instead of 0 in causal attention?\n",
    "# a) -inf is faster to compute\n",
    "# b) softmax(-inf) = 0, giving zero weight\n",
    "# c) 0 would cause division by zero\n",
    "# d) It's just a convention\n",
    "q4_answer = 'b'\n",
    "check_answer(\"Why mask with -inf\", q4_answer, 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **The quadratic bottleneck:** Which operation creates the N×N matrix? Can we avoid computing it all at once?\n",
    "\n",
    "2. **Multi-head attention:** Why use multiple heads instead of one big attention? (Hint: diversity of patterns)\n",
    "\n",
    "3. **Memory vs compute:** Is attention memory-bound or compute-bound? How do you know?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Component | Formula | Purpose |\n",
    "|-----------|---------|--------|\n",
    "| QK^T | Query × Key^T | Compute pairwise similarities |\n",
    "| ÷√d_k | scores / √d_k | Normalize variance |\n",
    "| softmax | exp(x) / Σexp(x) | Convert to probabilities |\n",
    "| × V | weights × Values | Weighted combination |\n",
    "\n",
    "**The problem:** Materializing the full N×N attention matrix requires O(N²) memory.\n",
    "\n",
    "**Tomorrow:** Online softmax — computing softmax without storing all values at once.\n",
    "\n",
    "---\n",
    "\n",
    "**Interactive Reference:** [attention-math.html](../attention-math.html) Section 5 — Full Attention Visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
