{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 6: Tiled Attention — Block-by-Block on GPU\n",
    "\n",
    "**Time:** ~1 hour\n",
    "\n",
    "**Goal:** Apply online softmax to compute attention in tiles, leveraging fast SRAM on GPU.\n",
    "\n",
    "## The Challenge\n",
    "\n",
    "We have the online softmax algorithm from yesterday. Now we need to:\n",
    "1. **Tile** the computation to fit in GPU shared memory (SRAM)\n",
    "2. **Fuse** the operations to minimize memory traffic\n",
    "3. **Handle both Q and K/V tiling** for the complete FlashAttention algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge — GPU Memory Hierarchy (5 min)\n",
    "\n",
    "### GPU Memory Hierarchy Recap\n",
    "\n",
    "| Memory Type | Size | Bandwidth | Latency |\n",
    "|-------------|------|-----------|--------|\n",
    "| Registers | ~256KB per SM | N/A | ~1 cycle |\n",
    "| Shared Memory (SRAM) | 64-228KB per SM | ~19 TB/s | ~20 cycles |\n",
    "| L2 Cache | 40-60MB | ~8 TB/s | ~200 cycles |\n",
    "| HBM (Global) | 40-80GB | ~2 TB/s | ~400 cycles |\n",
    "\n",
    "**Key insight:** We want to:\n",
    "1. Load Q, K, V tiles into **shared memory** (fast)\n",
    "2. Compute attention **within** shared memory\n",
    "3. Only write the final output to HBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the tiling strategy\n",
    "def visualize_tiling(seq_len, block_q, block_kv):\n",
    "    \"\"\"Show how attention is tiled.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    # Draw the full attention matrix\n",
    "    ax.set_xlim(0, seq_len)\n",
    "    ax.set_ylim(seq_len, 0)  # Flip y-axis\n",
    "    \n",
    "    # Draw grid lines for tiles\n",
    "    for i in range(0, seq_len + 1, block_q):\n",
    "        ax.axhline(y=i, color='blue', linewidth=0.5, alpha=0.5)\n",
    "    for j in range(0, seq_len + 1, block_kv):\n",
    "        ax.axvline(x=j, color='red', linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Highlight one Q block processing all K blocks\n",
    "    q_block_idx = 1\n",
    "    for kv_idx in range(seq_len // block_kv):\n",
    "        rect = plt.Rectangle(\n",
    "            (kv_idx * block_kv, q_block_idx * block_q),\n",
    "            block_kv, block_q,\n",
    "            fill=True, facecolor='green', alpha=0.3, edgecolor='green', linewidth=2\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    ax.set_xlabel('K/V position (columns)')\n",
    "    ax.set_ylabel('Q position (rows)')\n",
    "    ax.set_title(f'Tiled Attention: seq_len={seq_len}, block_Q={block_q}, block_KV={block_kv}\\n'\n",
    "                 f'Green = one Q block iterating over all K/V blocks')\n",
    "    \n",
    "    # Add annotations\n",
    "    num_q_blocks = seq_len // block_q\n",
    "    num_kv_blocks = seq_len // block_kv\n",
    "    ax.text(seq_len/2, -2, f'{num_kv_blocks} K/V blocks', ha='center', fontsize=10)\n",
    "    ax.text(-2, seq_len/2, f'{num_q_blocks} Q blocks', ha='center', va='center', \n",
    "            rotation=90, fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_tiling(seq_len=16, block_q=4, block_kv=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Explore — The Tiling Strategy (15 min)\n",
    "\n",
    "### FlashAttention's Key Insight\n",
    "\n",
    "Standard attention:\n",
    "1. Compute all of QK^T → Store N×N matrix to HBM\n",
    "2. Apply softmax → Read/write N×N from HBM\n",
    "3. Multiply by V → Read N×N, store N×d\n",
    "\n",
    "**Memory I/O:** O(N² + N²) = O(N²)\n",
    "\n",
    "FlashAttention:\n",
    "1. For each Q block:\n",
    "   - For each K/V block:\n",
    "     - Load Q, K, V tiles into SRAM\n",
    "     - Compute partial attention in SRAM\n",
    "     - Update running output using online softmax\n",
    "2. Write final output to HBM\n",
    "\n",
    "**Memory I/O:** O(N × d) = O(N) — linear!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_attention_reference(Q, K, V, block_q, block_kv):\n",
    "    \"\"\"\n",
    "    Reference implementation of tiled attention.\n",
    "    Processes Q in blocks, K/V in blocks, using online softmax.\n",
    "    \n",
    "    Q: [seq_len, d_k]\n",
    "    K: [seq_len, d_k]\n",
    "    V: [seq_len, d_v]\n",
    "    \"\"\"\n",
    "    seq_len, d_k = Q.shape\n",
    "    d_v = V.shape[1]\n",
    "    scale = np.sqrt(d_k)\n",
    "    \n",
    "    # Output matrix\n",
    "    O = np.zeros((seq_len, d_v))\n",
    "    \n",
    "    # Process Q in blocks\n",
    "    for q_start in range(0, seq_len, block_q):\n",
    "        q_end = min(q_start + block_q, seq_len)\n",
    "        q_block_size = q_end - q_start\n",
    "        \n",
    "        # Get Q block\n",
    "        Q_block = Q[q_start:q_end]  # [block_q, d_k]\n",
    "        \n",
    "        # Initialize per-query accumulators for this Q block\n",
    "        m = np.full(q_block_size, float('-inf'))  # [block_q]\n",
    "        l = np.zeros(q_block_size)                 # [block_q]\n",
    "        O_block = np.zeros((q_block_size, d_v))    # [block_q, d_v]\n",
    "        \n",
    "        # Iterate over K/V blocks\n",
    "        for kv_start in range(0, seq_len, block_kv):\n",
    "            kv_end = min(kv_start + block_kv, seq_len)\n",
    "            \n",
    "            # Get K, V blocks\n",
    "            K_block = K[kv_start:kv_end]  # [block_kv, d_k]\n",
    "            V_block = V[kv_start:kv_end]  # [block_kv, d_v]\n",
    "            \n",
    "            # Compute attention scores for this tile\n",
    "            # [block_q, d_k] @ [d_k, block_kv] = [block_q, block_kv]\n",
    "            S = (Q_block @ K_block.T) / scale\n",
    "            \n",
    "            # Online softmax update\n",
    "            m_block = S.max(axis=1)  # [block_q]\n",
    "            m_new = np.maximum(m, m_block)\n",
    "            \n",
    "            # Correction factor for previous accumulator\n",
    "            correction = np.exp(np.where(m == float('-inf'), 0, m - m_new))\n",
    "            \n",
    "            # Update l and O\n",
    "            l = l * correction\n",
    "            O_block = O_block * correction[:, np.newaxis]\n",
    "            \n",
    "            # Add this block's contribution\n",
    "            exp_S = np.exp(S - m_new[:, np.newaxis])  # [block_q, block_kv]\n",
    "            l = l + exp_S.sum(axis=1)\n",
    "            O_block = O_block + exp_S @ V_block  # [block_q, d_v]\n",
    "            \n",
    "            m = m_new\n",
    "        \n",
    "        # Final normalization for this Q block\n",
    "        O[q_start:q_end] = O_block / l[:, np.newaxis]\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "seq_len, d_k, d_v = 16, 8, 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_k)\n",
    "K = np.random.randn(seq_len, d_k)\n",
    "V = np.random.randn(seq_len, d_v)\n",
    "\n",
    "# Standard attention\n",
    "scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "weights = np.exp(scores - scores.max(axis=1, keepdims=True))\n",
    "weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "output_standard = weights @ V\n",
    "\n",
    "# Tiled attention\n",
    "output_tiled = tiled_attention_reference(Q, K, V, block_q=4, block_kv=4)\n",
    "\n",
    "print(f\"Max difference: {np.abs(output_standard - output_tiled).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Causal Masking\n",
    "\n",
    "For causal attention, we need to mask positions where query index < key index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_causal_attention(Q, K, V, block_q, block_kv):\n",
    "    \"\"\"\n",
    "    Tiled attention with causal masking.\n",
    "    \"\"\"\n",
    "    seq_len, d_k = Q.shape\n",
    "    d_v = V.shape[1]\n",
    "    scale = np.sqrt(d_k)\n",
    "    \n",
    "    O = np.zeros((seq_len, d_v))\n",
    "    \n",
    "    for q_start in range(0, seq_len, block_q):\n",
    "        q_end = min(q_start + block_q, seq_len)\n",
    "        q_block_size = q_end - q_start\n",
    "        \n",
    "        Q_block = Q[q_start:q_end]\n",
    "        \n",
    "        m = np.full(q_block_size, float('-inf'))\n",
    "        l = np.zeros(q_block_size)\n",
    "        O_block = np.zeros((q_block_size, d_v))\n",
    "        \n",
    "        for kv_start in range(0, seq_len, block_kv):\n",
    "            kv_end = min(kv_start + block_kv, seq_len)\n",
    "            \n",
    "            # Skip blocks entirely in the future (optimization)\n",
    "            if kv_start > q_end - 1:\n",
    "                break\n",
    "            \n",
    "            K_block = K[kv_start:kv_end]\n",
    "            V_block = V[kv_start:kv_end]\n",
    "            \n",
    "            # Compute scores\n",
    "            S = (Q_block @ K_block.T) / scale\n",
    "            \n",
    "            # Apply causal mask\n",
    "            # Create mask where True means \"mask this position\"\n",
    "            q_indices = np.arange(q_start, q_end)[:, np.newaxis]  # [block_q, 1]\n",
    "            k_indices = np.arange(kv_start, kv_end)[np.newaxis, :]  # [1, block_kv]\n",
    "            causal_mask = k_indices > q_indices  # [block_q, block_kv]\n",
    "            \n",
    "            S = np.where(causal_mask, float('-inf'), S)\n",
    "            \n",
    "            # Online softmax update\n",
    "            m_block = np.where(np.all(causal_mask, axis=1), float('-inf'), \n",
    "                               np.max(np.where(causal_mask, float('-inf'), S), axis=1))\n",
    "            m_new = np.maximum(m, m_block)\n",
    "            \n",
    "            correction = np.exp(np.where(m == float('-inf'), 0, m - m_new))\n",
    "            \n",
    "            l = l * correction\n",
    "            O_block = O_block * correction[:, np.newaxis]\n",
    "            \n",
    "            exp_S = np.exp(np.where(causal_mask, float('-inf'), S) - m_new[:, np.newaxis])\n",
    "            exp_S = np.where(np.isinf(exp_S) | np.isnan(exp_S), 0, exp_S)\n",
    "            \n",
    "            l = l + exp_S.sum(axis=1)\n",
    "            O_block = O_block + exp_S @ V_block\n",
    "            \n",
    "            m = m_new\n",
    "        \n",
    "        # Avoid division by zero for rows that are fully masked\n",
    "        l = np.where(l == 0, 1, l)\n",
    "        O[q_start:q_end] = O_block / l[:, np.newaxis]\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Test causal\n",
    "# Standard causal attention\n",
    "scores = (Q @ K.T) / np.sqrt(d_k)\n",
    "causal_mask = np.triu(np.ones((seq_len, seq_len)), k=1).astype(bool)\n",
    "scores_masked = np.where(causal_mask, float('-inf'), scores)\n",
    "weights = np.exp(scores_masked - scores_masked.max(axis=1, keepdims=True))\n",
    "weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "output_standard_causal = weights @ V\n",
    "\n",
    "# Tiled causal attention\n",
    "output_tiled_causal = tiled_causal_attention(Q, K, V, block_q=4, block_kv=4)\n",
    "\n",
    "print(f\"Causal max difference: {np.abs(output_standard_causal - output_tiled_causal).max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: The Concept — Memory I/O Analysis (10 min)\n",
    "\n",
    "### Standard Attention I/O\n",
    "\n",
    "```\n",
    "S = QK^T                    # Write N×N to HBM\n",
    "P = softmax(S)              # Read N×N, write N×N\n",
    "O = PV                      # Read N×N + N×d, write N×d\n",
    "```\n",
    "\n",
    "Total HBM reads: O(N² + N² + N²) = O(N²)\n",
    "Total HBM writes: O(N² + N² + N×d) = O(N²)\n",
    "\n",
    "### Tiled (FlashAttention) I/O\n",
    "\n",
    "```\n",
    "For each Q block:\n",
    "  Load Q block once: N/Bq × Bq × d = N×d\n",
    "  For each K/V block:\n",
    "    Load K, V blocks: N/Bkv × (Bkv × d + Bkv × d)\n",
    "    Compute in SRAM (no HBM access)\n",
    "  Write O block: Bq × d\n",
    "```\n",
    "\n",
    "Total: O(N × d) reads, O(N × d) writes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def io_analysis(seq_len, d_model, block_q, block_kv, dtype_bytes=2):\n",
    "    \"\"\"\n",
    "    Analyze HBM I/O for standard vs tiled attention.\n",
    "    \"\"\"\n",
    "    # Standard attention\n",
    "    # Read Q, K for S=QK^T: 2 * N * d\n",
    "    # Write S: N * N\n",
    "    # Read S for softmax: N * N\n",
    "    # Write P: N * N\n",
    "    # Read P, V for O=PV: N * N + N * d\n",
    "    # Write O: N * d\n",
    "    standard_reads = (2 * seq_len * d_model + seq_len * seq_len + \n",
    "                     seq_len * seq_len + seq_len * d_model) * dtype_bytes\n",
    "    standard_writes = (seq_len * seq_len + seq_len * seq_len + \n",
    "                      seq_len * d_model) * dtype_bytes\n",
    "    standard_total = standard_reads + standard_writes\n",
    "    \n",
    "    # Tiled (FlashAttention)\n",
    "    # Outer loop: N/Bq iterations\n",
    "    # Each outer: read Q block (Bq * d), write O block (Bq * d)\n",
    "    # Inner loop: N/Bkv iterations\n",
    "    # Each inner: read K block (Bkv * d), read V block (Bkv * d)\n",
    "    \n",
    "    num_q_blocks = seq_len // block_q\n",
    "    num_kv_blocks = seq_len // block_kv\n",
    "    \n",
    "    # Q is loaded once per Q block\n",
    "    q_reads = num_q_blocks * block_q * d_model * dtype_bytes\n",
    "    # K, V are loaded for each (Q block, KV block) pair\n",
    "    kv_reads = num_q_blocks * num_kv_blocks * 2 * block_kv * d_model * dtype_bytes\n",
    "    # O is written once per Q block\n",
    "    o_writes = seq_len * d_model * dtype_bytes\n",
    "    \n",
    "    tiled_total = q_reads + kv_reads + o_writes\n",
    "    \n",
    "    return {\n",
    "        'seq_len': seq_len,\n",
    "        'standard_gb': standard_total / 1e9,\n",
    "        'tiled_gb': tiled_total / 1e9,\n",
    "        'reduction': standard_total / tiled_total if tiled_total > 0 else float('inf')\n",
    "    }\n",
    "\n",
    "print(f\"{'Seq Len':>10} {'Standard':>12} {'Tiled':>12} {'Reduction':>12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for seq_len in [256, 512, 1024, 2048, 4096, 8192]:\n",
    "    stats = io_analysis(seq_len, d_model=128, block_q=64, block_kv=64)\n",
    "    print(f\"{stats['seq_len']:>10} {stats['standard_gb']:>10.3f}GB \"\n",
    "          f\"{stats['tiled_gb']:>10.3f}GB {stats['reduction']:>10.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Code It — PyTorch Tiled Attention (30 min)\n",
    "\n",
    "### Full Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tiled_attention_torch(Q, K, V, block_q, block_kv, causal=False):\n",
    "    \"\"\"\n",
    "    Tiled attention in PyTorch.\n",
    "    \n",
    "    Q, K, V: [batch, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_k = Q.shape\n",
    "    d_v = V.shape[-1]\n",
    "    scale = d_k ** 0.5\n",
    "    \n",
    "    O = torch.zeros_like(V)\n",
    "    \n",
    "    for q_start in range(0, seq_len, block_q):\n",
    "        q_end = min(q_start + block_q, seq_len)\n",
    "        q_block_size = q_end - q_start\n",
    "        \n",
    "        Q_block = Q[:, q_start:q_end, :]  # [batch, block_q, d_k]\n",
    "        \n",
    "        # Initialize accumulators\n",
    "        m = torch.full((batch, q_block_size), float('-inf'), device=Q.device)\n",
    "        l = torch.zeros((batch, q_block_size), device=Q.device)\n",
    "        O_block = torch.zeros((batch, q_block_size, d_v), device=Q.device)\n",
    "        \n",
    "        kv_end_limit = q_end if causal else seq_len\n",
    "        \n",
    "        for kv_start in range(0, seq_len, block_kv):\n",
    "            kv_end = min(kv_start + block_kv, seq_len)\n",
    "            \n",
    "            # Early exit for fully masked blocks\n",
    "            if causal and kv_start >= q_end:\n",
    "                break\n",
    "            \n",
    "            K_block = K[:, kv_start:kv_end, :]  # [batch, block_kv, d_k]\n",
    "            V_block = V[:, kv_start:kv_end, :]  # [batch, block_kv, d_v]\n",
    "            \n",
    "            # Compute scores: [batch, block_q, block_kv]\n",
    "            S = torch.bmm(Q_block, K_block.transpose(-2, -1)) / scale\n",
    "            \n",
    "            # Apply causal mask\n",
    "            if causal:\n",
    "                q_indices = torch.arange(q_start, q_end, device=Q.device).view(-1, 1)\n",
    "                k_indices = torch.arange(kv_start, kv_end, device=Q.device).view(1, -1)\n",
    "                mask = k_indices > q_indices  # [block_q, block_kv]\n",
    "                S = S.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
    "            \n",
    "            # Online softmax\n",
    "            m_block = S.max(dim=-1).values  # [batch, block_q]\n",
    "            m_new = torch.maximum(m, m_block)\n",
    "            \n",
    "            # Handle -inf case\n",
    "            correction = torch.exp(torch.where(\n",
    "                m == float('-inf'),\n",
    "                torch.zeros_like(m),\n",
    "                m - m_new\n",
    "            ))\n",
    "            \n",
    "            l = l * correction\n",
    "            O_block = O_block * correction.unsqueeze(-1)\n",
    "            \n",
    "            exp_S = torch.exp(S - m_new.unsqueeze(-1))\n",
    "            exp_S = torch.where(torch.isinf(S), torch.zeros_like(exp_S), exp_S)\n",
    "            \n",
    "            l = l + exp_S.sum(dim=-1)\n",
    "            O_block = O_block + torch.bmm(exp_S, V_block)\n",
    "            \n",
    "            m = m_new\n",
    "        \n",
    "        # Normalize and store\n",
    "        l = torch.where(l == 0, torch.ones_like(l), l)\n",
    "        O[:, q_start:q_end, :] = O_block / l.unsqueeze(-1)\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Test\n",
    "batch, seq_len, d_model = 2, 32, 64\n",
    "Q = torch.randn(batch, seq_len, d_model)\n",
    "K = torch.randn(batch, seq_len, d_model)\n",
    "V = torch.randn(batch, seq_len, d_model)\n",
    "\n",
    "# Standard attention\n",
    "scores = torch.bmm(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "# Causal mask\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "scores_masked = scores.masked_fill(mask, float('-inf'))\n",
    "weights = F.softmax(scores_masked, dim=-1)\n",
    "output_standard = torch.bmm(weights, V)\n",
    "\n",
    "# Tiled attention\n",
    "output_tiled = tiled_attention_torch(Q, K, V, block_q=8, block_kv=8, causal=True)\n",
    "\n",
    "print(f\"Max difference: {(output_standard - output_tiled).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark: Standard vs Tiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention(Q, K, V, num_runs=100):\n",
    "    \"\"\"Benchmark standard vs tiled attention.\"\"\"\n",
    "    # Standard attention\n",
    "    def standard_attn():\n",
    "        scores = torch.bmm(Q, K.transpose(-2, -1)) / (Q.shape[-1] ** 0.5)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        return torch.bmm(weights, V)\n",
    "    \n",
    "    # Tiled attention\n",
    "    def tiled_attn():\n",
    "        return tiled_attention_torch(Q, K, V, block_q=64, block_kv=64)\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = standard_attn()\n",
    "        _ = tiled_attn()\n",
    "    \n",
    "    if Q.is_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    # Benchmark standard\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        _ = standard_attn()\n",
    "    if Q.is_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    standard_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "    \n",
    "    # Benchmark tiled\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(num_runs):\n",
    "        _ = tiled_attn()\n",
    "    if Q.is_cuda:\n",
    "        torch.cuda.synchronize()\n",
    "    tiled_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "    \n",
    "    return standard_time, tiled_time\n",
    "\n",
    "# Run benchmark on CPU (tiled will be slower due to Python overhead)\n",
    "print(\"CPU Benchmark (illustrates memory pattern, not optimized):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for seq_len in [64, 128, 256, 512]:\n",
    "    Q = torch.randn(1, seq_len, 64)\n",
    "    K = torch.randn(1, seq_len, 64)\n",
    "    V = torch.randn(1, seq_len, 64)\n",
    "    \n",
    "    std_time, tiled_time = benchmark_attention(Q, K, V, num_runs=10)\n",
    "    print(f\"seq_len={seq_len:4d}: standard={std_time:7.2f}ms, tiled={tiled_time:7.2f}ms\")\n",
    "\n",
    "print(\"\\nNote: Tiled is slower in Python. Real gains come from Triton/CUDA implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Usage Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Memory Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    d_model = 64\n",
    "    \n",
    "    for seq_len in [256, 512, 1024, 2048]:\n",
    "        # Standard attention\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        Q = torch.randn(1, seq_len, d_model, device='cuda')\n",
    "        K = torch.randn(1, seq_len, d_model, device='cuda')\n",
    "        V = torch.randn(1, seq_len, d_model, device='cuda')\n",
    "        \n",
    "        scores = torch.bmm(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        _ = torch.bmm(weights, V)\n",
    "        torch.cuda.synchronize()\n",
    "        standard_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        \n",
    "        # Tiled attention\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        Q = torch.randn(1, seq_len, d_model, device='cuda')\n",
    "        K = torch.randn(1, seq_len, d_model, device='cuda')\n",
    "        V = torch.randn(1, seq_len, d_model, device='cuda')\n",
    "        \n",
    "        _ = tiled_attention_torch(Q, K, V, block_q=64, block_kv=64)\n",
    "        torch.cuda.synchronize()\n",
    "        tiled_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        \n",
    "        print(f\"seq_len={seq_len:5d}: standard={standard_mem:8.1f}MB, tiled={tiled_mem:8.1f}MB, \"\n",
    "              f\"savings={standard_mem/tiled_mem:.1f}x\")\n",
    "        \n",
    "        del Q, K, V\n",
    "else:\n",
    "    print(\"GPU not available for memory comparison.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Quiz & Reflection (10 min)\n",
    "\n",
    "### Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(question, your_answer, correct_answer):\n",
    "    if your_answer == correct_answer:\n",
    "        print(f\"✓ Correct! {question}\")\n",
    "    else:\n",
    "        print(f\"✗ Incorrect. Your answer: {your_answer}, Correct: {correct_answer}\")\n",
    "\n",
    "# Q1: Why does tiled attention reduce memory I/O?\n",
    "# a) It uses compression\n",
    "# b) It never stores the N×N attention matrix\n",
    "# c) It uses smaller data types\n",
    "# d) It skips some computations\n",
    "q1_answer = 'b'\n",
    "check_answer(\"Memory I/O reduction\", q1_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: In tiled attention, what determines how much SRAM is needed?\n",
    "# a) Total sequence length\n",
    "# b) Block sizes (block_q, block_kv)\n",
    "# c) Number of attention heads\n",
    "# d) Batch size\n",
    "q2_answer = 'b'\n",
    "check_answer(\"SRAM requirement\", q2_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3: For causal attention, which K/V blocks can be skipped entirely?\n",
    "# a) Blocks where kv_start < q_start\n",
    "# b) Blocks where kv_start >= q_end\n",
    "# c) All diagonal blocks\n",
    "# d) None can be skipped\n",
    "q3_answer = 'b'\n",
    "check_answer(\"Causal block skipping\", q3_answer, 'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4: What is the I/O complexity of tiled attention?\n",
    "# a) O(N²)\n",
    "# b) O(N log N)\n",
    "# c) O(N × d)\n",
    "# d) O(d²)\n",
    "q4_answer = 'c'\n",
    "check_answer(\"I/O complexity\", q4_answer, 'c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection Questions\n",
    "\n",
    "1. **Block size tradeoff:** Larger blocks mean more SRAM usage but fewer iterations. How would you choose optimal block sizes?\n",
    "\n",
    "2. **K/V reuse:** In our implementation, K/V are loaded once per (Q block, KV block) pair. Can we do better?\n",
    "\n",
    "3. **Parallelism:** Each Q block is independent. How does this affect GPU parallelization?\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "| Concept | Key Insight |\n",
    "|---------|------------|\n",
    "| Tiling | Process attention in blocks that fit in SRAM |\n",
    "| Fusion | Combine QK^T, softmax, and ×V into one kernel |\n",
    "| Online softmax | Enables block-by-block processing with correct results |\n",
    "| I/O reduction | O(N²) → O(N×d) by not materializing attention matrix |\n",
    "\n",
    "**Tomorrow:** The complete FlashAttention algorithm with all optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "**Interactive Reference:** [lessons/memory-hierarchy.html](../lessons/memory-hierarchy.html) — GPU Memory Hierarchy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
