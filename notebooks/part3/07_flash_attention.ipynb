{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3, Day 7: FlashAttention — The Complete Implementation\n",
    "\n",
    "**Time:** ~1.5 hours\n",
    "\n",
    "**Goal:** Implement the complete FlashAttention algorithm in Triton, achieving O(N) memory and competitive speed.\n",
    "\n",
    "## The Journey Complete\n",
    "\n",
    "This week we built up to this moment:\n",
    "- **Day 1:** Dot products — the foundation of attention\n",
    "- **Day 2:** Softmax overflow — why naive implementation fails\n",
    "- **Day 3:** Stable softmax — the max-subtraction trick\n",
    "- **Day 4:** Full attention — the quadratic memory problem\n",
    "- **Day 5:** Online softmax — streaming computation\n",
    "- **Day 6:** Tiled attention — block-wise processing\n",
    "- **Day 7:** FlashAttention — putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import time\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: The Challenge — FlashAttention Goals (5 min)\n",
    "\n",
    "FlashAttention achieves:\n",
    "\n",
    "| Metric | Standard | FlashAttention |\n",
    "|--------|----------|----------------|\n",
    "| Memory | O(N²) | O(N) |\n",
    "| I/O | O(N²) | O(N²/M) where M = SRAM size |\n",
    "| Speed | Baseline | 2-4x faster |\n",
    "\n",
    "**Key innovations:**\n",
    "1. Online softmax with output rescaling\n",
    "2. Tiling to fit in SRAM\n",
    "3. Fused kernel (no intermediate writes to HBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: The Algorithm — FlashAttention Pseudocode (15 min)\n",
    "\n",
    "### FlashAttention Forward Pass\n",
    "\n",
    "```\n",
    "Input: Q, K, V ∈ R^(N×d), block sizes B_q, B_kv\n",
    "Output: O ∈ R^(N×d)\n",
    "\n",
    "1. Divide Q into T_q = N/B_q blocks, K,V into T_kv = N/B_kv blocks\n",
    "2. For each Q block i = 1...T_q:\n",
    "   a. Load Q_i from HBM to SRAM\n",
    "   b. Initialize: O_i = 0, l_i = 0, m_i = -∞ (all in SRAM)\n",
    "   c. For each K,V block j = 1...T_kv:\n",
    "      i.   Load K_j, V_j from HBM to SRAM\n",
    "      ii.  Compute S_ij = Q_i × K_j^T (in SRAM)\n",
    "      iii. Compute m̃_ij = rowmax(S_ij)\n",
    "      iv.  Compute P̃_ij = exp(S_ij - m̃_ij)\n",
    "      v.   Compute l̃_ij = rowsum(P̃_ij)\n",
    "      vi.  Compute m_new = max(m_i, m̃_ij)\n",
    "      vii. Compute l_new = e^(m_i - m_new) × l_i + e^(m̃_ij - m_new) × l̃_ij\n",
    "      viii.Update O_i = (l_i × e^(m_i - m_new) × O_i + e^(m̃_ij - m_new) × P̃_ij × V_j) / l_new\n",
    "      ix.  Update m_i = m_new, l_i = l_new\n",
    "   d. Write O_i to HBM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_reference(Q, K, V, block_q=64, block_kv=64, causal=False):\n",
    "    \"\"\"\n",
    "    Reference FlashAttention implementation (Python).\n",
    "    Follows the algorithm exactly for clarity.\n",
    "    \"\"\"\n",
    "    batch, seq_len, d = Q.shape\n",
    "    scale = d ** -0.5\n",
    "    \n",
    "    O = torch.zeros_like(Q)\n",
    "    L = torch.zeros(batch, seq_len, device=Q.device)  # For debugging: stores final l\n",
    "    M = torch.full((batch, seq_len), float('-inf'), device=Q.device)  # Final m\n",
    "    \n",
    "    # Number of blocks\n",
    "    T_q = (seq_len + block_q - 1) // block_q\n",
    "    T_kv = (seq_len + block_kv - 1) // block_kv\n",
    "    \n",
    "    for i in range(T_q):  # Loop over Q blocks\n",
    "        q_start = i * block_q\n",
    "        q_end = min(q_start + block_q, seq_len)\n",
    "        \n",
    "        # Load Q block\n",
    "        Q_i = Q[:, q_start:q_end, :]  # [batch, B_q, d]\n",
    "        \n",
    "        # Initialize accumulators for this Q block\n",
    "        O_i = torch.zeros_like(Q_i)  # [batch, B_q, d]\n",
    "        l_i = torch.zeros(batch, q_end - q_start, device=Q.device)  # [batch, B_q]\n",
    "        m_i = torch.full((batch, q_end - q_start), float('-inf'), device=Q.device)\n",
    "        \n",
    "        for j in range(T_kv):  # Loop over K,V blocks\n",
    "            kv_start = j * block_kv\n",
    "            kv_end = min(kv_start + block_kv, seq_len)\n",
    "            \n",
    "            # Causal: skip blocks entirely in the future\n",
    "            if causal and kv_start >= q_end:\n",
    "                break\n",
    "            \n",
    "            # Load K, V blocks\n",
    "            K_j = K[:, kv_start:kv_end, :]  # [batch, B_kv, d]\n",
    "            V_j = V[:, kv_start:kv_end, :]  # [batch, B_kv, d]\n",
    "            \n",
    "            # Compute attention scores\n",
    "            S_ij = torch.bmm(Q_i, K_j.transpose(-2, -1)) * scale  # [batch, B_q, B_kv]\n",
    "            \n",
    "            # Apply causal mask\n",
    "            if causal:\n",
    "                q_idx = torch.arange(q_start, q_end, device=Q.device).view(-1, 1)\n",
    "                k_idx = torch.arange(kv_start, kv_end, device=Q.device).view(1, -1)\n",
    "                mask = k_idx > q_idx  # [B_q, B_kv]\n",
    "                S_ij = S_ij.masked_fill(mask.unsqueeze(0), float('-inf'))\n",
    "            \n",
    "            # Block max and exp\n",
    "            m_ij = S_ij.max(dim=-1).values  # [batch, B_q]\n",
    "            P_ij = torch.exp(S_ij - m_ij.unsqueeze(-1))  # [batch, B_q, B_kv]\n",
    "            P_ij = torch.where(torch.isinf(S_ij), torch.zeros_like(P_ij), P_ij)\n",
    "            l_ij = P_ij.sum(dim=-1)  # [batch, B_q]\n",
    "            \n",
    "            # Update max\n",
    "            m_new = torch.maximum(m_i, m_ij)\n",
    "            \n",
    "            # Compute scaling factors\n",
    "            alpha = torch.exp(m_i - m_new)  # Scale for old accumulator\n",
    "            alpha = torch.where(m_i == float('-inf'), torch.zeros_like(alpha), alpha)\n",
    "            beta = torch.exp(m_ij - m_new)   # Scale for new block\n",
    "            beta = torch.where(m_ij == float('-inf'), torch.zeros_like(beta), beta)\n",
    "            \n",
    "            # Update sum\n",
    "            l_new = alpha * l_i + beta * l_ij\n",
    "            \n",
    "            # Update output\n",
    "            # O_new = (l_i * alpha * O_i + beta * P_ij @ V_j) / l_new\n",
    "            O_i = (l_i.unsqueeze(-1) * alpha.unsqueeze(-1) * O_i + \n",
    "                   beta.unsqueeze(-1) * torch.bmm(P_ij, V_j))\n",
    "            \n",
    "            # Normalize (avoid div by zero)\n",
    "            l_new_safe = torch.where(l_new == 0, torch.ones_like(l_new), l_new)\n",
    "            O_i = O_i / l_new_safe.unsqueeze(-1)\n",
    "            \n",
    "            # Update accumulators\n",
    "            m_i = m_new\n",
    "            l_i = l_new\n",
    "        \n",
    "        # Store results\n",
    "        O[:, q_start:q_end, :] = O_i\n",
    "        L[:, q_start:q_end] = l_i\n",
    "        M[:, q_start:q_end] = m_i\n",
    "    \n",
    "    return O\n",
    "\n",
    "# Test\n",
    "torch.manual_seed(42)\n",
    "batch, seq_len, d = 2, 64, 32\n",
    "Q = torch.randn(batch, seq_len, d)\n",
    "K = torch.randn(batch, seq_len, d)\n",
    "V = torch.randn(batch, seq_len, d)\n",
    "\n",
    "# Standard attention\n",
    "scores = torch.bmm(Q, K.transpose(-2, -1)) / (d ** 0.5)\n",
    "mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "scores = scores.masked_fill(mask, float('-inf'))\n",
    "weights = F.softmax(scores, dim=-1)\n",
    "output_std = torch.bmm(weights, V)\n",
    "\n",
    "# FlashAttention\n",
    "output_flash = flash_attention_reference(Q, K, V, block_q=16, block_kv=16, causal=True)\n",
    "\n",
    "print(f\"Max difference: {(output_std - output_flash).abs().max():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Triton Implementation (30 min)\n",
    "\n",
    "### FlashAttention Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def flash_attention_kernel(\n",
    "    Q_ptr, K_ptr, V_ptr, O_ptr,\n",
    "    stride_qb, stride_qm, stride_qk,\n",
    "    stride_kb, stride_kn, stride_kk,\n",
    "    stride_vb, stride_vn, stride_vd,\n",
    "    stride_ob, stride_om, stride_od,\n",
    "    seq_len, d_model,\n",
    "    scale,\n",
    "    BLOCK_M: tl.constexpr,  # Q block size\n",
    "    BLOCK_N: tl.constexpr,  # K/V block size\n",
    "    BLOCK_D: tl.constexpr,  # Head dimension (must cover full d)\n",
    "    CAUSAL: tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "    FlashAttention forward kernel.\n",
    "    \n",
    "    Grid: (num_q_blocks, batch)\n",
    "    Each program processes one Q block for one batch element.\n",
    "    \"\"\"\n",
    "    # Program IDs\n",
    "    q_block_idx = tl.program_id(0)\n",
    "    batch_idx = tl.program_id(1)\n",
    "    \n",
    "    # Compute Q block start position\n",
    "    q_start = q_block_idx * BLOCK_M\n",
    "    \n",
    "    # Offsets within the block\n",
    "    offs_m = q_start + tl.arange(0, BLOCK_M)  # Q positions\n",
    "    offs_n = tl.arange(0, BLOCK_N)  # K/V positions (will be updated in loop)\n",
    "    offs_d = tl.arange(0, BLOCK_D)  # Head dimension\n",
    "    \n",
    "    # Pointers to Q block (stays constant)\n",
    "    q_ptrs = (Q_ptr + \n",
    "              batch_idx * stride_qb + \n",
    "              offs_m[:, None] * stride_qm + \n",
    "              offs_d[None, :] * stride_qk)\n",
    "    \n",
    "    # Load Q block\n",
    "    q_mask = (offs_m[:, None] < seq_len) & (offs_d[None, :] < d_model)\n",
    "    Q_block = tl.load(q_ptrs, mask=q_mask, other=0.0)\n",
    "    \n",
    "    # Initialize accumulators\n",
    "    m_i = tl.full([BLOCK_M], float('-inf'), dtype=tl.float32)  # Max scores\n",
    "    l_i = tl.zeros([BLOCK_M], dtype=tl.float32)  # Sum of exp\n",
    "    O_i = tl.zeros([BLOCK_M, BLOCK_D], dtype=tl.float32)  # Output accumulator\n",
    "    \n",
    "    # Determine K/V block range\n",
    "    if CAUSAL:\n",
    "        kv_block_end = (q_start + BLOCK_M + BLOCK_N - 1) // BLOCK_N\n",
    "    else:\n",
    "        kv_block_end = (seq_len + BLOCK_N - 1) // BLOCK_N\n",
    "    \n",
    "    # Loop over K/V blocks\n",
    "    for kv_block_idx in range(0, kv_block_end):\n",
    "        kv_start = kv_block_idx * BLOCK_N\n",
    "        offs_n = kv_start + tl.arange(0, BLOCK_N)\n",
    "        \n",
    "        # Pointers to K, V blocks\n",
    "        k_ptrs = (K_ptr + \n",
    "                  batch_idx * stride_kb + \n",
    "                  offs_n[:, None] * stride_kn + \n",
    "                  offs_d[None, :] * stride_kk)\n",
    "        v_ptrs = (V_ptr + \n",
    "                  batch_idx * stride_vb + \n",
    "                  offs_n[:, None] * stride_vn + \n",
    "                  offs_d[None, :] * stride_vd)\n",
    "        \n",
    "        # Load K, V blocks\n",
    "        kv_mask = (offs_n[:, None] < seq_len) & (offs_d[None, :] < d_model)\n",
    "        K_block = tl.load(k_ptrs, mask=kv_mask, other=0.0)\n",
    "        V_block = tl.load(v_ptrs, mask=kv_mask, other=0.0)\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T\n",
    "        # [BLOCK_M, BLOCK_D] @ [BLOCK_D, BLOCK_N] = [BLOCK_M, BLOCK_N]\n",
    "        S = tl.dot(Q_block, tl.trans(K_block)) * scale\n",
    "        \n",
    "        # Apply causal mask\n",
    "        if CAUSAL:\n",
    "            causal_mask = offs_m[:, None] < offs_n[None, :]\n",
    "            S = tl.where(causal_mask, float('-inf'), S)\n",
    "        \n",
    "        # Also mask out-of-bounds positions\n",
    "        S = tl.where(offs_n[None, :] >= seq_len, float('-inf'), S)\n",
    "        \n",
    "        # Compute block max and exp\n",
    "        m_ij = tl.max(S, axis=1)  # [BLOCK_M]\n",
    "        P = tl.exp(S - m_ij[:, None])  # [BLOCK_M, BLOCK_N]\n",
    "        l_ij = tl.sum(P, axis=1)  # [BLOCK_M]\n",
    "        \n",
    "        # Update max\n",
    "        m_new = tl.maximum(m_i, m_ij)\n",
    "        \n",
    "        # Compute scaling factors\n",
    "        alpha = tl.exp(m_i - m_new)\n",
    "        beta = tl.exp(m_ij - m_new)\n",
    "        \n",
    "        # Update sum\n",
    "        l_new = alpha * l_i + beta * l_ij\n",
    "        \n",
    "        # Update output\n",
    "        # Scale old output\n",
    "        O_i = O_i * (alpha * l_i)[:, None]\n",
    "        # Add new contribution: beta * P @ V\n",
    "        PV = tl.dot(P.to(V_block.dtype), V_block)  # [BLOCK_M, BLOCK_D]\n",
    "        O_i = O_i + beta[:, None] * PV\n",
    "        # Normalize\n",
    "        O_i = O_i / l_new[:, None]\n",
    "        \n",
    "        # Update accumulators\n",
    "        m_i = m_new\n",
    "        l_i = l_new\n",
    "    \n",
    "    # Store output\n",
    "    o_ptrs = (O_ptr + \n",
    "              batch_idx * stride_ob + \n",
    "              offs_m[:, None] * stride_om + \n",
    "              offs_d[None, :] * stride_od)\n",
    "    o_mask = (offs_m[:, None] < seq_len) & (offs_d[None, :] < d_model)\n",
    "    tl.store(o_ptrs, O_i.to(O_ptr.dtype.element_ty), mask=o_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_triton(Q, K, V, causal=False):\n",
    "    \"\"\"\n",
    "    FlashAttention using Triton kernel.\n",
    "    \n",
    "    Q, K, V: [batch, seq_len, d_model]\n",
    "    \"\"\"\n",
    "    batch, seq_len, d_model = Q.shape\n",
    "    scale = d_model ** -0.5\n",
    "    \n",
    "    # Output tensor\n",
    "    O = torch.empty_like(Q)\n",
    "    \n",
    "    # Block sizes\n",
    "    BLOCK_M = 64\n",
    "    BLOCK_N = 64\n",
    "    BLOCK_D = triton.next_power_of_2(d_model)\n",
    "    \n",
    "    # Grid\n",
    "    num_q_blocks = (seq_len + BLOCK_M - 1) // BLOCK_M\n",
    "    grid = (num_q_blocks, batch)\n",
    "    \n",
    "    # Launch kernel\n",
    "    flash_attention_kernel[grid](\n",
    "        Q, K, V, O,\n",
    "        Q.stride(0), Q.stride(1), Q.stride(2),\n",
    "        K.stride(0), K.stride(1), K.stride(2),\n",
    "        V.stride(0), V.stride(1), V.stride(2),\n",
    "        O.stride(0), O.stride(1), O.stride(2),\n",
    "        seq_len, d_model,\n",
    "        scale,\n",
    "        BLOCK_M=BLOCK_M,\n",
    "        BLOCK_N=BLOCK_N,\n",
    "        BLOCK_D=BLOCK_D,\n",
    "        CAUSAL=causal,\n",
    "    )\n",
    "    \n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Triton implementation\n",
    "if torch.cuda.is_available():\n",
    "    torch.manual_seed(42)\n",
    "    batch, seq_len, d = 2, 128, 64\n",
    "    \n",
    "    Q = torch.randn(batch, seq_len, d, device='cuda')\n",
    "    K = torch.randn(batch, seq_len, d, device='cuda')\n",
    "    V = torch.randn(batch, seq_len, d, device='cuda')\n",
    "    \n",
    "    # Standard attention (causal)\n",
    "    scores = torch.bmm(Q, K.transpose(-2, -1)) / (d ** 0.5)\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device='cuda'), diagonal=1).bool()\n",
    "    scores = scores.masked_fill(mask, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output_std = torch.bmm(weights, V)\n",
    "    \n",
    "    # Triton FlashAttention\n",
    "    output_flash = flash_attention_triton(Q, K, V, causal=True)\n",
    "    \n",
    "    print(f\"Max difference: {(output_std - output_flash).abs().max():.2e}\")\n",
    "    print(f\"Mean difference: {(output_std - output_flash).abs().mean():.2e}\")\n",
    "else:\n",
    "    print(\"GPU not available. Triton test skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Benchmark and Analysis (20 min)\n",
    "\n",
    "### Speed Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_attention_implementations(seq_lengths, d_model=64, batch=4, num_runs=100):\n",
    "    \"\"\"Benchmark different attention implementations.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        Q = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        K = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        V = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        \n",
    "        # Warmup\n",
    "        for _ in range(10):\n",
    "            _ = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "            _ = flash_attention_triton(Q, K, V, causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        \n",
    "        # Benchmark PyTorch SDPA\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(num_runs):\n",
    "            _ = F.scaled_dot_product_attention(Q, K, V, is_causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        pytorch_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "        \n",
    "        # Benchmark our FlashAttention\n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        for _ in range(num_runs):\n",
    "            _ = flash_attention_triton(Q, K, V, causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        triton_time = (time.perf_counter() - start) / num_runs * 1000\n",
    "        \n",
    "        results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'pytorch_ms': pytorch_time,\n",
    "            'triton_ms': triton_time,\n",
    "            'ratio': triton_time / pytorch_time\n",
    "        })\n",
    "        \n",
    "        del Q, K, V\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Speed Benchmark: PyTorch SDPA vs Our Triton FlashAttention\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    results = benchmark_attention_implementations(\n",
    "        seq_lengths=[128, 256, 512, 1024, 2048],\n",
    "        d_model=64,\n",
    "        batch=4\n",
    "    )\n",
    "    \n",
    "    print(f\"{'Seq Len':>10} {'PyTorch SDPA':>15} {'Our Triton':>15} {'Ratio':>10}\")\n",
    "    print(\"-\" * 55)\n",
    "    for r in results:\n",
    "        print(f\"{r['seq_len']:>10} {r['pytorch_ms']:>13.3f}ms {r['triton_ms']:>13.3f}ms {r['ratio']:>9.2f}x\")\n",
    "    \n",
    "    print(\"\\nNote: PyTorch SDPA uses optimized FlashAttention internally.\")\n",
    "    print(\"Our implementation is for learning; production should use torch.nn.functional.scaled_dot_product_attention\")\n",
    "else:\n",
    "    print(\"GPU not available for benchmarking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_benchmark(seq_lengths, d_model=64, batch=1):\n",
    "    \"\"\"Compare memory usage.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Standard attention (materializes N×N matrix)\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        Q = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        K = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        V = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        \n",
    "        # Force standard computation path by computing scores explicitly\n",
    "        scores = torch.bmm(Q, K.transpose(-2, -1)) / (d_model ** 0.5)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        _ = torch.bmm(weights, V)\n",
    "        torch.cuda.synchronize()\n",
    "        standard_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        \n",
    "        del scores, weights, Q, K, V\n",
    "        \n",
    "        # FlashAttention\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        \n",
    "        Q = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        K = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        V = torch.randn(batch, seq_len, d_model, device='cuda')\n",
    "        \n",
    "        _ = flash_attention_triton(Q, K, V, causal=True)\n",
    "        torch.cuda.synchronize()\n",
    "        flash_mem = torch.cuda.max_memory_allocated() / 1e6\n",
    "        \n",
    "        del Q, K, V\n",
    "        \n",
    "        # Theoretical attention matrix size\n",
    "        attn_matrix_mb = batch * seq_len * seq_len * 4 / 1e6  # FP32\n",
    "        \n",
    "        results.append({\n",
    "            'seq_len': seq_len,\n",
    "            'standard_mb': standard_mem,\n",
    "            'flash_mb': flash_mem,\n",
    "            'attn_matrix_mb': attn_matrix_mb,\n",
    "            'savings': standard_mem / flash_mem if flash_mem > 0 else float('inf')\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nMemory Benchmark: Standard vs FlashAttention\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    mem_results = memory_benchmark([128, 256, 512, 1024, 2048, 4096])\n",
    "    \n",
    "    print(f\"{'Seq Len':>10} {'Standard':>12} {'Flash':>12} {'Attn Matrix':>14} {'Savings':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    for r in mem_results:\n",
    "        print(f\"{r['seq_len']:>10} {r['standard_mb']:>10.1f}MB {r['flash_mb']:>10.1f}MB \"\n",
    "              f\"{r['attn_matrix_mb']:>12.1f}MB {r['savings']:>9.1f}x\")\n",
    "else:\n",
    "    print(\"GPU not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Verify — Final Quiz & Reflection (10 min)\n",
    "\n",
    "### Week 3 Comprehensive Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer(q, your_answer, correct):\n",
    "    if your_answer == correct:\n",
    "        print(f\"✓ {q}\")\n",
    "    else:\n",
    "        print(f\"✗ {q}\\n  Your: {your_answer}, Correct: {correct}\")\n",
    "\n",
    "print(\"Week 3 Final Quiz\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Q1\n",
    "check_answer(\n",
    "    \"Q1: Dot product measures ___ between vectors\",\n",
    "    'similarity',  # Your answer\n",
    "    'similarity'\n",
    ")\n",
    "\n",
    "# Q2\n",
    "check_answer(\n",
    "    \"Q2: exp(x) overflows in FP16 at approximately x =\",\n",
    "    11,  # Your answer\n",
    "    11\n",
    ")\n",
    "\n",
    "# Q3\n",
    "check_answer(\n",
    "    \"Q3: The max-subtraction trick works because softmax(x-c) = softmax(x)\",\n",
    "    True,  # Your answer\n",
    "    True\n",
    ")\n",
    "\n",
    "# Q4\n",
    "check_answer(\n",
    "    \"Q4: Standard attention memory complexity is\",\n",
    "    'O(N^2)',  # Your answer\n",
    "    'O(N^2)'\n",
    ")\n",
    "\n",
    "# Q5\n",
    "check_answer(\n",
    "    \"Q5: Online softmax enables streaming because it can ___ when max changes\",\n",
    "    'rescale',  # Your answer\n",
    "    'rescale'\n",
    ")\n",
    "\n",
    "# Q6\n",
    "check_answer(\n",
    "    \"Q6: FlashAttention achieves memory complexity of\",\n",
    "    'O(N)',  # Your answer\n",
    "    'O(N)'\n",
    ")\n",
    "\n",
    "# Q7\n",
    "check_answer(\n",
    "    \"Q7: Tiling enables attention to fit in fast ___ memory\",\n",
    "    'SRAM',  # Your answer (or 'shared')\n",
    "    'SRAM'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection: What We Built\n",
    "\n",
    "In one week, we went from basic dot products to a working FlashAttention implementation:\n",
    "\n",
    "```\n",
    "Day 1: a·b = Σaᵢbᵢ (similarity measurement)\n",
    "  ↓\n",
    "Day 2: exp(x) → overflow in FP16 at x≈11\n",
    "  ↓\n",
    "Day 3: softmax(x-max) = softmax(x) (stability trick)\n",
    "  ↓\n",
    "Day 4: Attention = softmax(QK^T/√d)V (O(N²) memory problem)\n",
    "  ↓\n",
    "Day 5: Online softmax (streaming with rescaling)\n",
    "  ↓\n",
    "Day 6: Tiled computation (SRAM utilization)\n",
    "  ↓\n",
    "Day 7: FlashAttention (O(N) memory, fused kernel)\n",
    "```\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Numerical stability matters:** Understanding FP limits prevents silent failures\n",
    "2. **Algorithms can reduce memory:** O(N²) → O(N) is possible with clever bookkeeping\n",
    "3. **Hardware awareness:** SRAM vs HBM determines kernel design\n",
    "4. **Fusion reduces I/O:** Combining operations saves memory bandwidth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: Week 3 Complete\n",
    "\n",
    "| Day | Topic | Key Insight |\n",
    "|-----|-------|-------------|\n",
    "| 1 | Dot Product | Similarity = a·b = \\|a\\|\\|b\\|cos(θ) |\n",
    "| 2 | Softmax Problem | exp(x) overflows at x≈11 (FP16) |\n",
    "| 3 | Stable Softmax | softmax(x-c) = softmax(x) |\n",
    "| 4 | Full Attention | O(N²) memory from N×N matrix |\n",
    "| 5 | Online Softmax | Rescale when max changes |\n",
    "| 6 | Tiled Attention | Process in SRAM-sized blocks |\n",
    "| 7 | FlashAttention | O(N) memory, fused kernel |\n",
    "\n",
    "**Next Week:** Quantization and production deployment — taking our kernels from learning exercises to real-world performance.\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- [FlashAttention Paper (Dao et al., 2022)](https://arxiv.org/abs/2205.14135)\n",
    "- [FlashAttention-2 Paper (Dao, 2023)](https://arxiv.org/abs/2307.08691)\n",
    "- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- [Online Softmax (Milakov & Gimelshein, 2018)](https://arxiv.org/abs/1805.02867)\n",
    "\n",
    "**Interactive Reference:** [attention-math.html](../attention-math.html) — Full attention visualization and calculators"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
